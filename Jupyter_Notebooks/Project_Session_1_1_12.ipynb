{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yp8jcEZ_7O2"
   },
   "source": [
    "#Session 1: Application of Inverse Problem and Data Assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETZGQLFdANEZ"
   },
   "source": [
    "## ðŸ¤– AI Coding Policy\n",
    "\n",
    "**Core Philosophy:** Treat AI as your **Co-pilot**, not your **Autopilot**.\n",
    "\n",
    "âœ… **Encouraged:**\n",
    "* Using AI to explain syntax, complex libraries, or error messages (Debugging).\n",
    "* Generating boilerplate code or optimizing specific snippets.\n",
    "* Brainstorming different implementation approaches.\n",
    "\n",
    "âŒ **Prohibited:**\n",
    "* **Copy-pasting homework prompts directly into AI to generate solutions.**\n",
    "* Submitting code that you do not understand or cannot explain.\n",
    "\n",
    "âš ï¸ **Be Careful:**\n",
    "1.  **Hallucinations:** AI makes confident mistakes. You must verify correctness.\n",
    "2.  **Accountability:** You are fully responsible for every line of code you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edtb1rwJCM-9"
   },
   "source": [
    "## 1. Problem Setup: Bayesian Inverse Problems\n",
    "\n",
    "We consider the general inverse problem of finding an unknown parameter $u$ from noisy data (Observation) $y$:\n",
    "\n",
    "$$y = G(u) + \\eta.$$\n",
    "\n",
    "### Bayesian Formulation\n",
    "\n",
    "* **Prior:** **$u\\sim\\rho(u)$**.\n",
    "* **Likelihood:** $y|u\\sim\\nu(y - G(u))$.\n",
    "* **Posterior:**\n",
    "$$\\pi^y(u) \\propto\\mathbb{P}(u)\\mathbb{P}(y|u)= \\nu(y - G(u)) \\cdot \\rho(u)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8EOyHrYtQ1v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def G_identity(u):\n",
    "    \"\"\"\n",
    "    Function: Identity map.\n",
    "    Input: u (array or float)\n",
    "    Output: u (array or float)\n",
    "    \"\"\"\n",
    "    return u\n",
    "\n",
    "def G_square(u):\n",
    "    \"\"\"\n",
    "    Function: Square map.\n",
    "    Input: u (array or float)\n",
    "    Output: u^2 (array or float)\n",
    "    \"\"\"\n",
    "    return u**2\n",
    "\n",
    "G_MAPS = {\n",
    "    'Linear: G(u) = u': G_identity,\n",
    "    'Non-linear: G(u) = u^2': G_square\n",
    "}\n",
    "\n",
    "def compute_densities(u_grid, y_obs, sigma_prior, sigma_noise, G_func):\n",
    "    \"\"\"\n",
    "    Function: Computes Prior, Likelihood, and Posterior densities.\n",
    "    Input:\n",
    "        u_grid (np.ndarray): Domain\n",
    "        y_obs (float): Observation\n",
    "        sigma_prior, sigma_noise (float): Standard deviations\n",
    "        G_func (callable): Forward map\n",
    "    Output: tuple (rho, likelihood, pi_y)\n",
    "    \"\"\"\n",
    "    rho = norm.pdf(u_grid, loc=0, scale=sigma_prior)\n",
    "    likelihood = norm.pdf(y_obs, loc=G_func(u_grid), scale=sigma_noise)\n",
    "    pi_y = rho * likelihood\n",
    "    return rho, likelihood, pi_y\n",
    "\n",
    "def viz_interactive(map_name, y_obs=4.0, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Function: Plots Prior, Likelihood, and Posterior on a single figure.\n",
    "    Input: map_name (str), y_obs (float), gamma (float)\n",
    "    Output: None (displays plot)\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    sigma_prior = 2.0\n",
    "    u_grid = np.linspace(-6, 6, 1000)\n",
    "    G_func = G_MAPS[map_name]\n",
    "\n",
    "    # Compute densities\n",
    "    rho, likelihood, pi_y = compute_densities(u_grid, y_obs, sigma_prior, gamma, G_func)\n",
    "\n",
    "    # Normalize posterior\n",
    "\n",
    "    normalization = np.trapezoid(pi_y, u_grid)\n",
    "    pi_y_norm = pi_y / normalization if normalization > 0 else pi_y\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # 1. Prior\n",
    "    ax.plot(u_grid, rho, 'b--', lw=1.5, label=r'Prior $\\rho(u)$')\n",
    "    ax.fill_between(u_grid, rho, color='blue', alpha=0.05)\n",
    "\n",
    "    # 2. Likelihood\n",
    "    ax.plot(u_grid, likelihood, 'r:', lw=2, label=r'Likelihood $L(u)$')\n",
    "\n",
    "    # 3. Posterior\n",
    "    ax.plot(u_grid, pi_y_norm, 'k-', lw=2.5, label=r'Posterior $\\pi^y(u)$')\n",
    "    ax.fill_between(u_grid, pi_y_norm, color='black', alpha=0.1)\n",
    "\n",
    "    # Mark roots if applicable (visual guide for modes)\n",
    "    if 'u^2' in map_name and y_obs > 0:\n",
    "        root = np.sqrt(y_obs)\n",
    "        ax.axvline(root, color='green', linestyle='-', alpha=0.3)\n",
    "        ax.axvline(-root, color='green', linestyle='-', alpha=0.3, label='Truth Roots')\n",
    "\n",
    "    ax.set_title(fr'Bayesian Inference (y={y_obs}, $\\gamma$={gamma})', fontsize=14)\n",
    "    ax.set_xlabel('$u$')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend(loc='upper right', frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Widget Setup ---\n",
    "# Note: continuous_update=False prevents updates while dragging\n",
    "print(\"Select Forward Map G(u) and adjust parameters:\")\n",
    "widgets.interactive(\n",
    "    viz_interactive,\n",
    "    map_name=widgets.Dropdown(options=list(G_MAPS.keys()), value='Non-linear: G(u) = u^2', description='Map:'),\n",
    "    y_obs=widgets.FloatSlider(value=4.0, min=-5.0, max=10.0, step=0.1, description=r'$y$ (Obs):', continuous_update=False),\n",
    "    gamma=widgets.FloatSlider(value=1.0, min=0.1, max=4.0, step=0.1, description=r'$\\gamma$ (Noise):', continuous_update=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHgntqULdaXg"
   },
   "source": [
    "## 2. Linear Inverse Problems: Denoising & Deblurring\n",
    "\n",
    "We generalize our problem to high-dimensional linear inverse problems:\n",
    "\n",
    "$$y = G(u) + \\eta$$\n",
    "\n",
    "where $u$ is the image we want to recover, and $G$ is a linear operator. To solve this, we minimize the following objective function (Variational Approach):\n",
    "\n",
    "$$u_{MAP} = \\operatorname*{argmin}_u \\left( \\underbrace{\\frac{1}{2}\\|G(u) - y\\|^2}_{\\text{Data Fidelity}} + \\underbrace{\\lambda \\|\\nabla u\\|_1}_{\\text{Regularization}} \\right)$$\n",
    "\n",
    "### Bayesian Interpretation\n",
    "Why this specific minimization? It corresponds exactly to finding the **Maximum A Posteriori (MAP)** estimate in a Bayesian framework: $u_{MAP} = \\operatorname*{argmax}_u \\pi^y(u)$.\n",
    "\n",
    "Taking the negative logarithm of the posterior, maximization becomes minimization:\n",
    "$$-\\log \\pi^y(u) = -\\log \\text{Likelihood} - \\log \\text{Prior} + C$$\n",
    "\n",
    "1.  **Likelihood ($\\text{Data Fidelity}$):**\n",
    "    Assuming the noise $\\eta$ is Gaussian $\\mathcal{N}(0, I)$, the likelihood of observing $y$ given $u$ is:\n",
    "    $$\\nu(y|u) \\propto \\exp\\left(-\\frac{1}{2}\\|G(u) - y\\|^2\\right)$$\n",
    "\n",
    "2.  **Prior ($\\text{Regularization}$):**\n",
    "    The Total Variation (TV) regularization implies a specific prior belief about the image statistics:\n",
    "    $$\\rho(u) \\propto \\exp\\left(-\\lambda \\|\\nabla u\\|_1\\right)$$\n",
    "    This is a **Laplace distribution** on the image gradients. Unlike a Gaussian prior (which smooths everything), a Laplace prior allows for \"sparse\" gradientsâ€”meaning it permits sharp edges (large jumps) while keeping flat regions smooth.\n",
    "\n",
    "### The Role of $G$\n",
    "* **Denoising:** $G = I$ (Identity). The data is simply a noisy version of $u$.\n",
    "* **Deblurring (Deconvolution):** $G$ is a convolution operator (e.g., Gaussian Blur). The data is both blurry and noisy, making the problem significantly harder (more ill-posed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81qSjTkECtWK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, img_as_float\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from skimage.util import random_noise\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Style setup\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "def text_image(size=128):\n",
    "    # Manually draw block letters \"CMS\"\n",
    "    img = np.zeros((size, size))\n",
    "\n",
    "    # Letter C\n",
    "    img[40:90, 10:20] = 1.0   # Left vertical\n",
    "    img[40:50, 10:40] = 1.0   # Top horizontal\n",
    "    img[80:90, 10:40] = 1.0   # Bottom horizontal\n",
    "\n",
    "    # Letter M (Blocky style)\n",
    "    img[40:90, 50:60] = 1.0   # Left vertical\n",
    "    img[40:90, 80:90] = 1.0   # Right vertical\n",
    "    img[40:50, 60:80] = 1.0   # Top bridge\n",
    "    img[50:70, 65:75] = 1.0   # Middle drop\n",
    "\n",
    "    # Letter S\n",
    "    img[40:50, 100:125] = 1.0 # Top\n",
    "    img[60:70, 100:125] = 1.0 # Middle\n",
    "    img[80:90, 100:125] = 1.0 # Bottom\n",
    "    img[40:65, 100:110] = 1.0 # Top-Left connector\n",
    "    img[65:90, 115:125] = 1.0 # Bottom-Right connector\n",
    "\n",
    "    return img\n",
    "\n",
    "# --- 1. Data Loading & Helpers ---\n",
    "def load_image(name):\n",
    "    if name == 'Cameraman':\n",
    "        return img_as_float(data.camera())\n",
    "    elif name == 'Text':\n",
    "        img = text_image()\n",
    "        return img\n",
    "\n",
    "def G_blur(u, sigma=2.0):\n",
    "    return gaussian_filter(u, sigma=sigma)\n",
    "\n",
    "# --- 2. Solver Logic ---\n",
    "def solve_inverse_problem(y, task_type, tv_weight, n_iters=50):\n",
    "    u = np.copy(y)\n",
    "    step_size = 0.9\n",
    "\n",
    "    if task_type == 'Denoising (G=I)':\n",
    "        return denoise_tv_chambolle(y, weight=tv_weight)\n",
    "\n",
    "    elif task_type == 'Deblurring (G=Blur)':\n",
    "        for _ in range(n_iters):\n",
    "            residual = G_blur(u) - y\n",
    "            gradient = G_blur(residual)\n",
    "            u = u - step_size * gradient\n",
    "            u = denoise_tv_chambolle(u, weight=tv_weight)\n",
    "        return u\n",
    "\n",
    "# --- 3. UI & Interaction Setup ---\n",
    "\n",
    "# A. Define Widgets\n",
    "w_img = widgets.Dropdown(\n",
    "    options=['Cameraman', 'Text'],\n",
    "    value='Cameraman',\n",
    "    description='Image:'\n",
    ")\n",
    "\n",
    "w_task = widgets.Dropdown(\n",
    "    options=['Denoising (G=I)', 'Deblurring (G=Blur)'],\n",
    "    value='Denoising (G=I)',\n",
    "    description='Task:'\n",
    ")\n",
    "\n",
    "w_noise = widgets.FloatSlider(\n",
    "    value=0.1, min=0.0, max=0.5, step=0.05,\n",
    "    description=r'Noise $\\sigma$:'\n",
    ")\n",
    "\n",
    "w_tv = widgets.FloatSlider(\n",
    "    value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "    description=r'TV $\\lambda$:'\n",
    ")\n",
    "\n",
    "btn_run = widgets.Button(\n",
    "    description='Run Reconstruction',\n",
    "    button_style='success',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "out_plot = widgets.Output()\n",
    "\n",
    "# B. Callback Function\n",
    "def on_button_clicked(b):\n",
    "    # Lock the button to prevent double clicks\n",
    "    btn_run.disabled = True\n",
    "    btn_run.description = \"Running...\"\n",
    "\n",
    "    with out_plot:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # 1. Get Params\n",
    "            img_name = w_img.value\n",
    "            task = w_task.value\n",
    "            noise_level = w_noise.value\n",
    "            tv_weight = w_tv.value\n",
    "\n",
    "            # 2. Compute\n",
    "            u_true = load_image(img_name)\n",
    "\n",
    "            if task == 'Denoising (G=I)':\n",
    "                y_clean = u_true\n",
    "            else:\n",
    "                y_clean = G_blur(u_true, sigma=2.0)\n",
    "\n",
    "            y_obs = random_noise(y_clean, mode='gaussian', var=noise_level**2)\n",
    "            u_est = solve_inverse_problem(y_obs, task, tv_weight)\n",
    "\n",
    "            # 3. Plot\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "            axes[0].imshow(u_true, cmap='gray')\n",
    "            axes[0].set_title(r\"Ground Truth $u_{true}$\", fontsize=14)\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            axes[1].imshow(y_obs, cmap='gray')\n",
    "            axes[1].set_title(fr\"Observation $y$ ({task})\", fontsize=14)\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            axes[2].imshow(u_est, cmap='gray')\n",
    "            axes[2].set_title(fr\"Reconstruction ($\\lambda={tv_weight}$)\", fontsize=14)\n",
    "            axes[2].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Use specific display function instead of plt.show()\n",
    "            display(fig)\n",
    "            plt.close(fig) # Prevent memory leak\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            # Reset button state\n",
    "            btn_run.disabled = False\n",
    "            btn_run.description = 'Run Reconstruction'\n",
    "\n",
    "btn_run.on_click(on_button_clicked)\n",
    "\n",
    "# C. Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([w_img, w_task]),\n",
    "    widgets.HBox([w_noise, w_tv]),\n",
    "    btn_run\n",
    "])\n",
    "\n",
    "print(\"Set parameters and click 'Run Reconstruction':\")\n",
    "display(ui, out_plot)\n",
    "\n",
    "# Run once at startup to ensure something is visible\n",
    "on_button_clicked(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIIQrmMYVOBQ"
   },
   "source": [
    "## 3. Data-Driven Approach: Learning a Direct Mapping\n",
    "\n",
    "Instead of solving an optimization problem for each new input, we train a neural network $\\mathcal{F}_\\theta$ to learn a direct, **deterministic mapping**:\n",
    "\n",
    "$$u \\approx \\mathcal{F}_\\theta(y)$$\n",
    "\n",
    "**The Hidden Prior:**\n",
    "Unlike the TV method where we explicitly wrote down the prior, here the prior is implicitly embedded in the network weights during the training process. By learning from massive datasets, the network encodes the statistical properties of natural images directly into its parameters.\n",
    "\n",
    "We demonstrate this with two examples:\n",
    "1.  **Super-Resolution (SR):** The network maps the low-diensional observation to a high-dimensional output.\n",
    "2.  **Inpainting:** The network deterministically fills in missing regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej3Y0Ib0Ialv"
   },
   "outputs": [],
   "source": [
    "# --- 0. Environment Setup ---\n",
    "try:\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing HuggingFace Transformers...\")\n",
    "    !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import data\n",
    "from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Load Pre-trained Model (SwinIR) ---\n",
    "model_name = \"caidas/swin2SR-classical-sr-x4-64\"\n",
    "print(f\"Loading model: {model_name} ...\")\n",
    "\n",
    "processor = Swin2SRImageProcessor.from_pretrained(model_name)\n",
    "model = Swin2SRForImageSuperResolution.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "\n",
    "def prepare_data_pair(img_hr_pil, crop_size=256):\n",
    "    \"\"\"\n",
    "    Crops the center of the image and simulates downsampling (degradation).\n",
    "    Returns:\n",
    "        img_hr: High-Res Ground Truth (cropped)\n",
    "        img_lr: Low-Res Observation (downsampled)\n",
    "    \"\"\"\n",
    "    w, h = img_hr_pil.size\n",
    "    left = (w - crop_size)/2\n",
    "    top = (h - crop_size)/2\n",
    "    img_hr_cropped = img_hr_pil.crop((left, top, left+crop_size, top+crop_size))\n",
    "\n",
    "    # Simulate degradation: Downsample by 4x\n",
    "    scale_factor = 4\n",
    "    w_lr, h_lr = img_hr_cropped.size[0] // scale_factor, img_hr_cropped.size[1] // scale_factor\n",
    "    img_lr = img_hr_cropped.resize((w_lr, h_lr), resample=Image.BICUBIC)\n",
    "\n",
    "    return img_hr_cropped, img_lr\n",
    "\n",
    "def solve_inverse_problem(img_lr_pil):\n",
    "    \"\"\"\n",
    "    Runs SwinIR inference to recover high-res image.\n",
    "    \"\"\"\n",
    "    inputs = processor(img_lr_pil, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process: Tensor -> PIL Image\n",
    "    output_tensor = outputs.reconstruction.data.squeeze().float().cpu().clamp_(0, 1)\n",
    "    output_numpy = output_tensor.numpy().transpose(1, 2, 0)\n",
    "    return Image.fromarray((output_numpy * 255.0).round().astype(np.uint8))\n",
    "\n",
    "# --- 3. Execution Pipeline ---\n",
    "print(\"\\nProcessing Astronaut image...\")\n",
    "\n",
    "# Load standard Astronaut image\n",
    "img_astro_full = Image.fromarray(data.astronaut())\n",
    "\n",
    "# Prepare High-Res and Low-Res pair\n",
    "hr_astro, lr_astro = prepare_data_pair(img_astro_full, crop_size=256)\n",
    "\n",
    "# Method A: Deep Learning Inverse (SwinIR)\n",
    "sr_astro_dl = solve_inverse_problem(lr_astro)\n",
    "\n",
    "# Method B: Classical Inverse (Bicubic Baseline)\n",
    "sr_astro_bicubic = lr_astro.resize(hr_astro.size, resample=Image.BICUBIC)\n",
    "\n",
    "# --- 4. Visualization ---\n",
    "print(\"Visualizing results...\")\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 6))\n",
    "\n",
    "cols = [\"Ground Truth ($x$)\", \"Observation ($y$)\\n(Low Res Input)\", \"Classical Inverse\\n(Bicubic)\", \"Deep Learning Inverse\\n(SwinIR)\"]\n",
    "\n",
    "# Plotting\n",
    "axes[0].imshow(hr_astro)\n",
    "axes[1].imshow(lr_astro.resize(hr_astro.size, Image.NEAREST)) # Nearest neighbor to emphasize pixels\n",
    "axes[2].imshow(sr_astro_bicubic)\n",
    "axes[3].imshow(sr_astro_dl)\n",
    "\n",
    "# Formatting\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(cols[i], fontsize=13, fontweight='bold' if i==3 else 'normal')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UiHQ7BrIz84"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Demo: Lightweight Inpainting (LaMa) - Simple Object\n",
    "# Model: LaMa (Resolution-robust Large Mask Inpainting) ~200MB\n",
    "# Data: Coffee Cup (skimage.data.coffee)\n",
    "# Task: Remove the spoon from the image\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Install lightweight library\n",
    "try:\n",
    "    import simple_lama_inpainting\n",
    "except ImportError:\n",
    "    print(\"Installing LaMa...\")\n",
    "    !pip install -q simple_lama_inpainting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import data\n",
    "from simple_lama_inpainting import SimpleLama\n",
    "\n",
    "# --- 1. Prepare Data (Simple Coffee Image) ---\n",
    "def prepare_simple_data():\n",
    "    \"\"\"\n",
    "    Loads coffee image and masks out the spoon.\n",
    "    \"\"\"\n",
    "    # Load image (Standard sample: Cup of coffee)\n",
    "    img_gt = Image.fromarray(data.coffee())\n",
    "\n",
    "    # Create Mask\n",
    "    w, h = img_gt.size\n",
    "    mask_np = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Define a box mask over the spoon (Right side of the image)\n",
    "    # Coordinates: [y1:y2, x1:x2]\n",
    "    mask_np[150:280, 300:500] = 255\n",
    "\n",
    "    mask = Image.fromarray(mask_np)\n",
    "\n",
    "    # Create Observation (Black out the spoon area)\n",
    "    img_masked_np = np.array(img_gt).copy()\n",
    "    img_masked_np[mask_np == 255] = 0\n",
    "    img_masked = Image.fromarray(img_masked_np)\n",
    "\n",
    "    return img_gt, img_masked, mask\n",
    "\n",
    "# --- 2. Inference ---\n",
    "print(\"Initializing LaMa model...\")\n",
    "lama = SimpleLama() # Automatically downloads ~200MB model on first run\n",
    "\n",
    "img_gt, img_masked, mask = prepare_simple_data()\n",
    "\n",
    "print(\"Running Inpainting (Removing the spoon)...\")\n",
    "# Inference is fast on CPU or GPU\n",
    "img_result = lama(img_masked, mask)\n",
    "\n",
    "# --- 3. Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(img_gt)\n",
    "axes[0].set_title(\"1. Ground Truth (With Spoon)\")\n",
    "\n",
    "axes[1].imshow(img_masked)\n",
    "axes[1].set_title(\"2. Input: Masked Image ($y$)\")\n",
    "\n",
    "axes[2].imshow(img_result)\n",
    "axes[2].set_title(\"3. Output: LaMa Inpainting\\n(Spoon Removed / Texture Restored)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J69PgfNMhG6"
   },
   "source": [
    "## 4. IP Summary: MAP Estimation vs. Uncertainty Quantification (UQ)\n",
    "\n",
    "Bayesian Inversion is not just about finding the *best* image; it is about understanding **reliability**.\n",
    "\n",
    "* **MAP Estimation (Point Estimate):**\n",
    "    * **Goal:** Find the single most likely solution.\n",
    "    * **Methodology:** Deterministic Deep Learning (like the **SwinIR** used above) excels here, producing visually sharp results.\n",
    "    * **Limitation:** It lacks **Uncertainty Quantification**. We do not know if a recovered detail is real or a \"hallucination\" of the model.\n",
    "\n",
    "* **Uncertainty Quantification (UQ):**\n",
    "    * **Goal:** Characterize the full posterior distribution $p(x|y)$ to assess confidence.\n",
    "    * **Methodology:** Classical mathematical methods (e.g., MCMC, Variational Inference) provide rigorous error bars and confidence intervals, which are often missing in standard deep learning.\n",
    "\n",
    "**Application Focus:**\n",
    "1.  **Medical Imaging & Geophysics (UQ focused):** reliability is paramount. We need to know the probability that a spot on an MRI is a tumor versus an artifact.\n",
    "2.  **Computational Photography (MAP focused):** Visual quality is paramount. A plausible high-resolution texture is acceptable even if not statistically guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PONgdsr-IvQ"
   },
   "source": [
    "## 5. Introduction to Data Assimilation\n",
    "\n",
    "**Data Assimilation (DA)** fuses a mathematical model with noisy observations to optimally estimate the state of a system.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**1. Stochastic Dynamics Model**\n",
    "The evolution of the true state $v^\\dagger$ is driven by dynamics $\\Psi$ and process noise $\\xi^\\dagger$:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{j+1}^\\dagger &= \\Psi(v_j^\\dagger) + \\xi_j^\\dagger, \\quad j \\in \\mathbb{Z}^+,  \\\\\n",
    "v_0^\\dagger &\\sim \\mathcal{N}(m_0, C_0), \\quad \\xi_j^\\dagger \\sim \\mathcal{N}(0, \\Sigma) \\text{ i.i.d.},\n",
    "\\end{align}\n",
    "$$\n",
    "* $v^\\dagger$: The unknown True State.\n",
    "* $\\Psi$: The Dynamics Operator (Physics).\n",
    "* $\\Sigma$: Process Noise Covariance (Model uncertainty).\n",
    "\n",
    "**2. Data Model**\n",
    "We observe the system via an operator $h$ with measurement noise $\\eta^\\dagger$:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{j+1}^\\dagger &= h(v_{j+1}^\\dagger) + \\eta_{j+1}^\\dagger, \\quad j \\in \\mathbb{Z}^+,  \\\\\n",
    "\\eta_{j+1}^\\dagger &\\sim \\mathcal{N}(0, \\Gamma) \\text{ i.i.d.}\n",
    "\\end{align}\n",
    "$$\n",
    "* $y^\\dagger$: The Observations.\n",
    "* $h$: Observation Operator (maps state space to observation space).\n",
    "* $\\Gamma$: Observation Noise Covariance (Sensor uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO3RWOEaVCMs"
   },
   "source": [
    "## 6: 2D Linear Gaussian Tracking\n",
    "\n",
    "In this first experiment, we visualize the core mechanism of the **Kalman Filter** in a 2D plane. We can clearly see how the filter combines the model prediction (Prior) with the observation (Likelihood) to produce a refined estimate (Posterior).\n",
    "\n",
    "### Experiment Setup\n",
    "* **State**: $v = [x_1, x_2]^\\top$.\n",
    "* **Dynamics ($\\Psi$)**: A rotation matrix $A$ that rotates the state by 0.35 radians at each step:\n",
    "    \\begin{bmatrix} \\cos(0.35) & -\\sin(0.35) \\\\ \\sin(0.35) & \\cos(0.35) \\end{bmatrix}\n",
    "* **Observation ($h$)**: We compare two scenarios:\n",
    "    1.  **Full Observation**: We see the exact position ($x_1$ and $x_2$). $H = I_2$.\n",
    "    2.  **Partial Observation**: We only see the first coordinate ($x_1$). $H = [1, 0]$.\n",
    "\n",
    "### Visualization Key\n",
    "* <font color=\"orange\">**Orange (Prior)**</font>: Forecast before seeing data.\n",
    "* <font color=\"red\">**Red (Observation)**</font>: The noisy data.\n",
    "    * **Dot**: Full observation ($x, y$).\n",
    "    * **Vertical Line**: Partial observation (only $x$ is seen).\n",
    "* <font color=\"blue\">**Blue (Posterior)**</font>: The corrected estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIW8PgwdVA4a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Global Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- 1. Helper Functions ---\n",
    "\n",
    "OBS_CONFIGS = {\n",
    "    'Full Observation (Identity)': {'matrix': np.eye(2), 'dim': 2},\n",
    "    'Partial Observation (Only x)': {'matrix': np.array([[1.0, 0.0]]), 'dim': 1}\n",
    "}\n",
    "\n",
    "def get_cov_ellipse(cov, centre, n_std=2.448, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib Ellipse patch. n_std=2.448 is 95% CI in 2D.\n",
    "    \"\"\"\n",
    "    cov = (cov + cov.T) / 2\n",
    "    lambda_, v = np.linalg.eigh(cov)\n",
    "    order = lambda_.argsort()[::-1]\n",
    "    lambda_ = lambda_[order]\n",
    "    v = v[:, order]\n",
    "    angle = np.degrees(np.arctan2(v[1, 0], v[0, 0]))\n",
    "    width, height = 2 * n_std * np.sqrt(np.maximum(lambda_, 0))\n",
    "    return Ellipse(xy=centre, width=width, height=height, angle=angle, **kwargs)\n",
    "\n",
    "def kalman_filter_step(m_prior, C_prior, y, H, Gamma):\n",
    "    \"\"\"\n",
    "    Single Kalman Analysis Step.\n",
    "    \"\"\"\n",
    "    S = H @ C_prior @ H.T + Gamma\n",
    "    K = C_prior @ H.T @ np.linalg.inv(S)\n",
    "    m_post = m_prior + K @ (y - H @ m_prior)\n",
    "    C_post = (np.eye(len(m_prior)) - K @ H) @ C_prior\n",
    "    return m_post, C_post\n",
    "\n",
    "def generate_data(steps, obs_mode_key, dyn_noise_std, obs_noise_std):\n",
    "    \"\"\"\n",
    "    Generates Truth (fixed start) and Noisy Observations.\n",
    "    \"\"\"\n",
    "    theta = 0.35\n",
    "    A = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                  [np.sin(theta),  np.cos(theta)]])\n",
    "\n",
    "    config = OBS_CONFIGS[obs_mode_key]\n",
    "    H = config['matrix']\n",
    "    obs_dim = config['dim']\n",
    "\n",
    "    Sigma = np.eye(2) * (dyn_noise_std**2)\n",
    "    Gamma = np.eye(obs_dim) * (obs_noise_std**2)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    v_true = np.zeros((steps + 1, 2))\n",
    "    v_true[0] = [5.0, 0.0] # Fixed Truth Start\n",
    "    ys = []\n",
    "\n",
    "    for k in range(steps):\n",
    "        v_true[k+1] = A @ v_true[k] + np.random.multivariate_normal([0,0], Sigma)\n",
    "        noise = np.random.multivariate_normal(np.zeros(obs_dim), Gamma)\n",
    "        ys.append(H @ v_true[k+1] + noise)\n",
    "\n",
    "    return v_true, ys, A, H, Sigma, Gamma\n",
    "\n",
    "# --- 2. UI & Interaction Setup ---\n",
    "\n",
    "# Obs Mode\n",
    "w_obs_mode = widgets.Dropdown(\n",
    "    options=list(OBS_CONFIGS.keys()),\n",
    "    value='Full Observation (Identity)',\n",
    "    description='Obs Mode:',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Initial Guess Controls\n",
    "w_guess_x = widgets.FloatText(value=5.0, description='Guess X:', step=0.5, layout=widgets.Layout(width='150px'))\n",
    "w_guess_y = widgets.FloatText(value=0.0, description='Guess Y:', step=0.5, layout=widgets.Layout(width='150px'))\n",
    "\n",
    "# Added style to ensure label is seen\n",
    "w_init_std = widgets.FloatSlider(\n",
    "    value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "    description='Guess Sigma:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Noise Controls\n",
    "w_dyn_noise = widgets.FloatSlider(value=0.01, min=0.001, max=0.05, step=0.001, description='Dyn Noise:', continuous_update=False)\n",
    "w_obs_noise = widgets.FloatSlider(value=1.0, min=0.1, max=3.0, step=0.1, description='Obs Noise:', continuous_update=False)\n",
    "\n",
    "# Button: Reduced width\n",
    "btn_run = widgets.Button(\n",
    "    description='Run Kalman Filter',\n",
    "    button_style='success',\n",
    "    icon='play',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "out_plot = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    btn_run.disabled = True\n",
    "    btn_run.description = \"Running...\"\n",
    "\n",
    "    with out_plot:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # 1. Retrieve Parameters\n",
    "            obs_mode = w_obs_mode.value\n",
    "            dyn_std = w_dyn_noise.value\n",
    "            obs_std = w_obs_noise.value\n",
    "\n",
    "            guess_mean = np.array([w_guess_x.value, w_guess_y.value])\n",
    "            init_std = w_init_std.value\n",
    "            steps = 12\n",
    "\n",
    "            # 2. Compute\n",
    "            v_true, ys, A, H, Sigma, Gamma = generate_data(steps, obs_mode, dyn_std, obs_std)\n",
    "\n",
    "            m_curr = guess_mean\n",
    "            C_curr = np.eye(2) * (init_std**2)\n",
    "\n",
    "            priors = []\n",
    "            posts = [(m_curr, C_curr)]\n",
    "\n",
    "            for k in range(steps):\n",
    "                # Forecast\n",
    "                m_prior = A @ m_curr\n",
    "                C_prior = A @ C_curr @ A.T + Sigma\n",
    "                priors.append((m_prior, C_prior))\n",
    "\n",
    "                # Analysis\n",
    "                m_post, C_post = kalman_filter_step(m_prior, C_prior, ys[k], H, Gamma)\n",
    "                posts.append((m_post, C_post))\n",
    "                m_curr, C_curr = m_post, C_post\n",
    "\n",
    "            # 3. Visualization\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "            # Plot Truth\n",
    "            ax.plot(v_true[:, 0], v_true[:, 1], 'k-', alpha=0.4, label='Truth (Fixed Start)', linewidth=2)\n",
    "            ax.scatter(v_true[:, 0], v_true[:, 1], c='k', s=20)\n",
    "\n",
    "            # Highlight Initial Guess\n",
    "            ell_init = get_cov_ellipse(posts[0][1], posts[0][0], edgecolor='green', linestyle='-', facecolor='green', alpha=0.1)\n",
    "            ax.add_patch(ell_init)\n",
    "            ax.plot(posts[0][0][0], posts[0][0][1], 's', color='green', label='Initial Guess (t=0)')\n",
    "\n",
    "            # Loop steps\n",
    "            for k in range(steps):\n",
    "                m_pri, C_pri = priors[k]\n",
    "                m_pos, C_pos = posts[k+1]\n",
    "                y_val = ys[k]\n",
    "                true_pos = v_true[k+1]\n",
    "\n",
    "                # --- Plot Obs ---\n",
    "                if obs_mode == 'Full Observation (Identity)':\n",
    "                    ax.plot(y_val[0], y_val[1], 'r*', markersize=10, alpha=0.7)\n",
    "                    # Link line\n",
    "                    ax.plot([true_pos[0], y_val[0]], [true_pos[1], y_val[1]],\n",
    "                            color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "                else:\n",
    "                    # Solid red line for Partial Obs\n",
    "                    ax.axvline(x=y_val[0], color='red', linestyle='-', alpha=0.3, linewidth=1.5)\n",
    "                    # Link line (horizontal)\n",
    "                    ax.plot([true_pos[0], y_val[0]], [true_pos[1], true_pos[1]],\n",
    "                            color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "                # --- Prior (Orange) ---\n",
    "                ell_pri = get_cov_ellipse(C_pri, m_pri, edgecolor='orange', linestyle='--', facecolor='none', alpha=0.6)\n",
    "                ax.add_patch(ell_pri)\n",
    "                ax.plot(m_pri[0], m_pri[1], 'x', color='orange', markersize=6)\n",
    "\n",
    "                # --- Posterior (Blue) ---\n",
    "                ell_pos = get_cov_ellipse(C_pos, m_pos, edgecolor='blue', linestyle='-', facecolor='blue', alpha=0.1)\n",
    "                ax.add_patch(ell_pos)\n",
    "                ax.plot(m_pos[0], m_pos[1], '.', color='blue', markersize=6)\n",
    "\n",
    "                # --- Arrow ---\n",
    "                ax.annotate(\"\", xy=(m_pos[0], m_pos[1]), xytext=(m_pri[0], m_pri[1]),\n",
    "                            arrowprops=dict(arrowstyle=\"->\", color=\"gray\", alpha=0.5))\n",
    "\n",
    "            # Legend\n",
    "            ax.plot([], [], 'x', color='orange', label='Prior (Forecast)')\n",
    "            ax.plot([], [], 'r*', label='Observation')\n",
    "            if obs_mode != 'Full Observation (Identity)':\n",
    "                ax.plot([], [], 'r-', label='Observed Subspace', alpha=0.5)\n",
    "            ax.plot([], [], 'o', color='blue', label='Posterior (Analysis)')\n",
    "\n",
    "            ax.set_xlim(-10, 10)\n",
    "            ax.set_ylim(-10, 10)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_title(f'Kalman Filter Tracking ({obs_mode})', fontsize=14)\n",
    "            ax.set_xlabel('$x_1$')\n",
    "            ax.set_ylabel('$x_2$')\n",
    "            ax.legend(loc='upper left', frameon=True)\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            btn_run.disabled = False\n",
    "            btn_run.description = 'Run Kalman Filter'\n",
    "\n",
    "btn_run.on_click(on_button_clicked)\n",
    "\n",
    "# Layout\n",
    "ui_guess = widgets.HBox([w_guess_x, w_guess_y, w_init_std])\n",
    "ui_noise = widgets.HBox([w_dyn_noise, w_obs_noise])\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<b>1. Settings:</b>\"),\n",
    "    w_obs_mode,\n",
    "    widgets.HTML(\"<b>2. Initial Guess (Prior at t=0):</b>\"),\n",
    "    ui_guess,\n",
    "    widgets.HTML(\"<b>3. System Noise:</b>\"),\n",
    "    ui_noise,\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    btn_run\n",
    "])\n",
    "\n",
    "print(\"Configure and Run:\")\n",
    "display(ui, out_plot)\n",
    "on_button_clicked(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W57I4cVZdcZg"
   },
   "source": [
    "## 7. Particle Filter on Lorenz '63 (Partial Observation)\n",
    "\n",
    "In this experiment, we explore the Particle Filter for the Lorenz '63 system. Unlike the Kalman Filter, which assumes all errors are Gaussian, the Particle Filter represents the probability distribution using a swarm of discrete particles.\n",
    "\n",
    "We specifically use the **Bootstrap Particle Filter**, which is a standard implementation of the sequential importance resampling method.\n",
    "\n",
    "### 1. Dynamics Model (Lorenz-63)\n",
    "The system is governed by the chaotic Lorenz equations ($\\sigma=10, \\rho=28, \\beta=8/3$):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} &= 10(y-x) \\\\\n",
    "\\frac{dy}{dt} &= x(28-z)-y \\\\\n",
    "\\frac{dz}{dt} &= xy - \\frac{8}{3} z\n",
    "\\end{align}\n",
    "$$\n",
    "We simulate this using a robust Runge-Kutta 4 (RK4) solver.\n",
    "\n",
    "### 2. Observation Model\n",
    "Crucially, this is a **partial observation** experiment. We assume we can only measure the first variable, $x$.\n",
    "The filter must infer the hidden variables $y$ and $z$ through the physical correlations maintained by the particle swarm.\n",
    "\n",
    "### 3. Algorithm Steps (Bootstrap PF)\n",
    "1.  **Forecast**: Each particle evolves independently according to the physics model plus random process noise.\n",
    "2.  **Weighting**: Particles are weighted based on the likelihood $p(y|x)$.\n",
    "3.  **Resampling**: High-weight particles are duplicated, while low-weight particles are discarded.\n",
    "\n",
    "### 4. Parameters & Interaction\n",
    "* **Assimilation Window**: $\\Delta t_{obs} = 0.15$.\n",
    "* **Solver Step**: $dt = 0.03$ (5 substeps per window).\n",
    "* **Noise**: System noise $\\Sigma=0.01$, Observation noise $\\Gamma=1.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gbWWza__Itl"
   },
   "source": [
    "Two papers related to the number of particles of the particle filter, and the mode collapse issue when the number of particles is not enough:\n",
    "\n",
    "1. Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems\n",
    "https://arxiv.org/abs/0805.3034\n",
    "\n",
    "2. Obstacles to High-Dimensional Particle Filtering\n",
    "https://journals.ametsoc.org/view/journals/mwre/136/12/2008mwr2529.1.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBRUnZPfVZNW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Global Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- 1. Physics & Solver ---\n",
    "\n",
    "def lorenz_dynamics(v):\n",
    "    \"\"\"\n",
    "    Description: Lorenz-63 derivatives.\n",
    "    Input: v=[x, y, z]\n",
    "    Output: dv/dt\n",
    "    \"\"\"\n",
    "    sigma, rho, beta = 10.0, 28.0, 8.0/3.0\n",
    "    x, y, z = v\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x * (rho - z) - y\n",
    "    dzdt = x * y - beta * z\n",
    "    return np.array([dxdt, dydt, dzdt])\n",
    "\n",
    "def rk4_integrate(v, dt, n_steps):\n",
    "    \"\"\"\n",
    "    Description: Propagates state v forward using RK4.\n",
    "    \"\"\"\n",
    "    curr_v = v.copy()\n",
    "    for _ in range(n_steps):\n",
    "        k1 = lorenz_dynamics(curr_v)\n",
    "        k2 = lorenz_dynamics(curr_v + 0.5 * dt * k1)\n",
    "        k3 = lorenz_dynamics(curr_v + 0.5 * dt * k2)\n",
    "        k4 = lorenz_dynamics(curr_v + dt * k3)\n",
    "        curr_v = curr_v + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "    return curr_v\n",
    "\n",
    "# --- 2. Particle Filter Logic ---\n",
    "\n",
    "def particle_filter_run(n_particles, steps, y_obs, obs_sigma, dyn_sigma, v0_fixed):\n",
    "    \"\"\"\n",
    "    Description: Runs SIR Particle Filter with Partial Observation (X only).\n",
    "    Input: y_obs is expected to be 1D array (sequence of x values).\n",
    "    \"\"\"\n",
    "    # Initialize particles around a guessed state\n",
    "    v0_guess = v0_fixed + np.random.normal(0, 3.0, 3)\n",
    "    particles = v0_guess + np.random.normal(0, 3.0, (n_particles, 3))\n",
    "\n",
    "    weights = np.ones(n_particles) / n_particles\n",
    "\n",
    "    # History\n",
    "    particles_hist = [particles.copy()]\n",
    "    mean_hist = [np.mean(particles, axis=0)]\n",
    "\n",
    "    # Params\n",
    "    dt_rk = 0.03\n",
    "    T_assim = 0.15\n",
    "    n_substeps = int(T_assim / dt_rk)\n",
    "\n",
    "    for k in range(steps):\n",
    "        # A. Forecast\n",
    "        for i in range(n_particles):\n",
    "            pred = rk4_integrate(particles[i], dt_rk, n_substeps)\n",
    "            noise = np.random.normal(0, dyn_sigma, 3)\n",
    "            particles[i] = pred + noise\n",
    "\n",
    "        # B. Update (Likelihood based on X ONLY)\n",
    "        # y_obs[k] is the observed scalar X\n",
    "        y_scalar = y_obs[k]\n",
    "\n",
    "        # Distance squared on X dimension only (index 0)\n",
    "        d2 = (particles[:, 0] - y_scalar)**2\n",
    "\n",
    "        # Log-weights\n",
    "        log_weights = -0.5 * d2 / (obs_sigma**2)\n",
    "\n",
    "        # Softmax normalization\n",
    "        max_log = np.max(log_weights)\n",
    "        weights = np.exp(log_weights - max_log)\n",
    "        weight_sum = np.sum(weights)\n",
    "        if weight_sum > 0:\n",
    "            weights /= weight_sum\n",
    "        else:\n",
    "            weights = np.ones(n_particles)/n_particles\n",
    "\n",
    "        # C. Resampling\n",
    "        indices = np.random.choice(n_particles, size=n_particles, p=weights)\n",
    "        particles = particles[indices]\n",
    "\n",
    "        # Store\n",
    "        particles_hist.append(particles.copy())\n",
    "        mean_hist.append(np.mean(particles, axis=0))\n",
    "\n",
    "    return particles_hist, np.array(mean_hist)\n",
    "\n",
    "def generate_lorenz_truth(steps, v0, dyn_sigma, obs_sigma):\n",
    "    \"\"\"\n",
    "    Description: Generates Truth (3D) and Observations (1D, X only).\n",
    "    \"\"\"\n",
    "    dt_rk = 0.03\n",
    "    T_assim = 0.15\n",
    "    n_substeps = int(T_assim / dt_rk)\n",
    "\n",
    "    v_true = np.zeros((steps + 1, 3))\n",
    "    v_true[0] = v0\n",
    "    ys = []\n",
    "\n",
    "    curr = v0.copy()\n",
    "    for k in range(steps):\n",
    "        curr = rk4_integrate(curr, dt_rk, n_substeps)\n",
    "        curr += np.random.normal(0, dyn_sigma, 3)\n",
    "        v_true[k+1] = curr\n",
    "\n",
    "        # Generate Observation (X only)\n",
    "        y_x = curr[0] + np.random.normal(0, obs_sigma)\n",
    "        ys.append(y_x)\n",
    "\n",
    "    return v_true, np.array(ys)\n",
    "\n",
    "# --- 3. UI & Visualization ---\n",
    "\n",
    "w_particles = widgets.IntSlider(\n",
    "    value=100, min=10, max=1000, step=10,\n",
    "    description='Particles:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "btn_pf_run = widgets.Button(\n",
    "    description='Run Particle Filter',\n",
    "    button_style='primary',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "out_pf = widgets.Output()\n",
    "\n",
    "def on_pf_click(b):\n",
    "    btn_pf_run.disabled = True\n",
    "    btn_pf_run.description = \"Computing...\"\n",
    "    with out_pf:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Fixed Seed for Reproducibility\n",
    "            np.random.seed(42)\n",
    "\n",
    "            # 1. Setup\n",
    "            steps = 25\n",
    "            v0_fixed = np.array([-5.0, -5.0, 20.0])\n",
    "            dyn_sigma = 0.01\n",
    "            obs_sigma = 1.0 # fixed\n",
    "            n_part = w_particles.value\n",
    "\n",
    "            # 2. Run\n",
    "            v_true, ys_obs = generate_lorenz_truth(steps, v0_fixed, dyn_sigma, obs_sigma)\n",
    "            part_hist, mean_hist = particle_filter_run(n_part, steps, ys_obs, obs_sigma, dyn_sigma, v0_fixed)\n",
    "\n",
    "            # 3. Plot (1 row, 3 columns)\n",
    "            fig = plt.figure(figsize=(18, 5))\n",
    "            time_points = np.arange(steps + 1)\n",
    "\n",
    "            # --- Plot 1: X Dimension (Observed) ---\n",
    "            ax1 = fig.add_subplot(1, 3, 1)\n",
    "            ax1.plot(time_points, v_true[:, 0], 'k-', lw=2, label='Truth X')\n",
    "            ax1.plot(time_points[1:], ys_obs, 'r*', ms=10, label='Obs X') # Show Obs Here\n",
    "            ax1.plot(time_points, mean_hist[:, 0], 'b--', lw=2, label='PF Est X')\n",
    "\n",
    "            # Particles Background\n",
    "            display_N = min(n_part, 50)\n",
    "            for t in range(steps + 1):\n",
    "                subset = part_hist[t][:display_N]\n",
    "                ax1.scatter([t]*len(subset), subset[:, 0], c='blue', alpha=0.1, s=5)\n",
    "\n",
    "            ax1.set_title('Observed State: X')\n",
    "            ax1.set_xlabel('Step')\n",
    "            ax1.set_ylabel('X Value')\n",
    "            ax1.legend()\n",
    "\n",
    "            # --- Plot 2: Y Dimension (Unobserved) ---\n",
    "            ax2 = fig.add_subplot(1, 3, 2)\n",
    "            ax2.plot(time_points, v_true[:, 1], 'k-', lw=2, label='Truth Y')\n",
    "            # NO Obs plot here\n",
    "            ax2.plot(time_points, mean_hist[:, 1], 'b--', lw=2, label='PF Est Y')\n",
    "\n",
    "            # Particles Background\n",
    "            for t in range(steps + 1):\n",
    "                subset = part_hist[t][:display_N]\n",
    "                ax2.scatter([t]*len(subset), subset[:, 1], c='blue', alpha=0.1, s=5)\n",
    "\n",
    "            ax2.set_title('Hidden State: Y (Inferred)')\n",
    "            ax2.set_xlabel('Step')\n",
    "            ax2.set_ylabel('Y Value')\n",
    "            ax2.legend()\n",
    "\n",
    "            # --- Plot 3: 3D Trajectory ---\n",
    "            ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "            ax3.plot(v_true[:,0], v_true[:,1], v_true[:,2], 'k-', lw=1.5, label='Truth', alpha=0.6)\n",
    "            ax3.scatter(v_true[0,0], v_true[0,1], v_true[0,2], c='k', marker='o', s=30, label='Start')\n",
    "\n",
    "            # Particles (Step by Step) - NO Obs Red Stars\n",
    "            for t in range(1, steps + 1):\n",
    "                p_curr = part_hist[t]\n",
    "                if n_part > 300:\n",
    "                    idx = np.random.choice(n_part, 300, replace=False)\n",
    "                    p_curr = p_curr[idx]\n",
    "                ax3.scatter(p_curr[:,0], p_curr[:,1], p_curr[:,2], c='blue', s=2, alpha=0.1, depthshade=True)\n",
    "\n",
    "            ax3.set_title('3D Particle Cloud')\n",
    "            ax3.set_xlabel('X')\n",
    "            ax3.set_ylabel('Y')\n",
    "            ax3.set_zlabel('Z')\n",
    "            ax3.view_init(elev=20, azim=-60)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            btn_pf_run.disabled = False\n",
    "            btn_pf_run.description = 'Run Particle Filter'\n",
    "\n",
    "btn_pf_run.on_click(on_pf_click)\n",
    "\n",
    "# Layout\n",
    "ui_pf = widgets.VBox([\n",
    "    widgets.HTML(\"<b>Adjust Particle Count (Observe stability at low N vs high N):</b>\"),\n",
    "    widgets.HBox([w_particles, btn_pf_run])\n",
    "])\n",
    "\n",
    "print(\"Particle Filter Demo (Partial Obs: X only):\")\n",
    "display(ui_pf, out_pf)\n",
    "on_pf_click(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFtEp1ltkps1"
   },
   "source": [
    "## 8. Real-World Impact: Why Weather Forecasting Needs DA?\n",
    "\n",
    "### Core Concept: Refining vs. Making Prediction\n",
    "It is crucial to understand that **Data Assimilation (DA) does not \"make\" predictions**â€”that is the job of the physical model ($\\Psi$).\n",
    "* **The Model's Job**: To calculate the future state based on physics.\n",
    "* **DA's Job**: To **refine** that calculation by constantly correcting the trajectory with new data.\n",
    "\n",
    "### The Challenge: The \"Butterfly Effect\"\n",
    "In **Numerical Weather Prediction (NWP)**, we use massive supercomputers to forward the chaotic dynamics:\n",
    "1.  **Sensitivity to Initial Conditions**: A tiny error in measuring today's temperature (even $0.01^\\circ$C) will grow exponentially. Without correction, even a perfect model would become useless after ~14 days.\n",
    "2.  **Model Imperfections**: No model is perfect. Approximations in physics (e.g., how clouds form) introduce systematic drift.\n",
    "\n",
    "### The Solution: DA as the \"Reality Check\"\n",
    "Data Assimilation acts as the necessary **correction mechanism**:\n",
    "* Every 6 hours, millions of observations (satellites, balloons, aircraft) are collected globally.\n",
    "* The model state is not simply \"overwritten\" by these measurements. Instead, noisy data is statistically fused with the model's previous forecast (the Prior) using DA methods (like **4D-Var** or **EnKF**).\n",
    "* **Result**: An optimal **\"Analysis\"** is generatedâ€”serving as the best possible starting point for the *next* prediction.\n",
    "\n",
    "Without this **refinement step**, modern weather forecasting would be impossible. The revolution of forecast accuracy over the last 30 years is largely attributed to better DA algorithms, not just faster computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83xyecObkxpy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
