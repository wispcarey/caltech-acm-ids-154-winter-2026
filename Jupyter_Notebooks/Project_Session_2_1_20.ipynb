{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBEla1WoOBU8"
   },
   "source": [
    "**Free Colab Pro** for students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsKZ5ltYhLa4"
   },
   "source": [
    "# PyTorch Basics: A Complete Machine Learning Pipeline\n",
    "\n",
    "In this session, we will translate the theoretical concepts of **Supervised Learning** into practical code. We will build a complete machine learning workflow from scratch using **PyTorch**.\n",
    "\n",
    "The goal is to understand the standard paradigm of deep learning programming. We will break down the process into five canonical steps:\n",
    "\n",
    "1.  **Data Preparation**: Generating synthetic data and wrapping it into PyTorch `Dataset` and `DataLoader`.\n",
    "2.  **Model Design**: Defining a Neural Network architecture using `torch.nn.Module`.\n",
    "3.  **Optimization Setup**: Choosing an appropriate `Loss Function` and `Optimizer`.\n",
    "4.  **Training Loop**: Implementing the core iterative process (Forward $\\to$ Loss $\\to$ Backward $\\to$ Update).\n",
    "5.  **Evaluation & Visualization**: Assessing model performance and visualizing the fitted curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxydbO4chZq8"
   },
   "source": [
    "# 1. Problem Setup / Dataset\n",
    "\n",
    "We consider a regression problem where the data $\\{x_i, y_i\\}$ satisfies:\n",
    "\n",
    "$$y_i = f(x_i) + \\epsilon_i$$\n",
    "\n",
    "where $\\epsilon_i$ represents the noise. The underlying function $f(x)$ is defined on the domain $x \\in [-1, 1]$ as:\n",
    "\n",
    "$$f(x) = \\sin(10\\pi x^2)$$\n",
    "\n",
    "**Goal:** Train a neural network to approximate $f(x)$ using only the noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 6505,
     "status": "ok",
     "timestamp": 1768844718577,
     "user": {
      "displayName": "Bohan Chen",
      "userId": "08649815860917755345"
     },
     "user_tz": 480
    },
    "id": "mUjK96DhhF8h",
    "outputId": "c4a42efe-1df7-47c4-9c14-22dee27c030e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data(n_samples=1000, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Generates synthetic data for the chirp signal regression task.\n",
    "\n",
    "    Input:\n",
    "        n_samples (int): Number of data points.\n",
    "        sigma (float): Standard deviation of the Gaussian noise.\n",
    "    Output:\n",
    "        x (Tensor): Input features of shape (n_samples, 1).\n",
    "        y (Tensor): Target values of shape (n_samples, 1).\n",
    "    \"\"\"\n",
    "    # Generate uniform random inputs in [-1, 1]\n",
    "    x = torch.rand(n_samples, 1) * 2 - 1\n",
    "\n",
    "    # Compute the true function: sin(10 * pi * x^2)\n",
    "    y_true = torch.sin(10 * torch.pi * x**2)\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    noise = torch.randn(y_true.shape) * sigma\n",
    "    y = y_true + noise\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# --- 1. Data Generation ---\n",
    "N_SAMPLES = 1000\n",
    "NOISE_LEVEL = 0.2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "x_train, y_train = generate_data(n_samples=N_SAMPLES, sigma=NOISE_LEVEL)\n",
    "\n",
    "# --- 2. Visualization ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Create a dense grid for plotting the true curve\n",
    "x_grid = torch.linspace(-1, 1, 500).unsqueeze(1)\n",
    "y_grid = torch.sin(10 * torch.pi * x_grid**2)\n",
    "\n",
    "plt.scatter(x_train, y_train, s=10, alpha=0.4, color='gray', label='Noisy Data')\n",
    "plt.plot(x_grid, y_grid, color='red', linewidth=2, label=r'True Function: $\\sin(10\\pi x^2)$')\n",
    "plt.title(\"Chirp Signal with Noise\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 3. DataLoader Setup ---\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Data ready: {len(train_dataset)} samples, Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Ps1GieYVJN"
   },
   "source": [
    "## 1.1. TensorDataset & Custom Datasets\n",
    "\n",
    "In the previous step, we used `TensorDataset`. It is a convenient wrapper when your data is already loaded into memory as Tensors.\n",
    "* **Documentation**: [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)\n",
    "\n",
    "**Beyond TensorDataset:**\n",
    "In most real-world scenarios (e.g., training on ImageNet or large scientific datasets), it is impossible to load all data into RAM at once. Instead, we use a **Custom Dataset**.\n",
    "\n",
    "To do this, we create a class that inherits from `torch.utils.data.Dataset` and implement three key methods:\n",
    "1.  `__init__`: Initialize file paths or data configurations.\n",
    "2.  `__len__`: Return the total number of samples.\n",
    "3.  `__getitem__`: Load and return **one** sample given an index.\n",
    "\n",
    "### Advanced Data Handling: Custom Dataset\n",
    "\n",
    "In real-world projects, we often need to manipulate data on the fly. We will create a `CustomDataset` class that allows prior knowledge in the dataset:\n",
    "\n",
    "1.  **Data Augmentation**: The function $\\sin(10\\pi x^2)$ is symmetric about $y$-axis. We can randomly flip the sign of $x$ without changing $y$ to increase data diversity.\n",
    "2.  **Sample Reweighting**: The signal oscillates faster as $|x| \\to 1$. High-frequency patterns are generally harder for neural networks to learn. We assign higher weights to samples with larger $|x|$ to force the model to focus on these difficult regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1768846337860,
     "user": {
      "displayName": "Bohan Chen",
      "userId": "08649815860917755345"
     },
     "user_tz": 480
    },
    "id": "BtXv5I9pYUVE",
    "outputId": "96cacbb2-c059-48c5-b257-276cfbac0c8e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChirpDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for Chirp signal with optional augmentation and weighting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, target, augment=False, return_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): Input features x.\n",
    "            target (Tensor): Label values y.\n",
    "            augment (bool): If True, randomly flip sign of x.\n",
    "            return_weights (bool): If True, return (x, y, w) tuple.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.augment = augment\n",
    "        self.return_weights = return_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load data and target\n",
    "        x = self.data[idx]\n",
    "        y = self.target[idx]\n",
    "\n",
    "        # 2. Apply Augmentation (Random Sign Flip)\n",
    "        # Since f(x) = sin(10*pi*x^2), f(x) == f(-x). Flipping x is valid.\n",
    "        if self.augment:\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                x = -x\n",
    "\n",
    "        # 3. Calculate Weights (if requested)\n",
    "        # Weight rule: w = 1 + |x|.\n",
    "        # This gives higher importance (up to 2x) to high-frequency regions near edges.\n",
    "        if self.return_weights:\n",
    "            weight = 1.0 + torch.abs(x)\n",
    "            return x, y, weight\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# --- Usage Example ---\n",
    "\n",
    "# Case 1: Standard training (No augmentation)\n",
    "train_ds_basic = ChirpDataset(x_train, y_train, augment=False, return_weights=False)\n",
    "loader_basic = DataLoader(train_ds_basic, batch_size=64, shuffle=True)\n",
    "\n",
    "# Case 2: Advanced training (With augmentation and weights)\n",
    "train_ds_advanced = ChirpDataset(x_train, y_train, augment=True, return_weights=True)\n",
    "loader_advanced = DataLoader(train_ds_advanced, batch_size=64, shuffle=True)\n",
    "\n",
    "# Verify the output\n",
    "print(\"Basic Loader Sample:\")\n",
    "for x, y in loader_basic:\n",
    "    print(f\"Shapes - x: {x.shape}, y: {y.shape}\")\n",
    "    break\n",
    "\n",
    "print(\"\\nAdvanced Loader Sample (x, y, w):\")\n",
    "for batch in loader_advanced:\n",
    "    # Unpack based on return length\n",
    "    x, y, w = batch\n",
    "    print(f\"Shapes - x: {x.shape}, y: {y.shape}, w: {w.shape}\")\n",
    "    print(f\"Sample Weight: {w[0].item():.4f} (for x={x[0].item():.4f})\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpBYMSAfiWfw"
   },
   "source": [
    "## 1.2 Important Considerations for Data Pipelines\n",
    "\n",
    "Defining a `Dataset` class is only the first step. In scientific machine learning and real-world applications, how you handle data preprocessing and memory management is often more critical than the model architecture itself. Here are four key concepts to keep in mind:\n",
    "\n",
    "### 1. Data Normalization & Standardization\n",
    "Neural networks struggle when input features have vastly different scales (e.g., temperature in Kelvin vs. pressure in Pascals). Large input values can lead to exploding gradients or slow convergence.\n",
    "* **Standardization**: Subtract the mean and divide by the standard deviation ($\\mu=0, \\sigma=1$).\n",
    "* **Normalization**: Scale data to a fixed range, typically $[-1, 1]$ or $[0, 1]$.\n",
    "* **Crucial Rule (Data Leakage)**: Always compute statistics (mean, min, max) using **only the Training Set**. Apply these stored statistics to transform the Validation and Test sets. Never use global statistics, as this \"leaks\" future information into the training process.\n",
    "\n",
    "### 2. Eager Loading vs. Lazy Loading\n",
    "* **Eager Loading**: Loading all data into RAM during `__init__`.\n",
    "    * *Pros*: Fast access during training.\n",
    "    * *Cons*: Limited by system RAM. Impossible for large datasets (e.g., ImageNet, terabytes of simulation data).\n",
    "* **Lazy Loading**: Storing only file paths or metadata in `__init__`. The actual data is loaded from the disk inside `__getitem__`.\n",
    "    * *Pros*: Scalable to infinite dataset sizes.\n",
    "    * *Cons*: I/O latency becomes the bottleneck (mitigated by using `num_workers > 0` in DataLoader).\n",
    "\n",
    "### 3. Data Splitting\n",
    "Never evaluate your model on the data it was trained on. A standard workflow involves three subsets:\n",
    "* **Training Set**: Used to compute gradients and update weights.\n",
    "* **Validation Set**: Used to tune hyperparameters (learning rate, architecture) and monitor overfitting (Early Stopping).\n",
    "* **Test Set**: Used **only once** at the very end to report final performance.\n",
    "* *Implementation*: Use `torch.utils.data.random_split` to create these subsets reproducibly.\n",
    "\n",
    "### 4. Custom Batching (`collate_fn`)\n",
    "The default `DataLoader` assumes that every sample in a batch has the exact same tensor shape so it can stack them.\n",
    "* **The Problem**: In many domains (e.g., variable-length time series, graphs with different node counts, NLP), samples have different sizes.\n",
    "* **The Solution**: You can pass a custom function to the `collate_fn` argument in `DataLoader`. This function defines how a list of individual samples should be combined into a batch (e.g., via padding or returning a Python list instead of a stacked Tensor).\n",
    "\n",
    "### 5. Data Augmentation & Prior Knowledge\n",
    "Data augmentation is not just about increasing dataset size; it is a mechanism to explicitly encode **prior knowledge** and **invariances** into the training process.\n",
    "* **Encoding Symmetries**: By transforming inputs (e.g., rotation, translation, or scaling) while keeping the target output fixed, you teach the model that the underlying features are **invariant** to these changes.\n",
    "* **Stability**: This forces the model to learn robust representations that are stable against perturbations, rather than memorizing specific pixel or feature arrangements.\n",
    "\n",
    "### 6. Parallel Data Loading\n",
    "Deep learning workflows are often I/O bound, meaning the high-speed GPU sits idle while waiting for the CPU to read and process data.\n",
    "* **Multiprocessing**: The `DataLoader` can use multiple CPU subprocesses (`num_workers > 0`) to load and preprocess batches in the background. This \"prefetching\" mechanism ensures a steady stream of data is always ready for the GPU, maximizing throughput.\n",
    "* **Note on Randomness**: When using parallel workers, standard random number generators might duplicate seeds across processes. You may need a `worker_init_fn` to ensure independent randomness (e.g., for noise injection) in each worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1HtVeV2pF7X"
   },
   "source": [
    "# 2. Model Architecture\n",
    "\n",
    "Now that we have our data ready, we need to design a function approximator. For this task, we will use a **Multi-Layer Perceptron (MLP)**.\n",
    "\n",
    "Instead of hard-coding a specific network structure, it is best practice to define a **flexible architecture**. This allows us to treat structural parameters (like depth and width) as hyperparameters that we can tune to improve performance.\n",
    "\n",
    "In the code below, we define a `FlexibleMLP` class where you can easily configure:\n",
    "* **`hidden_dim`**: The width of the layers.\n",
    "* **`num_hidden_layers`**: The depth of the network.\n",
    "* **`activation`**: The non-linear activation function (e.g., `nn.Tanh`, `nn.ReLU`).\n",
    "\n",
    "We typically use `nn.Tanh` for smooth function approximation tasks like this one, as it provides continuous derivatives unlike `nn.ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSKkFsmmdF-t"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A configurable Multi-Layer Perceptron (MLP) architecture.\n",
    "    It allows dynamic adjustments of depth, width, and activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_hidden_layers=3, activation=nn.Tanh):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension.\n",
    "            output_dim (int): Output label dimension.\n",
    "            hidden_dim (int): Number of neurons in each hidden layer.\n",
    "            num_hidden_layers (int): Number of hidden layers (excluding output layer).\n",
    "            activation (nn.Module class): The activation function class (e.g., nn.Tanh, nn.ReLU).\n",
    "        \"\"\"\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # 1. Input Layer -> First Hidden Layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(activation())\n",
    "\n",
    "        # 2. Intermediate Hidden Layers\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "\n",
    "        # 3. Output Layer (Linear, usually no activation for regression)\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        # Register all layers as a Sequential module\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn1DfkvxpY72"
   },
   "source": [
    "## 2.1. Beyond MLP: A Tour of Classic Architectures\n",
    "\n",
    "While we are using a **Multi-Layer Perceptron (MLP)** for this 1D regression task, modern Deep Learning relies on specialized architectures designed to handle specific data structures (images, sequences, etc.) or to solve training difficulties.\n",
    "\n",
    "Here are some milestone architectures you should know:\n",
    "\n",
    "### 1. The Universal Approximator: MLP\n",
    "The architecture we are using today.\n",
    "* **Key Concept**: The **Universal Approximation Theorem** states that a feed-forward network with a single hidden layer (and sufficient width) can approximate any continuous function.\n",
    "* **Relevance**: It is the building block of almost all deep learning models.\n",
    "* **Paper**: [Approximation by Superpositions of a Sigmoidal Function (Cybenko, 1989)](https://link.springer.com/article/10.1007/BF00332918) or [Multilayer Feedforward Networks are Universal Approximators (Hornik et al., 1989)](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208)\n",
    "\n",
    "### 2. Convolutional Neural Networks (CNNs)\n",
    "The architecture that sparked the modern Deep Learning revolution.\n",
    "* **Key Concept**: **Convolution** and **Pooling**. Instead of fully connected layers, it uses shared weights (filters) to scan the input, making it translation invariant and highly efficient for grid data (images, time-series, PDEs).\n",
    "* **Relevance**: Standard for Image Processing and solving PDEs on regular grids.\n",
    "* **Paper**: [ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "### 3. Residual Networks (ResNet)\n",
    "Perhaps the most important architectural innovation for training **deep** networks.\n",
    "* **Key Concept**: **Skip Connections** (or Shortcut Connections). Instead of learning $H(x)$, layers learn the residual $F(x) = H(x) - x$. This allows gradients to flow through the network without vanishing, enabling the training of networks with 100+ or 1000+ layers.\n",
    "* **Relevance**: Essential for Deep Scientific Learning to capture multi-scale physics without gradient instability.\n",
    "* **Paper**: [Deep Residual Learning for Image Recognition (He et al., 2016)](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "### 4. U-Net\n",
    "A specialized CNN architecture originally for biomedical image segmentation.\n",
    "* **Key Concept**: **Encoder-Decoder** structure with skip connections between corresponding encoder and decoder layers. It captures context (low resolution) and localization (high resolution) simultaneously.\n",
    "* **Relevance**: The gold standard for **Image-to-Image** tasks and **Inverse Problems** (e.g., mapping a noisy seismic wave field to a velocity model).\n",
    "* **Paper**: [U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "### 5. Transformers\n",
    "The current state-of-the-art in NLP and increasingly in Vision/Science.\n",
    "* **Key Concept**: **Self-Attention Mechanism**. It allows the model to weigh the importance of different parts of the input data regardless of their distance, discarding the sequential bias of RNNs.\n",
    "* **Relevance**: Foundation models (GPT, BERT) and Neural Operators (Transformer-based FNO).\n",
    "* **Paper**: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwG2FElJrdlv"
   },
   "source": [
    "## 2.2 Advanced Architectural Concepts\n",
    "\n",
    "While standard layers like `nn.Linear` and `nn.Conv2d` cover most use cases, PyTorch allows for deep customization. Here are a few advanced concepts related to model architecture that you should be aware of:\n",
    "\n",
    "### 1. Custom Layers\n",
    "You are not limited to the pre-defined layers in `torch.nn`. You can create your own layer by defining a class that inherits from `nn.Module`.\n",
    "* **Use Case**: Implementing a specific physical formula, a non-standard activation function, or a domain-specific operation (e.g., a Fourier Transform layer) directly into the gradient flow.\n",
    "* **Mechanism**: You simply define parameters in `__init__` using `nn.Parameter` and define the computation in `forward`.\n",
    "\n",
    "### 2. Weight Initialization\n",
    "How you initialize your model's parameters (weights and biases) can significantly impact convergence speed and stability.\n",
    "* **The Problem**: If weights are initialized too small, gradients vanish; if too large, they explode.\n",
    "* **Tools**: `torch.nn.init` provides methods like **Xavier (Glorot) initialization** (good for Tanh/Sigmoid) and **Kaiming (He) initialization** (good for ReLU). While PyTorch layers have sensible defaults, manual initialization is often necessary for specialized architectures.\n",
    "\n",
    "### 3. Regularization Layers\n",
    "Some \"layers\" are not designed to learn features but to improve training stability or prevent overfitting.\n",
    "* **Dropout (`nn.Dropout`)**: Randomly zeroes out some elements of the input tensor during training. This prevents neurons from co-adapting too much and acts as an ensemble method.\n",
    "* **Normalization (`nn.BatchNorm`, `nn.LayerNorm`)**: Normalizes the input to have zero mean and unit variance. This stabilizes the learning process and allows the use of higher learning rates.\n",
    "\n",
    "### 4. Dynamic Connectivity (Beyond `nn.Sequential`)\n",
    "In our simple example, we used `nn.Sequential` for a linear stack of layers. However, defining the `forward` method manually gives you complete control.\n",
    "* **Skip Connections**: You can implement Residual connections (ResNet) by simply doing `output = layer(x) + x`.\n",
    "* **Multiple Inputs/Outputs**: Your model can take multiple tensors as input (e.g., spatial coordinates `x` and time `t`) and process them through different branches before merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc8adhJ_tILs"
   },
   "source": [
    "# 3. Optimization Setup: Loss and Optimizer\n",
    "\n",
    "We frame the training of the neural network as an **optimization problem**. Our goal is to find the set of parameters $\\theta$ that **minimizes** a specific objective function. This requires two key components:\n",
    "\n",
    "1.  **The Loss Function**: Defines the objective to be minimized.\n",
    "2.  **The Optimizer**: The algorithm that updates the parameters $\\theta$ based on gradients to reduce the loss.\n",
    "\n",
    "While PyTorch provides standard loss functions (like `nn.MSELoss`), it is highly flexible: **any differentiable function** can serve as a custom objective. This is particularly important in scientific computing, where we often need to incorporate physical constraints or weighted penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zokt7N_UpO0J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Option 1: Functional Implementation ---\n",
    "def weighted_mse_loss_fn(prediction, target, weight=None):\n",
    "    \"\"\"\n",
    "    Computes Weighted Mean Squared Error as a standalone function.\n",
    "\n",
    "    Args:\n",
    "        prediction (Tensor): Model output.\n",
    "        target (Tensor): Ground truth.\n",
    "        weight (Tensor, optional): Importance weight for each sample.\n",
    "    \"\"\"\n",
    "    loss = (prediction - target) ** 2\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# --- Option 2: Class-based Implementation (nn.Module) ---\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes Weighted Mean Squared Error as a module.\n",
    "    Useful when integration into a larger pipeline or state management is needed.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target, weight=None):\n",
    "        \"\"\"\n",
    "        Calculates the loss during the forward pass.\n",
    "\n",
    "        Args:\n",
    "            prediction (Tensor): Model output.\n",
    "            target (Tensor): Ground truth.\n",
    "            weight (Tensor, optional): Importance weight for each sample.\n",
    "        \"\"\"\n",
    "        loss = (prediction - target) ** 2\n",
    "        if weight is not None:\n",
    "            loss = loss * weight\n",
    "        return loss.mean()\n",
    "\n",
    "# Usage Examples (Mental check):\n",
    "# loss = weighted_mse_loss_fn(pred, target, w)\n",
    "# criterion = WeightedMSELoss(); loss = criterion(pred, target, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTpwfgAOzPv7"
   },
   "source": [
    "## 3.1 Advanced PyTorch Implementation Details for the Loss\n",
    "\n",
    "In research, simply calling `criterion(y_pred, y_true)` is rarely enough. Here are three critical engineering aspects of Loss Functions in PyTorch that you need to master:\n",
    "\n",
    "### 1. Reduction Modes (`reduction='none'`)\n",
    "Standard PyTorch loss functions (like `MSELoss`) average the error over the batch by default (`reduction='mean'`). However, scientific applications often require granular control.\n",
    "* **The Feature**: Setting `reduction='none'` returns the loss for **each individual sample** as a vector, rather than a single scalar.\n",
    "* **Why use it?**\n",
    "    * **Per-sample Weighting**: If your weights are complex or computed dynamically, you calculate raw losses first and multiply by weights manually.\n",
    "    * **Debugging**: You can inspect which specific samples have the highest error (e.g., \"Is the model failing at the boundaries $x=\\pm 1$?\").\n",
    "\n",
    "### 2. The Broadcasting Trap (Shape Safety)\n",
    "A common \"silent bug\" occurs when tensor shapes do not match exactly.\n",
    "* **Scenario**: `prediction` has shape `(64, 1)` but `target` has shape `(64,)`.\n",
    "* **The Bug**: PyTorch will **broadcast** the subtraction, resulting in a `(64, 64)` matrix instead of a `(64, 1)` vector. The code runs without error, but the loss is mathematically wrong.\n",
    "* **Best Practice**: Always ensure shapes match explicitly before calculating loss:\n",
    "    ```python\n",
    "    # Safe practice\n",
    "    loss = criterion(prediction.squeeze(), target.squeeze())\n",
    "    ```\n",
    "\n",
    "### 3. Multi-Objective Optimization\n",
    "In Inverse Problems, we often combine a data-fitting term with a regularization term (e.g., physical constraints).\n",
    "* **Implementation**: Since PyTorch builds a dynamic computational graph, you can simply sum the losses. Autograd handles the chain rule for the combined objective.\n",
    "\n",
    "$$ \\mathcal{L}_{total} = \\mathcal{L}_{data} + \\lambda \\cdot \\mathcal{L}_{reg} $$\n",
    "* **Code**:\n",
    "    ```python\n",
    "    loss_data = mse(pred, target)\n",
    "    loss_reg = torch.mean(model.weights ** 2) # L2 Regularization\n",
    "    total_loss = loss_data + 1e-4 * loss_reg\n",
    "    total_loss.backward() # Gradients flow to both terms\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii776xxm0-m1"
   },
   "source": [
    "## 3.2 The Optimizer\n",
    "\n",
    "The optimizer is the algorithm that updates the model parameters $\\theta$ to minimize the loss function. It determines *how* the network learns.\n",
    "\n",
    "**Common Optimizers:**\n",
    "1.  **SGD (Stochastic Gradient Descent)**: The classic approach. It updates weights based on the gradient of the current batch. It is robust but can be slow to converge and sensitive to the learning rate.\n",
    "2.  **Adam (Adaptive Moment Estimation)**: The default choice for most deep learning tasks. It computes adaptive learning rates for each parameter. It generally converges faster than SGD and requires less tuning.\n",
    "\n",
    "**The Learning Rate ($\\eta$):**\n",
    "This is arguably the most critical hyperparameter. In gradient-based optimization, the learning rate serves as the **step size**. It is the coefficient that scales the gradient during the parameter update. For example, the update rule for standard SGD is:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3bi4ZhUt8h6"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def get_optimizer(model, opt_type='adam', learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Initializes the optimizer for the model.\n",
    "\n",
    "    Input:\n",
    "        model (nn.Module): The neural network.\n",
    "        opt_type (str): 'adam' or 'sgd'.\n",
    "        learning_rate (float): Step size for updates.\n",
    "    Output:\n",
    "        optimizer (torch.optim.Optimizer): The configured optimizer.\n",
    "    \"\"\"\n",
    "    if opt_type.lower() == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif opt_type.lower() == 'sgd':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer type: {opt_type}\")\n",
    "\n",
    "# Example usage\n",
    "# optimizer = get_optimizer(model, opt_type='adam', lr=0.01)\n",
    "# print(f\"Optimizer initialized: {type(optimizer).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XSSseAh1Wy7"
   },
   "source": [
    "## 3.3 Advanced Optimization Concepts\n",
    "\n",
    "While standard Adam works well for most of the problems, scientific research and large-scale training often require specialized tools and a deeper understanding of optimizer hyperparameters.\n",
    "\n",
    "### 1. Modern Optimizers\n",
    "\n",
    "* **AdamW**: [Documentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)\n",
    "    * **What it is**: A modification of Adam that **decouples** weight decay from the gradient update.\n",
    "    * **Why use it**: It is the standard for training Transformers (LLMs, ViT). It generally generalizes better than Adam because the L2 regularization is applied more correctly during the adaptive update.\n",
    "* **L-BFGS**: [Documentation](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html)\n",
    "    * **What it is**: A Quasi-Newton method that uses second-order information (Hessian approximation) to guide updates.\n",
    "    * **Use Case**: Extremely popular in **Scientific Machine Learning (e.g., PINNs)**. It is computationally expensive per step but converges in very few steps, making it ideal for small datasets requiring high precision (low loss).\n",
    "* **Muon (Momentumized Orthogonal)**: [GitHub / Paper](https://github.com/KellerJordan/Muon)\n",
    "    * **What it is**: An emerging optimizer designed for massive matrix-based models (like Transformers). It performs updates based on the **Spectral Norm** of the weights and orthogonalizes updates using Newton-Schulz iterations.\n",
    "    * **Use Case**: It is currently setting records for training efficiency in high-dimensional spaces, often converging faster than AdamW for large-scale runs.\n",
    "\n",
    "### 2. Key Optimizer Parameters\n",
    "Understanding these two arguments is crucial for tuning advanced optimizers:\n",
    "\n",
    "* **Weight Decay ($\\lambda$)**:\n",
    "    * **Concept**: A regularization term that adds a penalty $\\lambda \\|\\theta\\|^2$ to the loss.\n",
    "    * **Effect**: It forces the model weights to remain small (close to zero). This prevents overfitting and improves generalization, especially in over-parameterized models.\n",
    "* **Momentum ($\\beta$)**:\n",
    "    * **Concept**: Instead of updating parameters based solely on the current gradient $g_t$, we update based on a moving average of past gradients: $v_t = \\beta v_{t-1} + (1-\\beta)g_t$.\n",
    "    * **Effect**: It helps the optimizer build up speed in directions with consistent gradients and dampens oscillations in chaotic valleys. Think of it as a heavy ball rolling down a hill—it resists being thrown off course by small bumps.\n",
    "\n",
    "### 3. Learning Rate Schedulers\n",
    "A constant learning rate is often suboptimal. We typically use a **Scheduler** to adjust $\\eta$ dynamically.\n",
    "\n",
    "* **StepLR**: Decays the learning rate by a factor (e.g., 0.1) every $N$ epochs.\n",
    "* **CosineAnnealingLR**: Smoothly decreases the learning rate following a cosine curve from $\\eta_{max}$ to 0. This is the gold standard for modern training pipelines as it avoids sudden drops.\n",
    "* **Documentation**: [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMmu1Z855wMs"
   },
   "source": [
    "# 4. Training Loop, Evaluation, and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzpth_rCLU_T"
   },
   "source": [
    "| Exp ID | Activation | Weights? | Augmentation? | Architecture |\n",
    "| :--- | :--- | :---: | :---: | :--- |\n",
    "| **1** | Tanh | ❌ | ❌ | 3 layers / 128 |\n",
    "| **2** | ReLU | ❌ | ❌ | 3 layers / 128 |\n",
    "| **3** | GELu | ❌ | ❌ | 3 layers / 128 |\n",
    "| **4** | GELu | ✅ | ❌ | 3 layers / 128 |\n",
    "| **5** | GELu | ✅ | ✅ | 3 layers / 128 |\n",
    "| **6** | GELU | ✅ | ✅ | 6 layers / 256 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Mo8kqwFKZvawqNTgEwsPFNtsZlA4Rbxu"
    },
    "executionInfo": {
     "elapsed": 29568,
     "status": "ok",
     "timestamp": 1768939097474,
     "user": {
      "displayName": "Bohan Chen",
      "userId": "08649815860917755345"
     },
     "user_tz": 480
    },
    "id": "jPfcdicf2VrU",
    "outputId": "9b857ffe-fe16-4ee6-eaf2-b4eaaed781e3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "\n",
    "# ==========================================\n",
    "# 1. Class & Function Definitions\n",
    "# ==========================================\n",
    "\n",
    "def generate_data(n_samples=1000, sigma=0.1):\n",
    "    \"\"\"Generates synthetic data for the chirp signal regression task.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.rand(n_samples, 1) * 2 - 1\n",
    "    y_true = torch.sin(10 * torch.pi * x**2)\n",
    "    noise = torch.randn(y_true.shape) * sigma\n",
    "    y = y_true + noise\n",
    "    return x, y\n",
    "\n",
    "class ChirpDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Chirp signal with optional augmentation and weighting.\"\"\"\n",
    "    def __init__(self, data, target, augment=False, return_weights=False):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.augment = augment\n",
    "        self.return_weights = return_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.target[idx]\n",
    "        if self.augment:\n",
    "            if torch.rand(1).item() < 0.5: x = -x\n",
    "        if self.return_weights:\n",
    "            weight = 1 + 2 * torch.abs(x)**2\n",
    "            return x, y, weight\n",
    "        return x, y\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    \"\"\"A configurable Multi-Layer Perceptron (MLP) architecture.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_hidden_layers=3, activation=nn.Tanh):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(activation())\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    \"\"\"Computes Weighted Mean Squared Error.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target, weight=None):\n",
    "        loss = (prediction - target) ** 2\n",
    "        if weight is not None: loss = loss * weight\n",
    "        return loss.mean()\n",
    "\n",
    "def get_optimizer(model, opt_type='Adam', lr=1e-3, weight_decay=0.0, momentum=0.9):\n",
    "    \"\"\"Initializes the optimizer.\"\"\"\n",
    "    if opt_type == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif opt_type == 'AdamW':\n",
    "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif opt_type == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Interactive Training Pipeline\n",
    "# ==========================================\n",
    "\n",
    "# @title Training Configuration\n",
    "# @markdown ### 1. Dataset Settings\n",
    "N_SAMPLES = 2500 # @param {type:\"slider\", min:500, max:5000, step:100}\n",
    "NOISE_LEVEL = 0.1 # @param {type:\"number\"}\n",
    "USE_AUGMENTATION = False # @param {type:\"boolean\"}\n",
    "USE_WEIGHTS = True # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown ### 2. Model Architecture\n",
    "HIDDEN_DIM = 128 # @param {type:\"integer\"}\n",
    "NUM_LAYERS = 4 # @param {type:\"integer\"}\n",
    "ACTIVATION_TYPE = \"GELU\" # @param [\"Tanh\", \"ReLU\", \"GELU\"]\n",
    "\n",
    "# @markdown ### 3. Optimization\n",
    "OPTIMIZER_TYPE = \"AdamW\" # @param [\"Adam\", \"AdamW\", \"SGD\"]\n",
    "LEARNING_RATE = 0.002 # @param {type:\"number\"}\n",
    "WEIGHT_DECAY = 1e-4 # @param {type:\"number\"}\n",
    "MOMENTUM = 0.99 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown ### 4. Training Loop\n",
    "NUM_EPOCHS = 300 # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 2000 # @param {type:\"integer\"}\n",
    "TEST_INTERVAL = 10 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### 5. Visualization\n",
    "GENERATE_ANIMATION = True # @param {type:\"boolean\"}\n",
    "\n",
    "def run_experiment():\n",
    "    # --- A. Device Setup ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running on device: {device}\")\n",
    "\n",
    "    # --- B. Data Preparation ---\n",
    "    x_full, y_full = generate_data(n_samples=N_SAMPLES, sigma=NOISE_LEVEL)\n",
    "    dataset_full = TensorDataset(x_full, y_full)\n",
    "    train_size = int(0.8 * len(dataset_full))\n",
    "    test_size = len(dataset_full) - train_size\n",
    "    train_subset, test_subset = random_split(dataset_full, [train_size, test_size])\n",
    "\n",
    "    x_train, y_train = x_full[train_subset.indices], y_full[train_subset.indices]\n",
    "    x_test, y_test = x_full[test_subset.indices], y_full[test_subset.indices]\n",
    "\n",
    "    train_ds = ChirpDataset(x_train, y_train, augment=USE_AUGMENTATION, return_weights=USE_WEIGHTS)\n",
    "    test_ds = ChirpDataset(x_test, y_test, augment=False, return_weights=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(f\"Data Loaded: {len(train_ds)} Training samples, {len(test_ds)} Test samples.\")\n",
    "\n",
    "    # --- C. Model Initialization ---\n",
    "    act_map = {\"Tanh\": nn.Tanh, \"ReLU\": nn.ReLU, \"GELU\": nn.GELU}\n",
    "    model = FlexibleMLP(input_dim=1, output_dim=1,\n",
    "                        hidden_dim=HIDDEN_DIM, num_hidden_layers=NUM_LAYERS,\n",
    "                        activation=act_map[ACTIVATION_TYPE]).to(device)\n",
    "\n",
    "    # --- D. Optimizer & Loss ---\n",
    "    optimizer = get_optimizer(model, opt_type=OPTIMIZER_TYPE, lr=LEARNING_RATE,\n",
    "                              weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "    criterion = WeightedMSELoss()\n",
    "\n",
    "    # --- E. Training Loop ---\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    snapshots = [] # Store (epoch, y_pred) for animation\n",
    "\n",
    "    x_grid = torch.linspace(-1, 1, 1000).unsqueeze(1).to(device)\n",
    "    y_grid_true = torch.sin(10 * torch.pi * x_grid**2).cpu().numpy()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            if USE_WEIGHTS:\n",
    "                x, y, w = batch\n",
    "                x, y, w = x.to(device), y.to(device), w.to(device)\n",
    "            else:\n",
    "                x, y = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                w = None\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y, weight=w)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "\n",
    "        if (epoch + 1) % TEST_INTERVAL == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            test_losses = []\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    pred = model(x)\n",
    "                    loss = criterion(pred, y, weight=None)\n",
    "                    test_losses.append(loss.item())\n",
    "\n",
    "                # Only save snapshot if animation is requested (save memory/time)\n",
    "                if GENERATE_ANIMATION:\n",
    "                    y_grid_pred = model(x_grid).cpu().numpy()\n",
    "                    snapshots.append((epoch + 1, y_grid_pred))\n",
    "\n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "            test_loss_history.append((epoch + 1, avg_test_loss))\n",
    "\n",
    "            if (epoch + 1) % (NUM_EPOCHS // 5) == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Test MSE: {avg_test_loss:.6f}\")\n",
    "\n",
    "    # --- F. Static Visualization ---\n",
    "    print(\"\\nPlotting Final Results...\")\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "    # 1. Train Loss\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.plot(train_loss_history, label='Train Loss')\n",
    "    ax1.set_title('Training Convergence')\n",
    "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "\n",
    "    # 2. Test Error\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    test_epochs_static, test_errors_static = zip(*test_loss_history)\n",
    "    ax2.plot(test_epochs_static, test_errors_static, color='orange', label='Test MSE')\n",
    "    ax2.set_title('Test Set Error')\n",
    "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('MSE'); ax2.set_yscale('log')\n",
    "    ax2.legend()\n",
    "\n",
    "    # 3. Final Fit\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    ax3.scatter(x_test.cpu().numpy(), y_test.cpu().numpy(), s=5, color='gray', alpha=0.2)\n",
    "    ax3.plot(x_grid.cpu().numpy(), y_grid_true, 'g--', linewidth=1.5)\n",
    "    model.eval()\n",
    "    with torch.no_grad(): final_pred = model(x_grid).cpu().numpy()\n",
    "    ax3.plot(x_grid.cpu().numpy(), final_pred, 'r-', linewidth=2)\n",
    "    ax3.set_title(f'Final Fit (Test MSE: {test_errors_static[-1]:.4f})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- G. Animation Generation (Conditional) ---\n",
    "    if GENERATE_ANIMATION:\n",
    "        print(\"Generating Synchronized Animation...\")\n",
    "        fig_anim, (ax_fit, ax_train, ax_test) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "        # === Subplot 1: Curve Fitting ===\n",
    "        ax_fit.set_xlim(-1, 1)\n",
    "        ax_fit.set_ylim(y_full.min().item()-0.5, y_full.max().item()+0.5)\n",
    "        ax_fit.scatter(x_test.cpu().numpy(), y_test.cpu().numpy(), s=5, color='gray', alpha=0.2, label='Test Data')\n",
    "        ax_fit.plot(x_grid.cpu().numpy(), y_grid_true, 'g--', linewidth=1.5, label='True Function')\n",
    "        line_fit, = ax_fit.plot([], [], 'r-', linewidth=2, label='Prediction')\n",
    "        ax_fit.set_title(\"Function Approximation\")\n",
    "        ax_fit.legend(loc='upper right')\n",
    "\n",
    "        # === Subplot 2: Train Loss ===\n",
    "        ax_train.set_xlim(0, NUM_EPOCHS)\n",
    "        ax_train.set_ylim(min(train_loss_history)*0.9, max(train_loss_history)*1.1)\n",
    "        ax_train.set_yscale('log')\n",
    "        ax_train.set_xlabel('Epoch'); ax_train.set_ylabel('Loss')\n",
    "        ax_train.set_title('Training Loss')\n",
    "        line_train, = ax_train.plot([], [], 'b-', label='Train Loss')\n",
    "        point_train, = ax_train.plot([], [], 'bo', markersize=8)\n",
    "\n",
    "        # === Subplot 3: Test Error ===\n",
    "        ax_test.set_xlim(0, NUM_EPOCHS)\n",
    "        ax_test.set_ylim(min(test_errors_static)*0.9, max(test_errors_static)*1.1)\n",
    "        ax_test.set_yscale('log')\n",
    "        ax_test.set_xlabel('Epoch'); ax_test.set_ylabel('MSE')\n",
    "        ax_test.set_title('Test Error')\n",
    "        line_test, = ax_test.plot([], [], 'orange', label='Test MSE')\n",
    "        point_test, = ax_test.plot([], [], 'o', color='darkorange', markersize=8)\n",
    "\n",
    "        def init():\n",
    "            line_fit.set_data([], [])\n",
    "            line_train.set_data([], [])\n",
    "            point_train.set_data([], [])\n",
    "            line_test.set_data([], [])\n",
    "            point_test.set_data([], [])\n",
    "            return line_fit, line_train, point_train, line_test, point_test\n",
    "\n",
    "        def animate(i):\n",
    "            epoch, y_pred = snapshots[i]\n",
    "\n",
    "            # 1. Update Fit Curve\n",
    "            line_fit.set_data(x_grid.cpu().numpy(), y_pred)\n",
    "\n",
    "            # 2. Update Train Loss\n",
    "            current_train_loss = train_loss_history[:epoch]\n",
    "            line_train.set_data(range(1, epoch + 1), current_train_loss)\n",
    "            if len(current_train_loss) > 0:\n",
    "                point_train.set_data([epoch], [current_train_loss[-1]])\n",
    "\n",
    "            # 3. Update Test Loss\n",
    "            current_test_history = test_loss_history[:i+1]\n",
    "            t_epochs, t_errors = zip(*current_test_history)\n",
    "            line_test.set_data(t_epochs, t_errors)\n",
    "            point_test.set_data([t_epochs[-1]], [t_errors[-1]])\n",
    "\n",
    "            fig_anim.suptitle(f'Training Progress: Epoch {epoch}/{NUM_EPOCHS}', fontsize=14)\n",
    "            return line_fit, line_train, point_train, line_test, point_test\n",
    "\n",
    "        anim = animation.FuncAnimation(fig_anim, animate, init_func=init,\n",
    "                                    frames=len(snapshots), interval=100, blit=False)\n",
    "\n",
    "        plt.close(fig_anim)\n",
    "        return HTML(anim.to_jshtml())\n",
    "    else:\n",
    "        print(\"Animation skipped.\")\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKBQi0HmMBMW"
   },
   "source": [
    "# 5. Beyond 1D: Visualizing High-Dimensional Results\n",
    "\n",
    "In our toy example, we could easily plot the model predictions because the input $x$ was 1-dimensional. However, in real-world scientific problems, your input data often lives in a high-dimensional space (e.g., $x \\in \\mathbb{R}^{1000}$).\n",
    "\n",
    "How do we visualize \"correctness\" or \"learned features\" when we cannot simply draw a line?\n",
    "\n",
    "## 5.1 Dimensionality Reduction Techniques\n",
    "To visualize how a neural network groups or processes data, we project high-dimensional vectors into 2D or 3D.\n",
    "\n",
    "### 1. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "* **What it does**: It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the low-dimensional embedding and the high-dimensional data.\n",
    "* **Strength**: Excellent for revealing **local structure** and clusters (e.g., visualizing if a network has separated different classes of data in its latent space).\n",
    "* **Weakness**: It is computationally expensive and does not preserve global distances well.\n",
    "\n",
    "### 2. UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "* **What it does**: A topological data analysis method that constructs a high-dimensional graph representation of the data and optimizes a low-dimensional graph to be structurally similar.\n",
    "* **Strength**: The modern standard. It is **faster** than t-SNE and better at preserving **global structure** (relationships between clusters) while still maintaining local details.\n",
    "* **Library**: [UMAP Documentation](https://umap-learn.readthedocs.io/en/latest/)\n",
    "\n",
    "## 5.2 Diagnostic Plots for Regression\n",
    "For regression tasks (like our Chirp signal or Inverse Problems), simply reporting the MSE is not enough. You should use:\n",
    "\n",
    "### 1. Parity Plot (Predicted vs. Actual)\n",
    "* **X-axis**: Ground Truth $y$.\n",
    "* **Y-axis**: Predicted $\\hat{y}$.\n",
    "* **Goal**: All points should fall on the diagonal $y=x$ line. Deviations show systematic bias.\n",
    "\n",
    "### 2. Residual Plot\n",
    "\n",
    "* **X-axis**: Predicted value $\\hat{y}$ (or Input $x$).\n",
    "* **Y-axis**: Residuals ($y - \\hat{y}$).\n",
    "* **Goal**: The points should be randomly scattered around 0.\n",
    "* **Warning Signs**:\n",
    "    * **Funnel Shape**: Indicates Heteroscedasticity (error varies with magnitude).\n",
    "    * **Curve Pattern**: Indicates the model missed a non-linear trend (Underfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4491,
     "status": "ok",
     "timestamp": 1768939134888,
     "user": {
      "displayName": "Bohan Chen",
      "userId": "08649815860917755345"
     },
     "user_tz": 480
    },
    "id": "ZQzZL8qxcWO1",
    "outputId": "01932e5b-b728-4bdc-9b4a-201e93fd8a46"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 109746,
     "status": "ok",
     "timestamp": 1768939245775,
     "user": {
      "displayName": "Bohan Chen",
      "userId": "08649815860917755345"
     },
     "user_tz": 480
    },
    "id": "U5A_F4Vd6QCJ",
    "outputId": "a0f7b3bf-ed4f-4740-d948-6a879e176363"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import umap\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. Helper Functions & Model\n",
    "# ==========================================\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flat_dim = 16 * 7 * 7\n",
    "        self.fc = nn.Linear(self.flat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.get_features(x)\n",
    "        return self.fc(features)\n",
    "\n",
    "    def get_features(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.flat_dim)\n",
    "        return x\n",
    "\n",
    "def get_optimizer(model, lr=0.01):\n",
    "    return optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_stratified_subset(dataset, fraction, seed=42):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    if isinstance(dataset, Subset):\n",
    "        full_targets = dataset.dataset.targets\n",
    "        current_indices = dataset.indices\n",
    "        targets = full_targets[current_indices]\n",
    "    else:\n",
    "        targets = dataset.targets\n",
    "\n",
    "    if isinstance(targets, list):\n",
    "        targets = torch.tensor(targets)\n",
    "\n",
    "    indices = []\n",
    "    num_classes = 10\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        class_indices = (targets == i).nonzero(as_tuple=True)[0]\n",
    "        n_total = len(class_indices)\n",
    "        n_keep = int(n_total * fraction)\n",
    "        if n_keep < 1: n_keep = 1\n",
    "\n",
    "        perm = torch.randperm(n_total, generator=g)\n",
    "        selected = class_indices[perm[:n_keep]]\n",
    "        indices.append(selected)\n",
    "\n",
    "    all_indices = torch.cat(indices)\n",
    "    return Subset(dataset, all_indices)\n",
    "\n",
    "def get_balanced_subset_for_umap(dataset, num_samples=1000):\n",
    "    if isinstance(dataset, Subset):\n",
    "        full_targets = dataset.dataset.targets\n",
    "        if isinstance(full_targets, list): full_targets = torch.tensor(full_targets)\n",
    "        subset_indices = dataset.indices\n",
    "        targets = full_targets[subset_indices]\n",
    "    else:\n",
    "        targets = dataset.targets\n",
    "        if isinstance(targets, list): targets = torch.tensor(targets)\n",
    "\n",
    "    indices = []\n",
    "    class_counts = {i: 0 for i in range(10)}\n",
    "    samples_per_class = num_samples // 10\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        label = targets[i].item()\n",
    "        if class_counts[label] < samples_per_class:\n",
    "            indices.append(i)\n",
    "            class_counts[label] += 1\n",
    "        if all(c >= samples_per_class for c in class_counts.values()):\n",
    "            break\n",
    "\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Interactive Pipeline\n",
    "# ==========================================\n",
    "\n",
    "# @title MNIST Training Configuration\n",
    "DATA_FRACTION = 0.3 # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "BATCH_SIZE = 2048 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.01 # @param {type:\"number\"}\n",
    "NUM_EPOCHS = 10 # @param {type:\"integer\"}\n",
    "UMAP_NUM_POINTS = 1000 # @param {type:\"slider\", min:500, max:2000, step:100}\n",
    "GENERATE_ANIMATION = True # @param {type:\"boolean\"}\n",
    "\n",
    "def run_mnist_fast_experiment():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running on device: {device}\")\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    print(\"Loading MNIST...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    full_train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    full_test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    print(f\"Applying Stratified Sampling: Keeping {DATA_FRACTION*100:.0f}% of data.\")\n",
    "    train_data = get_stratified_subset(full_train_data, fraction=DATA_FRACTION)\n",
    "    test_data = get_stratified_subset(full_test_data, fraction=DATA_FRACTION)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"Selecting balanced subset for UMAP...\")\n",
    "    umap_n = min(UMAP_NUM_POINTS, len(test_data))\n",
    "    viz_subset = get_balanced_subset_for_umap(test_data, num_samples=umap_n)\n",
    "    viz_loader = DataLoader(viz_subset, batch_size=umap_n, shuffle=False)\n",
    "\n",
    "    viz_images, viz_labels = next(iter(viz_loader))\n",
    "    viz_images = viz_images.to(device)\n",
    "    viz_labels_np = viz_labels.numpy()\n",
    "\n",
    "    # --- Model Init ---\n",
    "    model = SmallCNN().to(device)\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Model Initialized. Total Trainable Parameters: {num_params:,}\")\n",
    "\n",
    "    optimizer = get_optimizer(model, lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    snapshots = []\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "\n",
    "    # Snapshot Epoch 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        feats_0 = model.get_features(viz_images).cpu().numpy()\n",
    "        snapshots.append((0, feats_0))\n",
    "        test_accs.append(10.0)\n",
    "        train_losses.append(2.30)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "\n",
    "        acc = 100. * correct / len(test_loader.dataset)\n",
    "        test_accs.append(acc)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_feats = model.get_features(viz_images).cpu().numpy()\n",
    "            snapshots.append((epoch + 1, current_feats))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Loss: {avg_loss:.4f} | Test Acc: {acc:.2f}%\")\n",
    "\n",
    "    # --- Visualization ---\n",
    "    if GENERATE_ANIMATION:\n",
    "        print(\"\\nComputing UMAP embeddings (n_jobs=1)...\")\n",
    "        umap_embeddings = []\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2,\n",
    "                            random_state=42, n_jobs=1)\n",
    "\n",
    "        for epoch, feats in snapshots:\n",
    "            embedding = reducer.fit_transform(feats)\n",
    "            umap_embeddings.append(embedding)\n",
    "\n",
    "        print(\"Generating Animation...\")\n",
    "        fig = plt.figure(figsize=(18, 6))\n",
    "        gs = fig.add_gridspec(1, 3)\n",
    "\n",
    "        # Plot 1 & 2\n",
    "        ax_loss = fig.add_subplot(gs[0, 0])\n",
    "        ax_loss.set_xlim(0, NUM_EPOCHS); ax_loss.set_ylim(0, max(train_losses)*1.1)\n",
    "        ax_loss.set_xlabel(\"Epoch\"); ax_loss.set_ylabel(\"Loss\"); ax_loss.set_title(\"Training Loss\")\n",
    "        line_loss, = ax_loss.plot([], [], 'b-', lw=2)\n",
    "        dot_loss, = ax_loss.plot([], [], 'bo', markersize=8)\n",
    "\n",
    "        ax_acc = fig.add_subplot(gs[0, 1])\n",
    "        ax_acc.set_xlim(0, NUM_EPOCHS); ax_acc.set_ylim(0, 100)\n",
    "        ax_acc.set_xlabel(\"Epoch\"); ax_acc.set_ylabel(\"Accuracy (%)\"); ax_acc.set_title(\"Test Accuracy\")\n",
    "        line_acc, = ax_acc.plot([], [], 'orange', lw=2)\n",
    "        dot_acc, = ax_acc.plot([], [], 'o', color='darkorange', markersize=8)\n",
    "\n",
    "        # Plot 3: UMAP (Fix: Explicit vmin/vmax)\n",
    "        ax_umap = fig.add_subplot(gs[0, 2])\n",
    "        cmap = plt.get_cmap('tab10', 10)\n",
    "\n",
    "        # --- FIX HERE: Added vmin=0, vmax=9 ---\n",
    "        scatter = ax_umap.scatter([], [], c=[], cmap=cmap, s=15, alpha=0.7, vmin=0, vmax=9)\n",
    "\n",
    "        ax_umap.set_title(\"Latent Space (UMAP)\")\n",
    "        ax_umap.axis('off')\n",
    "        cbar = plt.colorbar(scatter, ax=ax_umap, ticks=range(10))\n",
    "        cbar.set_label('Digit Class')\n",
    "\n",
    "        def init():\n",
    "            line_loss.set_data([], []); dot_loss.set_data([], [])\n",
    "            line_acc.set_data([], []); dot_acc.set_data([], [])\n",
    "            scatter.set_offsets(np.empty((0, 2)))\n",
    "            return line_loss, dot_loss, line_acc, dot_acc, scatter\n",
    "\n",
    "        def animate(i):\n",
    "            embedding = umap_embeddings[i]\n",
    "            x_min, x_max = embedding[:, 0].min(), embedding[:, 0].max()\n",
    "            y_min, y_max = embedding[:, 1].min(), embedding[:, 1].max()\n",
    "            ax_umap.set_xlim(x_min - 2, x_max + 2)\n",
    "            ax_umap.set_ylim(y_min - 2, y_max + 2)\n",
    "\n",
    "            scatter.set_offsets(embedding)\n",
    "            scatter.set_array(viz_labels_np) # Set colors (0-9 integers)\n",
    "\n",
    "            curr_epoch = snapshots[i][0]\n",
    "\n",
    "            # Curves\n",
    "            current_losses = train_losses[:i+1]\n",
    "            epochs_range = range(0, i+1)\n",
    "            line_loss.set_data(epochs_range, current_losses)\n",
    "            if len(current_losses) > 0: dot_loss.set_data([curr_epoch], [current_losses[-1]])\n",
    "\n",
    "            current_accs = test_accs[:i+1]\n",
    "            line_acc.set_data(epochs_range, current_accs)\n",
    "            if len(current_accs) > 0: dot_acc.set_data([curr_epoch], [current_accs[-1]])\n",
    "\n",
    "            title = \"Pre-training\" if curr_epoch == 0 else \"Training\"\n",
    "            fig.suptitle(f\"{title} - Epoch: {curr_epoch}\", fontsize=16)\n",
    "            return line_loss, dot_loss, line_acc, dot_acc, scatter\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                       frames=len(snapshots), interval=800, blit=False)\n",
    "        plt.close(fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "\n",
    "    else:\n",
    "        # Static Plots\n",
    "        print(\"\\nPlotting Static Results...\")\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        ax1.plot(train_losses); ax1.set_title('Training Loss')\n",
    "        ax2.plot(test_accs, color='orange'); ax2.set_title('Test Accuracy')\n",
    "\n",
    "        final_feats = snapshots[-1][1]\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2,\n",
    "                            random_state=42, n_jobs=1)\n",
    "        embedding = reducer.fit_transform(final_feats)\n",
    "\n",
    "        # Static plot also needs vmin/vmax just to be safe, though c is passed directly\n",
    "        scatter = ax3.scatter(embedding[:, 0], embedding[:, 1], c=viz_labels_np,\n",
    "                              cmap='tab10', s=10, vmin=0, vmax=9)\n",
    "        ax3.set_title(\"Final UMAP\"); ax3.axis('off')\n",
    "        plt.colorbar(scatter, ax=ax3, ticks=range(10))\n",
    "        plt.show()\n",
    "\n",
    "run_mnist_fast_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw1OMuDYMe28"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN2Qwavv6MGcNDBoqNLq32J",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
