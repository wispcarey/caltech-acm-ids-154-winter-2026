{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiblxgMn-T4x"
      },
      "source": [
        "# Session 6: Machine Learning for Data Assimilation\n",
        "\n",
        "In this session, we explore the integration of machine learning techniques within data assimilation frameworks.\n",
        "\n",
        "We will discuss the following three key topics:\n",
        "\n",
        "1.  Physical Parameter Estimation\n",
        "2.  ML + EnKF\n",
        "3.  Likelihood-Free Ensemble Transport Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBUNh2Yq-1iP"
      },
      "source": [
        "##1. Physical Parameter Estimation\n",
        "Infer unknown physical parameters governing a dynamical system from sparse and noisy observations.\n",
        "\n",
        "### 1.1 Problem Setting: Joint Estimation of $(\\sigma, \\rho)$\n",
        "We consider the inverse problem of estimating the parameters $\\theta = (\\sigma, \\rho)$ in the Lorenz-63 system given a partial observation trajectory. The system dynamics are governed by$$\\begin{aligned}\n",
        "\\dot x &= \\sigma(y-x), \\\\\n",
        "\\dot y &= x(\\rho-z)-y, \\\\\n",
        "\\dot z &= xy-\\beta z.\n",
        "\\end{aligned}$$The ground truth parameters are set to the standard chaotic regime $\\theta^* = (\\sigma^*, \\rho^*) = (10, 28)$ with fixed $\\beta = 8/3$. The system is integrated using a Runge-Kutta 4th order scheme with a time step $dt=0.03$. Observations are restricted to the first state component $x$ at discrete intervals $\\Delta t_{\\text{obs}} = 0.15$. The dataset $Y^\\dagger_{50}$ consists of 50 noisy measurements generated from the ground truth trajectory$$Y^\\dagger_{50} = \\{y^\\dagger_1, \\ldots, y^\\dagger_{50}\\}, \\quad y^\\dagger_j = x(t_j) + \\eta_j, \\quad \\eta_j \\sim \\mathcal{N}(0, 1.0).$$We assume a Gaussian prior centered near the chaotic attractor but biased from the truth to challenge the assimilation process$$\\theta \\sim \\mathcal{N}\\left( \\begin{bmatrix} 9.0 \\\\ 30.0 \\end{bmatrix}, \\begin{bmatrix} 4.0 & 0 \\\\ 0 & 9.0 \\end{bmatrix} \\right).$$\n",
        "\n",
        "**Note regarding Model Error:** We assume a perfect model structure (deterministic ODE) where uncertainty arises only from unknown parameters and noisy data.\n",
        "\n",
        "**Ground Truth by MCMC:** Given a candidate parameter set $\\theta = (\\sigma, \\rho)$ and a fixed initial condition $x_0$, the model generates a deterministic trajectory $v(t; \\theta)$. Since the observation noise is independent and Gaussian, the likelihood of observing the dataset $Y^\\dagger_{50}$ is:\n",
        "$$p(Y^\\dagger_{50} | \\theta) \\propto \\exp\\left( -\\frac{1}{2\\sigma_{\\text{obs}}^2} \\sum_{j=1}^{50} \\|y^\\dagger_j - H(v(t_j; \\theta))\\|^2 \\right)$$\n",
        "Based on this likelihood, we use the MCMC algorithm (Metropolis-Hastings) to sampling from the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "Ekt8U-zs-Qud",
        "outputId": "06da8b2a-5cfa-451f-dd88-eebd94d78d91"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, lax\n",
        "from jax.scipy.stats import norm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# 1. Colab Form Configuration (Compact)\n",
        "# ==========================================\n",
        "# @title Experiment Parameters\n",
        "\n",
        "# @markdown **Ground Truth**\n",
        "true_sigma = 10.0  # @param {type:\"number\"}\n",
        "true_rho = 28.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **Prior Beliefs (Mean / Std)**\n",
        "prior_mu_sigma = 9.0  # @param {type:\"number\"}\n",
        "prior_std_sigma = 2.0  # @param {type:\"number\"}\n",
        "prior_mu_rho = 30.0  # @param {type:\"number\"}\n",
        "prior_std_rho = 3.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **MCMC Settings**\n",
        "obs_num = 50  # @param {type:\"number\"}\n",
        "step_size = 0.15  # @param {type:\"number\"}\n",
        "\n",
        "# Constants\n",
        "DT_INT = 0.03\n",
        "FIXED_OBS_FREQ = 5\n",
        "SIGMA_OBS = 1.0\n",
        "N_SAMPLES_FIXED = 50000\n",
        "\n",
        "# ==========================================\n",
        "# 2. System Dynamics & Solvers (JAX)\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def lorenz63_dynamics(state, params):\n",
        "    \"\"\"\n",
        "    Computes Lorenz-63 time derivative.\n",
        "    Input: state (x, y, z), params (sigma, rho, beta)\n",
        "    Output: derivative (dx, dy, dz)\n",
        "    \"\"\"\n",
        "    x, y, z = state\n",
        "    sigma, rho, beta = params\n",
        "    dx = sigma * (y - x)\n",
        "    dy = x * (rho - z) - y\n",
        "    dz = x * y - beta * z\n",
        "    return jnp.array([dx, dy, dz])\n",
        "\n",
        "@jit\n",
        "def rk4_step(state, params, dt):\n",
        "    \"\"\"\n",
        "    Performs a single RK4 integration step.\n",
        "    Input: current state, params, dt\n",
        "    Output: next state\n",
        "    \"\"\"\n",
        "    k1 = lorenz63_dynamics(state, params)\n",
        "    k2 = lorenz63_dynamics(state + 0.5 * dt * k1, params)\n",
        "    k3 = lorenz63_dynamics(state + 0.5 * dt * k2, params)\n",
        "    k4 = lorenz63_dynamics(state + dt * k3, params)\n",
        "    return state + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "@partial(jit, static_argnums=(4,))\n",
        "def generate_truth(x0, sigma, rho, beta, total_steps):\n",
        "    \"\"\"\n",
        "    Generates the ground truth trajectory.\n",
        "    Input: x0, physical params, total_steps\n",
        "    Output: Full state trajectory (N+1, 3)\n",
        "    \"\"\"\n",
        "    params = jnp.array([sigma, rho, beta])\n",
        "    def step_fn(carry, _):\n",
        "        next_state = rk4_step(carry, params, DT_INT)\n",
        "        return next_state, next_state\n",
        "    _, traj = lax.scan(step_fn, x0, None, length=total_steps)\n",
        "    return jnp.concatenate([x0[None, :], traj], axis=0)\n",
        "\n",
        "# ==========================================\n",
        "# 3. MCMC Engine\n",
        "# ==========================================\n",
        "\n",
        "def log_posterior_point(params, y_obs, obs_idx_jax, prior_mu, prior_std, x0_fixed, sim_length):\n",
        "    \"\"\"\n",
        "    Calculates unnormalized log posterior for a parameter set.\n",
        "    Input: params (sigma, rho), observations, priors, fixed x0\n",
        "    Output: log_probability scalar\n",
        "    \"\"\"\n",
        "    sigma, rho = params\n",
        "    # Prior\n",
        "    lp_sigma = norm.logpdf(sigma, loc=prior_mu[0], scale=prior_std[0])\n",
        "    lp_rho   = norm.logpdf(rho,   loc=prior_mu[1], scale=prior_std[1])\n",
        "\n",
        "    # Likelihood (Simulation)\n",
        "    def sim_func(carry, _):\n",
        "        ns = rk4_step(carry, jnp.array([sigma, rho, 8.0/3.0]), DT_INT)\n",
        "        return ns, ns\n",
        "\n",
        "    _, traj_internal = lax.scan(sim_func, x0_fixed, None, length=sim_length)\n",
        "    full_traj = jnp.concatenate([x0_fixed[None, :], traj_internal], axis=0)\n",
        "\n",
        "    y_model = full_traj[obs_idx_jax, 0]\n",
        "    sse = jnp.sum((y_obs - y_model)**2)\n",
        "    log_lik = -0.5 * sse / (SIGMA_OBS**2)\n",
        "\n",
        "    return lp_sigma + lp_rho + log_lik\n",
        "\n",
        "@partial(jit, static_argnums=(6,))\n",
        "def run_mcmc_engine(\n",
        "    key, start_params, y_obs, obs_idx_jax, prior_mu, prior_std,\n",
        "    sim_length, x0_fixed, rw_step_size\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the Metropolis-Hastings MCMC chain.\n",
        "    Input: PRNGKey, start params, data, priors, simulation specs\n",
        "    Output: (chain samples, acceptance count)\n",
        "    \"\"\"\n",
        "    init_lp = log_posterior_point(start_params, y_obs, obs_idx_jax, prior_mu, prior_std, x0_fixed, sim_length)\n",
        "\n",
        "    def mcmc_kernel(carry, key_step):\n",
        "        curr_params, curr_lp, acc = carry\n",
        "        noise = random.normal(key_step, (2,)) * rw_step_size\n",
        "        prop_params = curr_params + noise\n",
        "        prop_lp = log_posterior_point(prop_params, y_obs, obs_idx_jax, prior_mu, prior_std, x0_fixed, sim_length)\n",
        "\n",
        "        log_ratio = prop_lp - curr_lp\n",
        "        accept = jnp.log(random.uniform(key_step)) < log_ratio\n",
        "\n",
        "        new_params = jnp.where(accept, prop_params, curr_params)\n",
        "        new_lp = jnp.where(accept, prop_lp, curr_lp)\n",
        "        new_acc = acc + accept.astype(int)\n",
        "        return (new_params, new_lp, new_acc), new_params\n",
        "\n",
        "    keys = random.split(key, N_SAMPLES_FIXED)\n",
        "    (_, _, total_acc), chain = lax.scan(mcmc_kernel, (start_params, init_lp, 0), keys)\n",
        "    return chain, total_acc\n",
        "\n",
        "# ==========================================\n",
        "# 4. Orchestration & Plotting\n",
        "# ==========================================\n",
        "\n",
        "def plot_results(chain, p_mu_s, p_std_s, p_mu_r, p_std_r, t_sigma, t_rho, acc, n_obs):\n",
        "    \"\"\"\n",
        "    Visualizes the posterior density and ground truth.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Prior Contours\n",
        "    x_range = np.linspace(p_mu_s - 3*p_std_s, p_mu_s + 3*p_std_s, 100)\n",
        "    y_range = np.linspace(p_mu_r - 3*p_std_r, p_mu_r + 3*p_std_r, 100)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = (1/(2*np.pi*p_std_s*p_std_r)) * np.exp(-0.5*(((X-p_mu_s)/p_std_s)**2 + ((Y-p_mu_r)/p_std_r)**2))\n",
        "\n",
        "    custom_levels = Z.max() * np.array([0.01, 0.05, 0.1, 0.3, 0.6, 0.8, 0.9, 0.95, 0.99])\n",
        "    plt.contour(X, Y, Z, levels=custom_levels, colors='orange', linestyles='--', alpha=0.6)\n",
        "\n",
        "    # Posterior Scatter (Downsampled)\n",
        "    step = max(1, len(chain) // 2000)\n",
        "    plot_chain = chain[::step]\n",
        "    plt.scatter(plot_chain[:, 0], plot_chain[:, 1],\n",
        "                color='#1f77b4', s=5, alpha=0.3, label='Posterior (Samples)')\n",
        "\n",
        "    # Markers\n",
        "    plt.scatter(t_sigma, t_rho, c='red', marker='*', s=300, zorder=10, label=f'Truth ({t_sigma:.1f}, {t_rho:.1f})')\n",
        "    plt.scatter(p_mu_s, p_mu_r, c='orange', marker='x', s=150, zorder=10, lw=3, label='Prior Mean')\n",
        "\n",
        "    plt.title(f\"Joint Estimation (MCMC Steps: {N_SAMPLES_FIXED}, Accept: {acc:.1%})\\nObs: {n_obs} points\")\n",
        "    plt.xlabel('Sigma'); plt.ylabel('Rho')\n",
        "\n",
        "    # Dynamic Zoom\n",
        "    all_x = [t_sigma, p_mu_s, chain[:,0].mean()]\n",
        "    all_y = [t_rho, p_mu_r, chain[:,1].mean()]\n",
        "    center_x, center_y = np.mean(all_x), np.mean(all_y)\n",
        "    span_x = max(np.max(all_x) - np.min(all_x), 4.0) * 0.8 + 2.0\n",
        "    span_y = max(np.max(all_y) - np.min(all_y), 10.0) * 0.8 + 5.0\n",
        "\n",
        "    plt.xlim(center_x - span_x, center_x + span_x)\n",
        "    plt.ylim(center_y - span_y, center_y + span_y)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "def run_experiment_logic():\n",
        "    \"\"\"\n",
        "    Main driver function using global form variables.\n",
        "    Executes simulation, MCMC, data saving, and plotting.\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 1. Generate Truth\n",
        "    total_steps = int(obs_num * FIXED_OBS_FREQ)\n",
        "    key = random.PRNGKey(42)\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "\n",
        "    # Burn-in for initial condition\n",
        "    x0 = random.normal(k1, (3,))\n",
        "    for _ in range(100):\n",
        "        x0 = rk4_step(x0, jnp.array([true_sigma, true_rho, 8.0/3.0]), DT_INT)\n",
        "\n",
        "    truth_traj = generate_truth(x0, true_sigma, true_rho, 8.0/3.0, total_steps)\n",
        "\n",
        "    # 2. Observations\n",
        "    obs_indices = np.arange(0, total_steps + 1, FIXED_OBS_FREQ)\n",
        "    obs_indices_jax = jnp.array(obs_indices)\n",
        "    y_truth = truth_traj[obs_indices_jax, 0]\n",
        "    y_obs = y_truth + random.normal(k2, y_truth.shape) * SIGMA_OBS\n",
        "\n",
        "    # 3. MCMC Setup\n",
        "    prior_mu = jnp.array([prior_mu_sigma, prior_mu_rho])\n",
        "    prior_std = jnp.array([prior_std_sigma, prior_std_rho])\n",
        "    rw_step_val = jnp.array([step_size/2.0, step_size])\n",
        "    sim_len_int = int(obs_indices[-1])\n",
        "\n",
        "    # 4. Run Chain\n",
        "    # Run the compiled JAX MCMC engine\n",
        "    chain, total_acc = run_mcmc_engine(\n",
        "        k3, prior_mu, y_obs, obs_indices_jax,\n",
        "        prior_mu, prior_std, sim_len_int, x0, rw_step_val\n",
        "    )\n",
        "\n",
        "    # 5. Process, Save & Plot\n",
        "    chain = np.array(chain)\n",
        "    t_calc = time.time() - t0\n",
        "    burn_in = int(N_SAMPLES_FIXED * 0.1)\n",
        "\n",
        "    # Filter burn-in to get effective samples\n",
        "    clean_chain = chain[burn_in:]\n",
        "    acc_rate = total_acc / N_SAMPLES_FIXED\n",
        "\n",
        "    # Save to file\n",
        "    file_path = 'mcmc_chain.npy'\n",
        "    np.save(file_path, clean_chain)\n",
        "\n",
        "    print(f\"JAX Calculation: {t_calc:.3f}s\")\n",
        "    print(f\"Effective samples (post burn-in): {clean_chain.shape[0]}\")\n",
        "    print(f\"Chain saved to: {file_path}\")\n",
        "\n",
        "    plot_results(clean_chain, prior_mu_sigma, prior_std_sigma, prior_mu_rho, prior_std_rho,\n",
        "                 true_sigma, true_rho, acc_rate, obs_num)\n",
        "\n",
        "# Execute\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment_logic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDPQGC84G0qQ"
      },
      "source": [
        "### 1.2 State Augmentation with EnKF\n",
        "\n",
        "In this section, we solve the inverse problem using **State Augmentation** within the Ensemble Kalman Filter (EnKF).\n",
        "\n",
        "### 1.2.1 The Concept\n",
        "We treat the unknown parameters $\\theta = (\\sigma, \\rho)$ as dynamical variables with zero time derivatives (subject to small random perturbations). We augment the physical state $\\mathbf{v} = [x, y, z]^T$ with these parameters to form an augmented state vector $\\mathbf{u} \\in \\mathbb{R}^5$:\n",
        "\n",
        "$$\n",
        "\\mathbf{u}_j = [\\mathbf{v}_j, \\theta_j ]^\\top = [x_j, y_j, z_j, \\sigma_j, \\rho_j]^\\top\n",
        "$$\n",
        "\n",
        "### 1.2.2 Dynamics and Jittering\n",
        "The evolution of this augmented system combines the physical ODE solver (Runge-Kutta 4) and a random walk model for the parameters (to prevent variance collapse):\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{v}_{k+1} &= \\text{RK4}(\\mathbf{v}_k, \\theta_k, \\Delta t) \\\\\n",
        "\\theta_{k+1} &= \\theta_k + \\xi_k, \\quad \\xi_k \\sim \\mathcal{N}(0, \\Sigma_{\\text{jitter}})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### 1.2.3 Estimation via EnKF\n",
        "We apply the standard Ensemble Kalman Filter to this augmented system. By updating the full augmented ensemble $\\mathbf{u}^{(i)}$ using observations of only the first component $x$, the filter naturally adjusts the unobserved parameters $\\theta$.\n",
        "\n",
        "The **final estimation** is derived from the marginal distribution of the parameter dimensions ($\\sigma, \\rho$) within the ensemble at the final time step $T$. We interpret the ensemble members $\\{\\theta_T^{(1)}, \\dots, \\theta_T^{(N)}\\}$ as samples from the approximate posterior distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "wu3f53FICFBI",
        "outputId": "fb6ca6ed-218b-4189-a5f0-12dde0a3ecf9"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, vmap, lax\n",
        "from jax.scipy.stats import norm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "from matplotlib.lines import Line2D\n",
        "from functools import partial\n",
        "import time\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. Colab Form Configuration\n",
        "# ==========================================\n",
        "# @title EnKF Experiment Parameters\n",
        "\n",
        "# @markdown **Ground Truth**\n",
        "true_sigma = 10.0  # @param {type:\"number\"}\n",
        "true_rho = 28.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **Prior Beliefs**\n",
        "prior_mu_sigma = 9.0  # @param {type:\"number\"}\n",
        "prior_std_sigma = 2.0  # @param {type:\"number\"}\n",
        "prior_mu_rho = 30.0  # @param {type:\"number\"}\n",
        "prior_std_rho = 5.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **EnKF Settings**\n",
        "obs_num = 50  # @param {type:\"number\"}\n",
        "ensemble_size = 200  # @param {type:\"number\"}\n",
        "step_size = 0.15 # @param {type:\"number\"}\n",
        "\n",
        "# Constants\n",
        "DT_INT = 0.03\n",
        "FIXED_OBS_FREQ = 5\n",
        "SIGMA_OBS = 1.0\n",
        "\n",
        "# ==========================================\n",
        "# 2. System Dynamics (JAX)\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def lorenz63_dynamics(state, params):\n",
        "    x, y, z = state\n",
        "    sigma, rho, beta = params\n",
        "    dx = sigma * (y - x)\n",
        "    dy = x * (rho - z) - y\n",
        "    dz = x * y - beta * z\n",
        "    return jnp.array([dx, dy, dz])\n",
        "\n",
        "@jit\n",
        "def rk4_step(state, params, dt):\n",
        "    k1 = lorenz63_dynamics(state, params)\n",
        "    k2 = lorenz63_dynamics(state + 0.5 * dt * k1, params)\n",
        "    k3 = lorenz63_dynamics(state + 0.5 * dt * k2, params)\n",
        "    k4 = lorenz63_dynamics(state + dt * k3, params)\n",
        "    return state + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "# Generator for Truth\n",
        "@partial(jit, static_argnums=(4,))\n",
        "def generate_truth(x0, sigma, rho, beta, total_steps):\n",
        "    params = jnp.array([sigma, rho, beta])\n",
        "    def step_fn(carry, _):\n",
        "        next_state = rk4_step(carry, params, DT_INT)\n",
        "        return next_state, next_state\n",
        "    _, traj = lax.scan(step_fn, x0, None, length=total_steps)\n",
        "    return jnp.concatenate([x0[None, :], traj], axis=0)\n",
        "\n",
        "# ==========================================\n",
        "# 3. EnKF Implementation\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def enkf_update(ensemble, y_obs, R_noise_std):\n",
        "    \"\"\"Standard Stochastic EnKF Update Step (Augmented State).\"\"\"\n",
        "    N = ensemble.shape[0]\n",
        "    ens_mean = jnp.mean(ensemble, axis=0)\n",
        "    A = ensemble - ens_mean\n",
        "    HX = ensemble[:, 0:1] # Observe x\n",
        "    Y_A = A[:, 0:1]\n",
        "\n",
        "    # S = H P H^T + R\n",
        "    S = (1.0 / (N - 1.0)) * (Y_A.T @ Y_A) + R_noise_std**2\n",
        "\n",
        "    # Kalman Gain K\n",
        "    K = (1.0 / (N - 1.0)) * (A.T @ Y_A) * (1.0 / S)\n",
        "\n",
        "    # Innovation\n",
        "    innov = y_obs - HX\n",
        "\n",
        "    # Update\n",
        "    ensemble_new = ensemble + (innov @ K.T)\n",
        "    return ensemble_new\n",
        "\n",
        "# We use static_argnums for ensemble size (arg 6) to avoid shape errors\n",
        "@partial(jit, static_argnums=(6,))\n",
        "def run_enkf_cycle(key, x0_ens_init, prior_mu, prior_std, y_obs_seq, total_steps, n_ens):\n",
        "    k1, k2 = random.split(key)\n",
        "\n",
        "    # Initial Parameter Ensemble\n",
        "    sigma_ens = prior_mu[0] + prior_std[0] * random.normal(k1, (n_ens, 1))\n",
        "    rho_ens   = prior_mu[1] + prior_std[1] * random.normal(k2, (n_ens, 1))\n",
        "\n",
        "    # Initial State Ensemble\n",
        "    state_ens = x0_ens_init + 0.1 * random.normal(key, (n_ens, 3))\n",
        "\n",
        "    # Combine\n",
        "    ensemble = jnp.hstack([state_ens, sigma_ens, rho_ens])\n",
        "\n",
        "    param_jitter_std = jnp.array([0.01, 0.01])\n",
        "    inflation = 1.02\n",
        "\n",
        "    def body_fn(carry, t):\n",
        "        ens, key_iter = carry\n",
        "        key_step, key_jitter = random.split(key_iter)\n",
        "\n",
        "        # --- 1. Forecast ---\n",
        "        def integ_step(e_state, _):\n",
        "            s, r = e_state[3], e_state[4]\n",
        "            xyz_next = rk4_step(e_state[0:3], jnp.array([s, r, 8.0/3.0]), DT_INT)\n",
        "            return jnp.concatenate([xyz_next, e_state[3:5]]), None\n",
        "\n",
        "        def integrate_ensemble(e):\n",
        "            e_final, _ = lax.scan(integ_step, e, None, length=FIXED_OBS_FREQ)\n",
        "            return e_final\n",
        "\n",
        "        ens_forecast = vmap(integrate_ensemble)(ens)\n",
        "\n",
        "        # Jitter & Inflation\n",
        "        noise = random.normal(key_jitter, (n_ens, 2)) * param_jitter_std\n",
        "        ens_forecast = ens_forecast.at[:, 3:5].add(noise)\n",
        "        ens_mean = jnp.mean(ens_forecast, axis=0)\n",
        "        ens_forecast = ens_mean + inflation * (ens_forecast - ens_mean)\n",
        "\n",
        "        # --- 2. Analysis ---\n",
        "        ens_analysis = enkf_update(ens_forecast, y_obs_seq[t], SIGMA_OBS)\n",
        "        ens_analysis = jnp.clip(ens_analysis, a_min=0.1, a_max=100.0)\n",
        "\n",
        "        return (ens_analysis, key_step), ens_analysis\n",
        "\n",
        "    (final_ens, _), history = lax.scan(body_fn, (ensemble, key), jnp.arange(len(y_obs_seq)))\n",
        "    return final_ens\n",
        "\n",
        "# ==========================================\n",
        "# 4. Energy Distance Metric\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def compute_energy_distance(sample_a, sample_b):\n",
        "    \"\"\"\n",
        "    Computes Energy Distance D^2(X, Y) = 2E|X-Y| - E|X-X'| - E|Y-Y'|\n",
        "    \"\"\"\n",
        "    def dist_mean(x, y):\n",
        "        # x: (N, D), y: (M, D)\n",
        "        # Calculates pairwise distance matrix then takes mean\n",
        "        diff = x[:, None, :] - y[None, :, :]\n",
        "        return jnp.mean(jnp.linalg.norm(diff, axis=-1))\n",
        "\n",
        "    term_xy = dist_mean(sample_a, sample_b) # Cross distance\n",
        "    term_xx = dist_mean(sample_a, sample_a) # Internal distance X\n",
        "    term_yy = dist_mean(sample_b, sample_b) # Internal distance Y\n",
        "\n",
        "    return 2 * term_xy - term_xx - term_yy\n",
        "\n",
        "# ==========================================\n",
        "# 5. Visualization (Modified)\n",
        "# ==========================================\n",
        "\n",
        "def plot_enkf_results(ensemble, p_mu_s, p_std_s, p_mu_r, p_std_r, t_sigma, t_rho, n_obs, ens_size, ed_val=None, mcmc_samples=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    ens_sigma = np.array(ensemble[:, 3])\n",
        "    ens_rho   = np.array(ensemble[:, 4])\n",
        "\n",
        "    # --- Contours (Prior) ---\n",
        "    x_range = np.linspace(p_mu_s - 3*p_std_s, p_mu_s + 3*p_std_s, 100)\n",
        "    y_range = np.linspace(p_mu_r - 3*p_std_r, p_mu_r + 3*p_std_r, 100)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = (1/(2*np.pi*p_std_s*p_std_r)) * np.exp(-0.5*(((X-p_mu_s)/p_std_s)**2 + ((Y-p_mu_r)/p_std_r)**2))\n",
        "\n",
        "    custom_levels = Z.max() * np.array([0.01, 0.1, 0.5, 0.9])\n",
        "    plt.contour(X, Y, Z, levels=custom_levels, colors='orange', linestyles='--', alpha=0.6)\n",
        "\n",
        "    # --- MCMC Reference (Gray) ---\n",
        "    if mcmc_samples is not None:\n",
        "        step = max(1, len(mcmc_samples) // 5000)\n",
        "        plt.scatter(mcmc_samples[::step, 0], mcmc_samples[::step, 1],\n",
        "                    c='gray', s=5, alpha=0.15, zorder=1)\n",
        "\n",
        "    # --- EnKF Posterior (Blue) ---\n",
        "    plt.scatter(ens_sigma, ens_rho, c='blue', s=10, alpha=0.6, zorder=5)\n",
        "\n",
        "    # --- REMOVED GAUSSIAN FIT (ELLIPSE) ---\n",
        "\n",
        "    # --- Markers ---\n",
        "    plt.scatter(t_sigma, t_rho, c='red', marker='*', s=300, zorder=10, label=f'Truth ({t_sigma:.1f}, {t_rho:.1f})')\n",
        "    plt.scatter(p_mu_s, p_mu_r, c='orange', marker='x', s=150, zorder=10, lw=3, label='Prior Mean')\n",
        "\n",
        "    # Updated Title\n",
        "    title_str = f\"Method: State Augmentation (EnKF)\\nObs: {n_obs} points | Ensemble Size: {ens_size}\"\n",
        "    if ed_val is not None:\n",
        "        title_str += f\" | Energy Dist vs MCMC: {ed_val:.4f}\"\n",
        "\n",
        "    plt.title(title_str)\n",
        "    plt.xlabel('Sigma'); plt.ylabel('Rho')\n",
        "\n",
        "    # Smart Limits\n",
        "    center_x = (t_sigma + p_mu_s)/2\n",
        "    center_y = (t_rho + p_mu_r)/2\n",
        "    span_x = max(abs(t_sigma - p_mu_s), 4.0) * 1.5\n",
        "    span_y = max(abs(t_rho - p_mu_r), 10.0) * 1.5\n",
        "    plt.xlim(center_x - span_x, center_x + span_x)\n",
        "    plt.ylim(center_y - span_y, center_y + span_y)\n",
        "\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Custom Legend ---\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='orange', linestyle='--', lw=2),\n",
        "        Line2D([0], [0], color='blue', marker='o', linestyle='None', alpha=0.6),\n",
        "        # Removed Gaussian Fit Line\n",
        "        Line2D([0], [0], marker='*', color='w', markerfacecolor='r', markersize=15),\n",
        "        Line2D([0], [0], marker='x', color='orange', markersize=10, markeredgewidth=2, linestyle='None')\n",
        "    ]\n",
        "    legend_labels = ['Prior', 'EnKF Ensemble', 'Truth', 'Prior Mean']\n",
        "\n",
        "    # Add MCMC to legend if present\n",
        "    if mcmc_samples is not None:\n",
        "        legend_elements.insert(1, Line2D([0], [0], color='gray', marker='o', linestyle='None', alpha=0.5))\n",
        "        legend_labels.insert(1, 'MCMC Samples')\n",
        "\n",
        "    plt.legend(legend_elements, legend_labels, loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 6. Main Logic\n",
        "# ==========================================\n",
        "\n",
        "def run_experiment_logic():\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 1. Generate Truth\n",
        "    total_steps = obs_num * FIXED_OBS_FREQ\n",
        "    key = random.PRNGKey(42)\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "\n",
        "    x0 = random.normal(k1, (3,))\n",
        "    for _ in range(100): x0 = rk4_step(x0, jnp.array([true_sigma, true_rho, 8.0/3.0]), DT_INT)\n",
        "\n",
        "    truth_traj = generate_truth(x0, true_sigma, true_rho, 8.0/3.0, total_steps)\n",
        "\n",
        "    # 2. Observations\n",
        "    obs_indices = np.arange(FIXED_OBS_FREQ, total_steps + 1, FIXED_OBS_FREQ)\n",
        "    obs_indices_jax = jnp.array(obs_indices)\n",
        "    y_truth = truth_traj[obs_indices_jax, 0]\n",
        "    y_obs = y_truth + random.normal(k2, y_truth.shape) * SIGMA_OBS\n",
        "\n",
        "    # 3. Run EnKF\n",
        "    prior_mu = jnp.array([prior_mu_sigma, prior_mu_rho])\n",
        "    prior_std = jnp.array([prior_std_sigma, prior_std_rho])\n",
        "    x0_ens = jnp.tile(x0, (ensemble_size, 1))\n",
        "\n",
        "    final_ens = run_enkf_cycle(\n",
        "        k3, x0_ens, prior_mu, prior_std, y_obs, obs_num, ensemble_size\n",
        "    )\n",
        "\n",
        "    t_calc = time.time() - t0\n",
        "    print(f\"EnKF Cycle ({obs_num} obs): {t_calc:.4f}s\")\n",
        "\n",
        "    # 4. Process MCMC for Metric & Plotting\n",
        "    ed_val = None\n",
        "    mcmc_params = None\n",
        "\n",
        "    if os.path.exists('mcmc_chain.npy'):\n",
        "        mcmc_full = np.load('mcmc_chain.npy')\n",
        "\n",
        "        # 1. For Metric Calculation (Downsampled)\n",
        "        enkf_params = final_ens[:, 3:5]\n",
        "\n",
        "        # Ensure we don't take more samples than available\n",
        "        n_metric = min(1000, mcmc_full.shape[0])\n",
        "        idx_metric = np.linspace(0, mcmc_full.shape[0]-1, n_metric).astype(int)\n",
        "        mcmc_metric_sample = jnp.array(mcmc_full[idx_metric])\n",
        "\n",
        "        ed_val = compute_energy_distance(enkf_params, mcmc_metric_sample)\n",
        "        print(f\"Energy Distance (EnKF vs MCMC): {ed_val:.5f}\")\n",
        "\n",
        "        # 2. For Plotting\n",
        "        mcmc_params = mcmc_full\n",
        "    else:\n",
        "        print(\"Warning: 'mcmc_chain.npy' not found. Cannot calculate Energy Distance.\")\n",
        "\n",
        "    # 5. Plot\n",
        "    plot_enkf_results(\n",
        "        final_ens,\n",
        "        prior_mu_sigma, prior_std_sigma,\n",
        "        prior_mu_rho, prior_std_rho,\n",
        "        true_sigma, true_rho,\n",
        "        obs_num,\n",
        "        ensemble_size,\n",
        "        ed_val,\n",
        "        mcmc_samples=mcmc_params\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment_logic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhu1G9w-OVDD"
      },
      "source": [
        "### 1.3 Parameter Estimation via Optimization (MAP)\n",
        "\n",
        "In this section, we solve the inverse problem by formulating it as a variational optimization task. Unlike the EnKF, which updates a distribution of particles, this method seeks the single parameter set $\\theta$ that maximizes the posterior probability density.\n",
        "\n",
        "#### 1.3.1 The Objective Function\n",
        "We define the Maximum A Posteriori (MAP) estimator $\\hat{\\theta}_{\\text{MAP}}$ as the minimizer of a negative log-posterior cost function $J(\\theta)$. This cost function consists of two terms: a data fidelity term (likelihood) and a regularization term (prior).\n",
        "\n",
        "Given the Gaussian assumptions on observation noise and the prior distribution defined in Section 1.1, the cost function is defined as:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\underbrace{\\frac{1}{2\\sigma_{\\text{obs}}^2} \\sum_{j=1}^{50} \\|y^\\dagger_j - H(u(t_j; \\theta))\\|^2}_{\\mathcal{L}_{\\text{data}}(\\theta)} + \\underbrace{\\frac{1}{2} (\\theta - \\mu_{\\text{prior}})^\\top \\Sigma_{\\text{prior}}^{-1} (\\theta - \\mu_{\\text{prior}})}_{\\mathcal{L}_{\\text{reg}}(\\theta)}\n",
        "$$\n",
        "\n",
        "Here, $u(t_j; \\theta)$ represents the deterministic solution of the Lorenz-63 ODEs integrated from $t_0$ to $t_j$ using parameters $\\theta$. The term $\\mathcal{L}_{\\text{reg}}$ penalizes deviations from the prior mean $\\mu_{\\text{prior}} = [9.0, 30.0]^\\top$, preventing overfitting to the sparse noise.\n",
        "\n",
        "#### 1.3.2 Optimization Strategy\n",
        "We treat the parameter estimation as a non-linear optimization problem to find the global minimum of the cost function:\n",
        "\n",
        "$$\n",
        "\\hat{\\theta}_{\\text{MAP}} = \\operatorname*{argmin}_{\\theta} J(\\theta)\n",
        "$$\n",
        "\n",
        "We solve this using gradient-based optimization (e.g., L-BFGS-B or Adam). The gradients $\\nabla_\\theta J$ are computed via Automatic Differentiation (AD) through the Runge-Kutta solver, allowing backpropagation of error signals from the observation times $t_j$ back to the parameters $\\theta$.\n",
        "\n",
        "#### 1.3.3 Uncertainty Quantification via Laplace Approximation\n",
        "A standard optimization yields only a point estimate $\\hat{\\theta}_{\\text{MAP}}$. To recover the uncertainty (posterior covariance) without the computational cost of MCMC, we employ the **Laplace Approximation**.\n",
        "\n",
        "We approximate the posterior distribution $p(\\theta | Y^\\dagger)$ as a Gaussian centered at the MAP estimate:\n",
        "$$\n",
        "p(\\theta | Y^\\dagger) \\approx \\mathcal{N}(\\hat{\\theta}_{\\text{MAP}}, \\Sigma_{\\text{post}})\n",
        "$$\n",
        "The posterior covariance $\\Sigma_{\\text{post}}$ is approximated by the inverse of the Hessian (second-order partial derivatives) of the cost function evaluated at the optimum:\n",
        "$$\n",
        "\\Sigma_{\\text{post}} \\approx \\left( \\nabla^2 J(\\hat{\\theta}_{\\text{MAP}}) \\right)^{-1}\n",
        "$$\n",
        "Intuitively, the Hessian $\\mathcal{H} = \\nabla^2 J$ measures the curvature of the loss landscape. A sharp valley (large curvature) implies high certainty (small variance), while a flat valley implies high uncertainty (large variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "PSGVLA7MYjxb",
        "outputId": "0347da24-f6c4-43d5-f1eb-c778afae3ce1"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, vmap, lax, value_and_grad, hessian\n",
        "from jax.scipy.stats import norm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "from matplotlib.lines import Line2D\n",
        "from functools import partial\n",
        "import time\n",
        "import os\n",
        "import scipy.optimize\n",
        "\n",
        "# ==========================================\n",
        "# 1. Colab Form Configuration\n",
        "# ==========================================\n",
        "# @title MAP & Laplace Experiment Parameters\n",
        "\n",
        "# @markdown **Ground Truth**\n",
        "true_sigma = 10.0  # @param {type:\"number\"}\n",
        "true_rho = 28.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **Prior Beliefs**\n",
        "prior_mu_sigma = 9.0  # @param {type:\"number\"}\n",
        "prior_std_sigma = 2.0  # @param {type:\"number\"}\n",
        "prior_mu_rho = 30.0  # @param {type:\"number\"}\n",
        "prior_std_rho = 5.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **Optimization Settings**\n",
        "optimizer_type = \"Adam\" # @param [\"L-BFGS-B\", \"Adam\"]\n",
        "obs_num = 50  # @param {type:\"number\"}\n",
        "learning_rate = 0.01 # @param {type:\"number\"} (For Adam only)\n",
        "adam_steps = 2000 # @param {type:\"number\"} (For Adam only)\n",
        "\n",
        "# Constants\n",
        "DT_INT = 0.03\n",
        "FIXED_OBS_FREQ = 5\n",
        "SIGMA_OBS = 1.0\n",
        "\n",
        "# ==========================================\n",
        "# 2. System Dynamics (JAX)\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def lorenz63_dynamics(state, params):\n",
        "    x, y, z = state\n",
        "    sigma, rho, beta = params\n",
        "    dx = sigma * (y - x)\n",
        "    dy = x * (rho - z) - y\n",
        "    dz = x * y - beta * z\n",
        "    return jnp.array([dx, dy, dz])\n",
        "\n",
        "@jit\n",
        "def rk4_step(state, params, dt):\n",
        "    k1 = lorenz63_dynamics(state, params)\n",
        "    k2 = lorenz63_dynamics(state + 0.5 * dt * k1, params)\n",
        "    k3 = lorenz63_dynamics(state + 0.5 * dt * k2, params)\n",
        "    k4 = lorenz63_dynamics(state + dt * k3, params)\n",
        "    return state + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "# Generator for Truth\n",
        "# total_steps is arg 4, mark as static\n",
        "@partial(jit, static_argnums=(4,))\n",
        "def generate_truth(x0, sigma, rho, beta, total_steps):\n",
        "    params = jnp.array([sigma, rho, beta])\n",
        "    def step_fn(carry, _):\n",
        "        next_state = rk4_step(carry, params, DT_INT)\n",
        "        return next_state, next_state\n",
        "    _, traj = lax.scan(step_fn, x0, None, length=total_steps)\n",
        "    return jnp.concatenate([x0[None, :], traj], axis=0)\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAP Optimization & Laplace Approximation\n",
        "# ==========================================\n",
        "\n",
        "# FIX: Marked total_steps (index 2) as static to satisfy lax.scan requirement\n",
        "@partial(jit, static_argnums=(2,))\n",
        "def forward_model(theta, x0, total_steps):\n",
        "    \"\"\"Integrates system for given params theta=[sigma, rho]. Returns observations.\"\"\"\n",
        "    params = jnp.array([theta[0], theta[1], 8.0/3.0])\n",
        "\n",
        "    def step(carry, _):\n",
        "        s = rk4_step(carry, params, DT_INT)\n",
        "        return s, s\n",
        "\n",
        "    # Run full trajectory\n",
        "    _, traj = lax.scan(step, x0, None, length=total_steps)\n",
        "\n",
        "    # Extract observations at specific indices\n",
        "    # Since total_steps is static, jnp.arange generates a constant array\n",
        "    obs_indices = jnp.arange(FIXED_OBS_FREQ - 1, total_steps, FIXED_OBS_FREQ)\n",
        "    model_obs = traj[obs_indices, 0] # Return only x component\n",
        "    return model_obs\n",
        "\n",
        "# cost_function expects total_steps as static (index 5)\n",
        "@partial(jit, static_argnums=(5,))\n",
        "def cost_function(theta, x0, y_obs, prior_mu, prior_cov_inv, total_steps):\n",
        "    \"\"\"Negative Log Posterior: Data Likelihood + Prior Regularization.\"\"\"\n",
        "    # 1. Data Likelihood (MSE)\n",
        "    # forward_model handles total_steps as static now\n",
        "    y_pred = forward_model(theta, x0, total_steps)\n",
        "    diff = y_obs - y_pred\n",
        "    nll_data = (0.5 / SIGMA_OBS**2) * jnp.sum(diff**2)\n",
        "\n",
        "    # 2. Prior Regularization (Mahalanobis)\n",
        "    delta = theta - prior_mu\n",
        "    nll_prior = 0.5 * (delta @ prior_cov_inv @ delta.T)\n",
        "\n",
        "    return nll_data + nll_prior\n",
        "\n",
        "def run_optimization(x0, y_obs, prior_mu, prior_std, obs_num):\n",
        "    \"\"\"Solves argmin J(theta) using selected optimizer.\"\"\"\n",
        "\n",
        "    prior_cov = jnp.diag(prior_std**2)\n",
        "    prior_cov_inv = jnp.diag(1.0 / prior_std**2)\n",
        "    total_steps = obs_num * FIXED_OBS_FREQ\n",
        "\n",
        "    # JIT the value and gradient function\n",
        "    # total_steps (arg 5) is static\n",
        "    loss_and_grad = jit(value_and_grad(cost_function), static_argnums=(5,))\n",
        "\n",
        "    # Initial Guess (Mean of Prior)\n",
        "    theta_init = prior_mu\n",
        "\n",
        "    print(f\"Starting Optimization ({optimizer_type})...\")\n",
        "\n",
        "    if optimizer_type == \"L-BFGS-B\":\n",
        "        # Wrap JAX function for Scipy\n",
        "        # We convert inputs to numpy float64 for stability in Scipy\n",
        "        def scipy_fun(w):\n",
        "            # w comes from scipy as numpy array\n",
        "            l, g = loss_and_grad(jnp.array(w), x0, y_obs, prior_mu, prior_cov_inv, total_steps)\n",
        "            return np.array(l, dtype=np.float64), np.array(g, dtype=np.float64)\n",
        "\n",
        "        # Removed 'disp' from options to fix DeprecationWarning\n",
        "        res = scipy.optimize.minimize(\n",
        "            scipy_fun,\n",
        "            np.array(theta_init, dtype=np.float64),\n",
        "            method='L-BFGS-B',\n",
        "            jac=True,\n",
        "            options={'maxiter': 100}\n",
        "        )\n",
        "        theta_opt = jnp.array(res.x)\n",
        "        print(f\"  Optimization Success: {res.success}\")\n",
        "        print(f\"  Iterations: {res.nit}\")\n",
        "\n",
        "    elif optimizer_type == \"Adam\":\n",
        "        # Simple Adam Loop\n",
        "        theta = theta_init\n",
        "        m = jnp.zeros_like(theta)\n",
        "        v = jnp.zeros_like(theta)\n",
        "        beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
        "\n",
        "        @jit\n",
        "        def adam_step(i, carry):\n",
        "            t, m_t, v_t = carry\n",
        "            l, g = loss_and_grad(t, x0, y_obs, prior_mu, prior_cov_inv, total_steps)\n",
        "\n",
        "            m_t = beta1 * m_t + (1 - beta1) * g\n",
        "            v_t = beta2 * v_t + (1 - beta2) * g**2\n",
        "            m_hat = m_t / (1 - beta1**(i+1))\n",
        "            v_hat = v_t / (1 - beta2**(i+1))\n",
        "            t_new = t - learning_rate * m_hat / (jnp.sqrt(v_hat) + eps)\n",
        "            return (t_new, m_t, v_t)\n",
        "\n",
        "        final_carry = lax.fori_loop(0, adam_steps, adam_step, (theta, m, v))\n",
        "        theta_opt = final_carry[0]\n",
        "\n",
        "    return theta_opt\n",
        "\n",
        "def compute_laplace_covariance(theta_opt, x0, y_obs, prior_mu, prior_std, obs_num):\n",
        "    \"\"\"Computes Inverse Hessian at theta_opt.\"\"\"\n",
        "    prior_cov_inv = jnp.diag(1.0 / prior_std**2)\n",
        "    total_steps = obs_num * FIXED_OBS_FREQ\n",
        "\n",
        "    # JIT Hessian computation\n",
        "    H_fn = jit(hessian(cost_function), static_argnums=(5,))\n",
        "    H = H_fn(theta_opt, x0, y_obs, prior_mu, prior_cov_inv, total_steps)\n",
        "\n",
        "    # Covariance = Inverse Hessian\n",
        "    cov_post = jnp.linalg.inv(H)\n",
        "    return cov_post\n",
        "\n",
        "# ==========================================\n",
        "# 4. Energy Distance Metric (Reused)\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def compute_energy_distance(sample_a, sample_b):\n",
        "    \"\"\"D^2(X, Y) = 2E|X-Y| - E|X-X'| - E|Y-Y'|\"\"\"\n",
        "    def dist_mean(x, y):\n",
        "        diff = x[:, None, :] - y[None, :, :]\n",
        "        return jnp.mean(jnp.linalg.norm(diff, axis=-1))\n",
        "\n",
        "    term_xy = dist_mean(sample_a, sample_b)\n",
        "    term_xx = dist_mean(sample_a, sample_a)\n",
        "    term_yy = dist_mean(sample_b, sample_b)\n",
        "    return 2 * term_xy - term_xx - term_yy\n",
        "\n",
        "# ==========================================\n",
        "# 5. Visualization\n",
        "# ==========================================\n",
        "\n",
        "def plot_map_results(theta_opt, laplace_samples, p_mu, p_std, t_sigma, t_rho, n_obs, ed_val=None, mcmc_samples=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    p_mu_s, p_mu_r = p_mu[0], p_mu[1]\n",
        "    p_std_s, p_std_r = p_std[0], p_std[1]\n",
        "\n",
        "    # --- Contours (Prior) ---\n",
        "    x_range = np.linspace(p_mu_s - 3*p_std_s, p_mu_s + 3*p_std_s, 100)\n",
        "    y_range = np.linspace(p_mu_r - 3*p_std_r, p_mu_r + 3*p_std_r, 100)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = (1/(2*np.pi*p_std_s*p_std_r)) * np.exp(-0.5*(((X-p_mu_s)/p_std_s)**2 + ((Y-p_mu_r)/p_std_r)**2))\n",
        "    plt.contour(X, Y, Z, levels=Z.max() * np.array([0.01, 0.1, 0.5, 0.9]), colors='orange', linestyles='--', alpha=0.6)\n",
        "\n",
        "    # --- MCMC Reference (Gray) ---\n",
        "    if mcmc_samples is not None:\n",
        "        step = max(1, len(mcmc_samples) // 5000)\n",
        "        plt.scatter(mcmc_samples[::step, 0], mcmc_samples[::step, 1], c='gray', s=5, alpha=0.15, zorder=1)\n",
        "\n",
        "    # --- Laplace Samples (Blue) ---\n",
        "    plt.scatter(laplace_samples[:, 0], laplace_samples[:, 1], c='blue', s=10, alpha=0.3, zorder=5, label='Laplace Samples')\n",
        "\n",
        "    # --- MAP Point Estimate (Green Star) ---\n",
        "    plt.scatter(theta_opt[0], theta_opt[1], c='lime', marker='*', s=400, edgecolors='black', zorder=20, label='MAP Estimate')\n",
        "\n",
        "    # --- Markers ---\n",
        "    plt.scatter(t_sigma, t_rho, c='red', marker='*', s=300, zorder=10, label=f'Truth ({t_sigma:.1f}, {t_rho:.1f})')\n",
        "    plt.scatter(p_mu_s, p_mu_r, c='orange', marker='x', s=150, zorder=10, lw=3, label='Prior Mean')\n",
        "\n",
        "    # Title\n",
        "    title_str = f\"Method: MAP Optimization ({optimizer_type}) + Laplace Approx\\nObs: {n_obs} points\"\n",
        "    if ed_val is not None:\n",
        "        title_str += f\" | Energy Dist vs MCMC: {ed_val:.4f}\"\n",
        "    plt.title(title_str)\n",
        "    plt.xlabel('Sigma'); plt.ylabel('Rho')\n",
        "\n",
        "    # Limits\n",
        "    center_x = (t_sigma + p_mu_s)/2\n",
        "    center_y = (t_rho + p_mu_r)/2\n",
        "    span_x = max(abs(t_sigma - p_mu_s), 4.0) * 1.5\n",
        "    span_y = max(abs(t_rho - p_mu_r), 10.0) * 1.5\n",
        "    plt.xlim(center_x - span_x, center_x + span_x)\n",
        "    plt.ylim(center_y - span_y, center_y + span_y)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Legend\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='orange', linestyle='--', lw=2),\n",
        "        Line2D([0], [0], color='blue', marker='o', linestyle='None', alpha=0.6),\n",
        "        Line2D([0], [0], marker='*', color='lime', markersize=15, markeredgecolor='black', linestyle='None'),\n",
        "        Line2D([0], [0], marker='*', color='w', markerfacecolor='r', markersize=15),\n",
        "    ]\n",
        "    legend_labels = ['Prior', 'Laplace Approx', 'MAP Estimate', 'Truth']\n",
        "\n",
        "    if mcmc_samples is not None:\n",
        "        legend_elements.insert(1, Line2D([0], [0], color='gray', marker='o', linestyle='None', alpha=0.5))\n",
        "        legend_labels.insert(1, 'MCMC Samples')\n",
        "\n",
        "    plt.legend(legend_elements, legend_labels, loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 6. Main Logic\n",
        "# ==========================================\n",
        "\n",
        "def run_experiment_logic():\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 1. Generate Truth (Identical seed to EnKF)\n",
        "    total_steps = obs_num * FIXED_OBS_FREQ\n",
        "    key = random.PRNGKey(42)\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "\n",
        "    x0 = random.normal(k1, (3,))\n",
        "    # Spin up\n",
        "    for _ in range(100): x0 = rk4_step(x0, jnp.array([true_sigma, true_rho, 8.0/3.0]), DT_INT)\n",
        "\n",
        "    truth_traj = generate_truth(x0, true_sigma, true_rho, 8.0/3.0, total_steps)\n",
        "\n",
        "    # 2. Observations\n",
        "    obs_indices = np.arange(FIXED_OBS_FREQ, total_steps + 1, FIXED_OBS_FREQ)\n",
        "    y_truth = truth_traj[jnp.array(obs_indices), 0]\n",
        "    y_obs = y_truth + random.normal(k2, y_truth.shape) * SIGMA_OBS\n",
        "\n",
        "    # 3. Optimization (MAP)\n",
        "    prior_mu = jnp.array([prior_mu_sigma, prior_mu_rho])\n",
        "    prior_std = jnp.array([prior_std_sigma, prior_std_rho])\n",
        "\n",
        "    theta_opt = run_optimization(x0, y_obs, prior_mu, prior_std, obs_num)\n",
        "\n",
        "    # 4. Uncertainty (Laplace Approximation)\n",
        "    cov_post = compute_laplace_covariance(theta_opt, x0, y_obs, prior_mu, prior_std, obs_num)\n",
        "\n",
        "    print(f\"MAP Estimate: sigma={theta_opt[0]:.2f}, rho={theta_opt[1]:.2f}\")\n",
        "    print(f\"Time Elapsed: {time.time() - t0:.4f}s\")\n",
        "\n",
        "    # 5. Sample from Laplace for Metrics & Plotting\n",
        "    # Sample 1000 points from N(theta_opt, cov_post)\n",
        "    try:\n",
        "        laplace_samples = random.multivariate_normal(k3, theta_opt, cov_post, shape=(1000,))\n",
        "    except ValueError:\n",
        "        print(\"Warning: Covariance matrix is not positive definite. Falling back to diagonal approximation.\")\n",
        "        laplace_samples = random.multivariate_normal(k3, theta_opt, jnp.diag(jnp.diag(cov_post)), shape=(1000,))\n",
        "\n",
        "    # 6. Compare with MCMC (Energy Distance)\n",
        "    ed_val = None\n",
        "    mcmc_params = None\n",
        "\n",
        "    if os.path.exists('mcmc_chain.npy'):\n",
        "        mcmc_full = np.load('mcmc_chain.npy')\n",
        "\n",
        "        # Downsample MCMC for metric\n",
        "        n_metric = min(1000, mcmc_full.shape[0])\n",
        "        idx_metric = np.linspace(0, mcmc_full.shape[0]-1, n_metric).astype(int)\n",
        "        mcmc_metric_sample = jnp.array(mcmc_full[idx_metric])\n",
        "\n",
        "        ed_val = compute_energy_distance(laplace_samples, mcmc_metric_sample)\n",
        "        print(f\"Energy Distance (Laplace vs MCMC): {ed_val:.5f}\")\n",
        "\n",
        "        mcmc_params = mcmc_full\n",
        "    else:\n",
        "        print(\"Warning: 'mcmc_chain.npy' not found. Cannot calculate Energy Distance.\")\n",
        "\n",
        "    # 7. Plot\n",
        "    plot_map_results(\n",
        "        theta_opt,\n",
        "        laplace_samples,\n",
        "        prior_mu, prior_std,\n",
        "        true_sigma, true_rho,\n",
        "        obs_num,\n",
        "        ed_val,\n",
        "        mcmc_samples=mcmc_params\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment_logic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3duoAovQuK9"
      },
      "source": [
        "## 2. ML + KF\n",
        "\n",
        "In this section, we transition to a high-dimensional chaotic system to evaluate the synergy between Machine Learning (ML) and Ensemble Kalman Filtering (EnKF). We consider the Lorenz-96 model, a standard benchmark for atmospheric dynamics and data assimilation.\n",
        "\n",
        "### 2.1 Problem Setting: Lorenz-96 System\n",
        "\n",
        "We consider a 40-dimensional Lorenz-96 system, where the state vector $\\mathbf{v} \\in \\mathbb{R}^{40}$ evolves according to the following set of coupled ordinary differential equations:\n",
        "\n",
        "$$\n",
        "\\frac{d v_k}{d t} = (v_{k+1} - v_{k-2}) v_{k-1} - v_k + F, \\quad k = 1, \\dots, 40,\n",
        "$$\n",
        "\n",
        "with cyclic boundary conditions ($v_{-1} = v_{39}, v_0 = v_{40}, v_{41} = v_1$). The forcing term is fixed at $F=8$ to ensure fully chaotic behavior.\n",
        "\n",
        "**Simulation and Observation Details:**\n",
        "* **Dynamic Noise** Denote the discrete Lorenz-96 dynamics by $\\Psi$. Then the forward dynamic is given by:\n",
        "$$v_{j+1} = \\Psi(v_j) + \\xi_j, \\quad \\xi_j\\sim \\mathcal{N}(0,\\Sigma),$$\n",
        "where $\\Sigma = \\sigma_v^2 I$ and we choose $\\sigma_v = 0.01$.\n",
        "* **Temporal Discretization:** The system is integrated using a 4th-order Runge-Kutta (RK4) scheme with an internal integration step of $\\delta t = 0.01$.\n",
        "* **Assimilation Window:** Observations are available every $\\Delta t_{\\text{obs}} = 0.05$, corresponding to 5 RK4 integration steps per assimilation cycle.\n",
        "* **Full Observations:** We consider a fully observed scenario where the entire state vector is measured. The observation vector $\\mathbf{y} \\in \\mathbb{R}^{40}$ is given by:\n",
        "$$\\mathbf{y}_j = \\mathbf{v}_j + \\eta_j, \\quad \\eta_j \\sim \\mathcal{N}(0, \\Gamma)$$\n",
        "where the observation operator $h(\\mathbf{v})$ is the identity map. We choose $\\Gamma = \\sigma_y^2 \\mathbf{I}$ with $\\sigma_y = 1$.\n",
        "\n",
        "### 2.2 Baseline: The Perturbed-Observation Ensemble Kalman Filter (EnKF)\n",
        "\n",
        "As a baseline for high-dimensional assimilation, we employ the **Perturbed-Observation EnKF**. This stochastic variant accounts for both the uncertainty in the forward dynamics and the statistical properties of the observation noise by treating the observations as random variables.\n",
        "\n",
        "#### 2.2.1 Forecast Step (with Dynamic Noise)\n",
        "Each ensemble member $\\mathbf{v}_{j-1}^{(n)}$ is evolved forward to time $t_j$ using the Lorenz-96 dynamics $\\Psi$. To account for model uncertainty, we inject additive dynamic noise $\\xi_j^{(n)}$:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{v}}_j^{(n)} = \\Psi(\\mathbf{v}_{j-1}^{(n)}) + \\xi_j^{(n)}, \\quad \\xi_j^{(n)} \\sim \\mathcal{N}(0, \\Sigma)\n",
        "$$\n",
        "\n",
        "where $\\Sigma = \\sigma_v^2 \\mathbf{I}$ is the dynamic noise covariance. The predictive ensemble mean in state space and its projection into observation space are defined as:\n",
        "\n",
        "$$\n",
        "\\bar{\\mathbf{v}}_j = \\frac{1}{N} \\sum_{n=1}^N \\hat{\\mathbf{v}}_j^{(n)}, \\quad \\bar{h}_j = \\frac{1}{N} \\sum_{n=1}^N h(\\hat{\\mathbf{v}}_j^{(n)})\n",
        "$$\n",
        "\n",
        "#### 2.2.2 Analysis Step\n",
        "When a sparse observation $\\mathbf{y}^\\dagger_j$ is received, the filter updates each forecast member. Following the perturbed-observation approach, each member incorporates a realization of the observation noise $\\eta_j^{(n)} \\sim \\mathcal{N}(0, \\Gamma)$, where $\\Gamma = \\sigma_y^2 \\mathbf{I}$.\n",
        "\n",
        "The update is governed by the empirical cross-covariance matrices. We denote the state-observation covariance $\\hat{\\mathbf{C}}^{vh}_j$ and the innovation covariance $\\hat{\\mathbf{C}}^{hh}_j$ using the tensor product $\\otimes$ as follows:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{C}}^{vh}_j = \\frac{1}{N-1} \\sum_{n=1}^N (\\hat{\\mathbf{v}}^{(n)}_j - \\bar{\\mathbf{v}}_j) \\otimes (h(\\hat{\\mathbf{v}}^{(n)}_j) - \\bar{h}_j)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{C}}^{hh}_j = \\frac{1}{N-1} \\sum_{n=1}^N (h(\\hat{\\mathbf{v}}^{(n)}_j) - \\bar{h}_j) \\otimes (h(\\hat{\\mathbf{v}}^{(n)}_j) - \\bar{h}_j)\n",
        "$$\n",
        "\n",
        "The analysis ensemble $\\mathbf{v}_j^{(n)}$ is then computed by correcting the forecast with the noisy observation residual:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_j^{(n)} = \\hat{\\mathbf{v}}_j^{(n)} + \\hat{\\mathbf{C}}^{vh}_j \\left( \\hat{\\mathbf{C}}^{hh}_j + \\Gamma \\right)^{-1} \\left( \\mathbf{y}^\\dagger_j - h(\\hat{\\mathbf{v}}_j^{(n)}) - \\eta_j^{(n)} \\right)\n",
        "$$\n",
        "\n",
        "#### 2.2.3 Regularization: Inflation and Localization\n",
        "\n",
        "In high-dimensional systems like Lorenz-96, using a limited ensemble size ($N \\ll 40$) typically leads to **filter divergence** due to systematic underestimation of error covariances and the appearance of spurious long-range correlations. We address these issues using distance-dependent localization and post-analysis multiplicative inflation.\n",
        "\n",
        "#### Covariance Localization via Gaspari-Cohn (GC) Function\n",
        "To eliminate spurious correlations between physically distant variables, we regularize the empirical covariance matrices using a tapering function. The state-observation covariance $\\hat{\\mathbf{C}}^{vh}_j$ is modified by an element-wise (Schur) product with a localization matrix $\\mathbf{L}$:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{C}}^{vh}_{j, \\text{loc}} = \\mathbf{L} \\circ \\hat{\\mathbf{C}}^{vh}_j\n",
        "$$\n",
        "\n",
        "The elements of $\\mathbf{L}$ are defined by the **Gaspari-Cohn (GC) function** $C_0(d, r)$, a compactly supported $5^{\\text{th}}$-order polynomial that mimics a Gaussian distribution:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "L_{ki} = C_0(d(k, i), r) =\n",
        "\\begin{cases}\n",
        "1 - \\frac{5}{3}x^2 + \\frac{5}{8}x^3 + \\frac{1}{2}x^4 - \\frac{1}{4}x^5, & 0 \\leq d < r \\\\\n",
        "4 - 5x + \\frac{5}{3}x^2 + \\frac{5}{8}x^3 - \\frac{1}{2}x^4 + \\frac{1}{12}x^5 - \\frac{2}{3x}, & r \\leq d < 2r \\\\\n",
        "0, & d \\geq 2r\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $r$ is the localization radius. This ensures that observations only influence state variables within a local neighborhood.\n",
        "\n",
        "#### Post-Analysis Multiplicative Inflation\n",
        "To compensate for the loss of variance caused by the linear update and sampling errors, we apply a scalar inflation factor $\\alpha > 1$ to the **analysis ensemble** $\\mathbf{v}_j^{(n)}$ after the update is completed:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_j^{(n)} \\leftarrow \\bar{\\mathbf{v}}_j^{\\text{ana}} + \\alpha (\\mathbf{v}_j^{(n)} - \\bar{\\mathbf{v}}_j^{\\text{ana}})\n",
        "$$\n",
        "\n",
        "where $\\bar{\\mathbf{v}}_j^{\\text{ana}} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{v}_j^{(n)}$ is the mean of the updated ensemble. This post-processing step ensures that the ensemble spread remains sufficiently large to represent the posterior uncertainty and avoids filter collapse in subsequent cycles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CuckgGHTolZl",
        "outputId": "f9602351-61cb-45b3-c5ba-50fcede6c06c"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, lax\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Simulation Parameters (Colab Forms) ---\n",
        "n = 40  # @param {type:\"integer\"}\n",
        "F = 8.0  # @param {type:\"number\"}\n",
        "dt_integration = 0.01  # @param {type:\"number\"}\n",
        "dt_obs = 0.05  # @param {type:\"number\"}\n",
        "num_cycles = 200  # @param {type:\"integer\"}\n",
        "sigma_v = 0.01  # @param {type:\"number\"}\n",
        "sigma_y = 1  # @param {type:\"number\"}\n",
        "\n",
        "# --- Derived Parameters ---\n",
        "m = n\n",
        "steps_per_obs = int(dt_obs / dt_integration)\n",
        "Q = jnp.eye(n) * (sigma_v ** 2)\n",
        "R_cov = jnp.eye(m) * (sigma_y ** 2)\n",
        "\n",
        "# --- Dynamics & Integration ---\n",
        "@jit\n",
        "def rk4_step(x, dt):\n",
        "    \"\"\"\n",
        "    Performs a single RK4 integration step.\n",
        "    Input: x (state), dt (time step)\n",
        "    Output: Updated state\n",
        "    \"\"\"\n",
        "    f = lambda y: (jnp.roll(y, 1) - jnp.roll(y, -2)) * jnp.roll(y, -1) - y + F\n",
        "    k1 = dt * f(x)\n",
        "    k2 = dt * f(x + k1/2)\n",
        "    k3 = dt * f(x + k2/2)\n",
        "    k4 = dt * f(x + k3)\n",
        "    return x + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def integrate_window(x, steps):\n",
        "    \"\"\"\n",
        "    Integrates system forward for 'steps' iterations.\n",
        "    Input: x (initial state), steps (int)\n",
        "    Output: State after integration window\n",
        "    \"\"\"\n",
        "    def body_fun(i, val):\n",
        "        return rk4_step(val, dt_integration)\n",
        "    return lax.fori_loop(0, steps, body_fun, x)\n",
        "\n",
        "@jit\n",
        "def simulation_step(carry, _):\n",
        "    \"\"\"\n",
        "    Executes one assimilation cycle (Full Observations).\n",
        "    Input: carry (rng_key, current_state), _ (ignored)\n",
        "    Output: (new_carry), (true_state, observation)\n",
        "    \"\"\"\n",
        "    key, x = carry\n",
        "    key, subkey_q, subkey_r = random.split(key, 3)\n",
        "\n",
        "    x_det = integrate_window(x, steps_per_obs)\n",
        "\n",
        "    process_noise = random.multivariate_normal(subkey_q, jnp.zeros(n), Q)\n",
        "    x_next = x_det + process_noise\n",
        "\n",
        "    obs_noise = random.multivariate_normal(subkey_r, jnp.zeros(n), R_cov)\n",
        "    y_full = x_next + obs_noise\n",
        "\n",
        "    return (key, x_next), (x_next, y_full)\n",
        "\n",
        "# --- Execution ---\n",
        "key = random.PRNGKey(42)\n",
        "key, subkey = random.split(key)\n",
        "\n",
        "# Initialize and Burn-in\n",
        "x0 = random.normal(subkey, (n,))\n",
        "x0 = integrate_window(x0, 1000)\n",
        "\n",
        "# Run Simulation\n",
        "initial_carry = (key, x0)\n",
        "_, (true_states_history, observations_history) = lax.scan(simulation_step, initial_carry, None, length=num_cycles)\n",
        "\n",
        "# Append initial state\n",
        "all_true_states = jnp.vstack([x0, true_states_history])\n",
        "\n",
        "# --- Visualization: System Evolution ---\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.imshow(all_true_states.T, aspect='auto', cmap='coolwarm', interpolation='nearest', vmin=-12, vmax=12)\n",
        "plt.ylabel('State Index (0-39)')\n",
        "plt.xlabel('Assimilation Cycle')\n",
        "plt.title('True States (Full System)')\n",
        "plt.colorbar(label='State Value')\n",
        "plt.tight_layout()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt3Fu5-6pv3d"
      },
      "source": [
        "### 2.3 Variational Inference\n",
        "\n",
        "Consider the filtering distributions for a dynamical system at times $j = 0,\\dots,J$:\n",
        "$$\\pi_j = \\mathbb{P}(v_j | y_1^\\dagger, \\dots, y_j^\\dagger)$$\n",
        "\n",
        "Filtering evolves according to the recursion:\\\n",
        "&nbsp; **Prediction step**: $\\widehat\\pi_{j+1} = \\mathsf{P}\\pi_j$, where $\\widehat\\pi_{j+1} = \\mathbb{P}(v_{j+1}|y_1^\\dagger, \\dots, y_j^\\dagger)$\\\n",
        "&nbsp; **Analysis step**: $\\pi_{j+1} = \\mathsf{A}(\\widehat\\pi_{j+1};y_{j+1}^\\dagger)$\n",
        "\n",
        "Approximate $\\pi_{j+1} \\approx q_{j+1}$ at each step of filtering by minmizing the KL divergence objective:\n",
        "$$\\arg\\min_q D_{KL}(q || \\pi_{j+1}) = \\arg\\min_q D_{KL}(q||\\widehat\\pi_{j+1}) - \\mathbb{E}_{u \\sim q}[\\log \\mathsf{I}(y_{j+1}^\\dagger;u)]$$\n",
        "\n",
        "We will define $q_j(\\theta)$ recursively using a procedure, which depends on parameters $\\theta = K$, where $K$ is the gain matrix. Then, the objective will be:\n",
        "$$\\min_\\theta \\frac{1}{J}\\sum_{j=0}^J L_{j}(q_j(\\theta))$$\n",
        "\n",
        "#### 2.3.1 Data Assumption\n",
        "We assume the physical dynamics $\\Psi$, the initial condition $\\mathbf{v}_0 \\sim \\mathcal{N}(m_0, C_0)$, and the noise covariances $\\Sigma$ and $\\Gamma$ are known. To generate training data, we simulate the system to obtain a set of ground truth trajectories $\\{\\mathbf{v}_j^\\dagger\\}_{j=1}^J$ and their corresponding noisy, sparse observations $\\{\\mathbf{y}_j^\\dagger\\}_{j=1}^J$, where $J$ denotes the total number of assimilation cycles in the training set.\n",
        "\n",
        "#### 2.3.1 Define objective function\n",
        "\n",
        "Consider Gaussian distributions $q_j(\\theta)$ defined recursively given $K = \\theta$:\n",
        "\n",
        "**Prediction step**:\n",
        "\n",
        "  \\begin{align}\n",
        "  \\widehat{m}_{j+1} &= \\Psi(m_j) \\\\\n",
        "  \\widehat{C}_{j+1} &= J_j C_j J_j^T + \\Sigma, \\qquad J_j = \\text{ Jacobian of } \\Psi\n",
        "  \\end{align}\n",
        "\n",
        "**Analysis step**:\n",
        "\n",
        "  \\begin{align*}\n",
        "  m_{j+1} &= \\widehat{m}_{j+1} + K(y_{j+1}^\\dagger - Hm_{j+1}) \\\\\n",
        "  C_{j+1} &= (I - KH)\\widehat{C}_{j+1} (I - KH)^T + K\\Gamma K^T\n",
        "  \\end{align*}\n",
        "\n",
        "Define: $\\widehat\\pi_{j+1} = \\mathcal{N}(\\widehat{m}_{j+1}, \\widehat{C}_{j+1})$ and $q_{j+1}(K) = \\mathcal{N}(m_{j+1}(K),C_{j+1}(K))$\n",
        "\n",
        "**Goal**: Learn $K$ that yields close filters for all time $j$ by minimizing the variational inference objective for each step of filtering:\n",
        " $$L(K) := \\frac{1}{J} \\sum_{j=1}^J D_{KL}(q_{j+1}(K)||\\widehat\\pi_{j+1}) - \\mathbb{E}_{u \\sim q_{j+1}(K)}[\\log \\mathsf{I}(u;y_{j+1})]$$\n",
        "\n",
        "### 2.3.2 Evaluation Metric\n",
        "\n",
        "To evaluate the tracking accuracy of the learned gain matrix $K$ against the EnKF baseline, we employ the Root Mean Square Error (RMSE). This metric quantifies the divergence of the filtered state estimate from the ground truth trajectory. Let $\\mathbf{v}_j^\\dagger$ denote the true state and $\\mathbf{m}_j$ the posterior mean estimate at time step $j$. Averaged over $J$ assimilation cycles and the system dimension $d=40$, the RMSE is defined as:\n",
        "\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{J \\cdot d} \\sum_{j=1}^J \\| \\mathbf{m}_j - \\mathbf{v}_j^\\dagger \\|_2^2}\n",
        "$$\n",
        "\n",
        "A lower RMSE indicates that the variational approximation successfully minimizes the information gap between the predicted and true distributions, maintaining the estimated trajectory close to the chaotic attractor.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a7d783c0405248b79c64d46cb4094d14",
            "6063e5c5f5a64fadb36c97bc2357858e",
            "21e7b61498684659992e758a6b8e949d",
            "9e04f998a0444877b07794157cf0b5de",
            "29e47a15c6a9487aa78e88cfe353b312",
            "a200acc39de54875aa39cc4fc4618fe8",
            "a6ac2119bbfe474bbb95841349f7f462",
            "b0b5be04d59547e88609eb14ca015ed7",
            "fee0baf6ca7349faae7e6dffe86c0ac3",
            "d3bf65d582c9482dbbe670cbdfbefd33",
            "8c6ec8bdade34c6bbe7e7a3f57c82c51"
          ]
        },
        "id": "vkKOPurerCIt",
        "outputId": "c77baece-3d1a-4fc5-8af1-40db5b998e97"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit, jacfwd, jacrev, lax\n",
        "from jax.scipy.linalg import inv, eigh\n",
        "from tqdm.auto import tqdm\n",
        "from jax.tree_util import Partial\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# ==========================================\n",
        "# 1. Colab Parameters & Setup\n",
        "# ==========================================\n",
        "\n",
        "# @title System & Simulation Parameters\n",
        "n = 40                  # @param {type:\"integer\"}\n",
        "F = 8.0                 # @param {type:\"number\"}\n",
        "dt = 0.03               # @param {type:\"number\"}\n",
        "dt_obs = 0.15           # @param {type:\"number\"}\n",
        "sigma_v = 0.01          # @param {type:\"number\"}\n",
        "sigma_y = 1           # @param {type:\"number\"}\n",
        "num_steps = 200         # @param {type:\"integer\"}\n",
        "\n",
        "# @title Training/Optimization Parameters\n",
        "J0 = 0                  # @param {type:\"integer\"}\n",
        "N = 10                  # @param {type:\"integer\"}\n",
        "alpha = 1e-5            # @param {type:\"number\"}\n",
        "n_iters = 100           # @param {type:\"integer\"}\n",
        "\n",
        "# --- Derived Parameters ---\n",
        "observation_interval = int(dt_obs / dt)  # Steps per observation\n",
        "print(f\"Derived: Observation Interval = {observation_interval} steps\")\n",
        "\n",
        "# Model parameters\n",
        "m0 = jnp.ones((n,))\n",
        "C0 = jnp.eye(n) * 1.0\n",
        "Q = jnp.eye(n) * (sigma_v ** 2)  # Process noise covariance\n",
        "H = jnp.eye(n)                   # Full Observation\n",
        "R = jnp.eye(n) * (sigma_y ** 2)  # Observation noise covariance\n",
        "\n",
        "print(f\"System Initialized: {n} States, Sigma_v={sigma_v}, Sigma_y={sigma_y}\")\n",
        "print(f\"Training Settings: Alpha={alpha}, Iterations={n_iters}, MC Samples={N}\")\n",
        "\n",
        "# Learning Parameters\n",
        "alpha = 1e-5        # Learning rate\n",
        "n_iters = 100       # Iterations\n",
        "\n",
        "print(f\"System: dt={dt}, Interval={observation_interval}, Steps={num_steps}, Sigma_y={sigma_y}\")\n",
        "print(f\"Training: Alpha={alpha}, Iters={n_iters}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Dynamics & Helpers\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def rk4_step(x, F, dt):\n",
        "    f = lambda y: (jnp.roll(y, 1) - jnp.roll(y, -2)) * jnp.roll(y, -1) - y + F\n",
        "    k1 = dt * f(x)\n",
        "    k2 = dt * f(x + k1/2)\n",
        "    k3 = dt * f(x + k2/2)\n",
        "    k4 = dt * f(x + k3)\n",
        "    return x + 1/6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "\n",
        "class Lorenz96:\n",
        "    def __init__(self, dt=0.01, F=8.0):\n",
        "        self.dt = dt\n",
        "        self.F = F\n",
        "\n",
        "    def step(self, x):\n",
        "        return rk4_step(x, self.F, self.dt)\n",
        "\n",
        "@jit\n",
        "def step_function(carry, input):\n",
        "    key, x, observation_interval, H, Q, R, model_step, counter = carry\n",
        "    n = len(x)\n",
        "    key, subkey = random.split(key)\n",
        "    x_j = model_step(x)\n",
        "\n",
        "    # Conditional update logic\n",
        "    def update_observation():\n",
        "        x_noise = x_j + random.multivariate_normal(key, jnp.zeros(n), Q)\n",
        "        obs_state = jnp.dot(H, x_noise)\n",
        "        obs_noise = random.multivariate_normal(subkey, jnp.zeros(H.shape[0]), R)\n",
        "        return x_noise, obs_state + obs_noise\n",
        "\n",
        "    def no_update():\n",
        "        return x_j, jnp.nan * jnp.ones(H.shape[0])\n",
        "\n",
        "    x_j, obs = lax.cond(counter % observation_interval == 0,\n",
        "                        update_observation,\n",
        "                        no_update)\n",
        "    counter += 1\n",
        "    carry = (key, x_j, observation_interval, H, Q, R, model_step, counter)\n",
        "    output = (x_j, obs)\n",
        "    return carry, output\n",
        "\n",
        "@partial(jit, static_argnums=(1, 2, 7))\n",
        "def generate_true_states(key, num_steps, n, x0, H, Q, R, model_step, observation_interval):\n",
        "    initial_carry = (key, x0, observation_interval, H, Q, R, model_step, 1)\n",
        "    _, (xs, observations) = lax.scan(step_function, initial_carry, None, length=num_steps-1)\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    initial_observation = H @ x0 + random.multivariate_normal(subkey, jnp.zeros(H.shape[0]), R)\n",
        "\n",
        "    xs = jnp.vstack([x0[jnp.newaxis, :], xs])\n",
        "    observations = jnp.vstack([initial_observation[jnp.newaxis, :], observations])\n",
        "    return observations, xs\n",
        "\n",
        "# Initialize\n",
        "initial_state = random.normal(random.PRNGKey(42), (n,))\n",
        "l96_model = Lorenz96(dt=dt, F=F)\n",
        "l96_step = Partial(l96_model.step)\n",
        "\n",
        "# Generate Data\n",
        "key = random.PRNGKey(42)\n",
        "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_step, observation_interval)\n",
        "y = observations\n",
        "\n",
        "# REMOVED: visualize_observations function\n",
        "\n",
        "# ==========================================\n",
        "# 3. Cost Function & Filtering\n",
        "# ==========================================\n",
        "\n",
        "jacobian_function = jacrev(l96_step, argnums=0)\n",
        "jac_func = Partial(jacobian_function)\n",
        "\n",
        "@jit\n",
        "def KL_gaussian(n, m1, C1, m2, C2):\n",
        "    C2_inv = inv(C2)\n",
        "    # Added abs() and epsilon to log to prevent NaNs during rapid optimization\n",
        "    vals_c2 = jnp.abs(eigh(C2)[0]) + 1e-10\n",
        "    vals_c1 = jnp.abs(eigh(C1)[0]) + 1e-10\n",
        "    log_det_ratio = (jnp.log(vals_c2).sum() - jnp.log(vals_c1).sum()).real\n",
        "    return 0.5 * (log_det_ratio - n + jnp.trace(C2_inv @ C1) + ((m2 - m1).T @ C2_inv @ (m2 - m1)))\n",
        "\n",
        "@jit\n",
        "def log_likelihood(v, y, H, R, J, J0):\n",
        "    def log_likelihood_j(_, v_y):\n",
        "        v_j, y_j = v_y\n",
        "        # Handle NaNs for sparse observations\n",
        "        is_obs = ~jnp.isnan(y_j[0])\n",
        "        error = jnp.where(is_obs, y_j - H @ v_j, 0.0)\n",
        "        # Safe computation even with NaNs in y_j (masked by is_obs)\n",
        "        ll = jnp.where(is_obs, error.T @ inv(R) @ error, 0.0)\n",
        "        return _, ll\n",
        "\n",
        "    _, lls = lax.scan(log_likelihood_j, None, (v, y))\n",
        "\n",
        "    # Correct count of actual observations\n",
        "    valid_count = J - jnp.sum(jnp.isnan(y[:, 0]))\n",
        "    sum_ll = jnp.sum(lls)\n",
        "\n",
        "    return -0.5 * sum_ll - 0.5 * (valid_count) * jnp.log(2 * jnp.pi) - 0.5 * (valid_count) * jnp.linalg.slogdet(R)[1]\n",
        "\n",
        "@partial(jit, static_argnums=())\n",
        "def KL_sum(m_preds, C_preds, m_updates, C_updates, n, state_transition_function, Q, key):\n",
        "    def KL_j(_, m_C_y):\n",
        "        m_pred, C_pred, m_update, C_update = m_C_y\n",
        "        return _, KL_gaussian(n, m_update, C_update, m_pred, C_pred)\n",
        "    _, mean_kls = lax.scan(KL_j, None, (m_preds, C_preds, m_updates, C_updates))\n",
        "    kl_sum = jnp.sum(mean_kls)\n",
        "    return kl_sum\n",
        "\n",
        "@partial(jit, static_argnums=(4))\n",
        "def apply_filtering_fixed_nonlinear(m0, C0, y, K, n, state_transition_function, jacobian_function, H, Q, R):\n",
        "    partial_filter_step = lambda m_C_prev, y_curr: filter_step_nonlinear(m_C_prev, y_curr, K, n, state_transition_function, jacobian_function, H, Q, R)\n",
        "    _, m_C = lax.scan(partial_filter_step, (m0, C0, m0, C0), y)\n",
        "    return m_C\n",
        "\n",
        "@partial(jit, static_argnums=(3))\n",
        "def filter_step_nonlinear(m_C_prev, y_curr, K, n, state_transition_function, jacobian_function, H, Q, R):\n",
        "    _, _, m_prev, C_prev = m_C_prev\n",
        "    m_pred = state_transition_function(m_prev)\n",
        "    F_jac = jacobian_function(m_prev)\n",
        "    C_pred = F_jac @ C_prev @ F_jac.T + Q\n",
        "\n",
        "    # Handle NaNs (Time sparsity)\n",
        "    is_obs = ~jnp.isnan(y_curr[0])\n",
        "\n",
        "    # Innov: if NaN, use 0 (masked later)\n",
        "    innov = jnp.where(is_obs, y_curr, H @ m_pred) - (H @ m_pred)\n",
        "    m_update = m_pred + jnp.where(is_obs, 1.0, 0.0) * (K @ innov)\n",
        "\n",
        "    # Covariance update\n",
        "    I_KH = jnp.eye(n) - K @ H\n",
        "    C_update_obs = I_KH @ C_pred @ I_KH.T + K @ R @ K.T\n",
        "\n",
        "    # If not observed, just C_pred\n",
        "    C_update = (C_update_obs * is_obs) + (C_pred * (1.0 - is_obs))\n",
        "\n",
        "    return (m_pred, C_pred, m_update, C_update), (m_pred, C_pred, m_update, C_update)\n",
        "\n",
        "@partial(jit, static_argnums=(3, 10))\n",
        "def var_cost(K, m0, C0, n, H, Q, R, y, key, num_steps, J0):\n",
        "    m_preds, C_preds, m_updates, C_updates = apply_filtering_fixed_nonlinear(m0, C0, y, K, n, l96_step, jac_func, H, Q, R)\n",
        "    key, *subkeys = random.split(key, num=N+1)\n",
        "\n",
        "    kl_sum = KL_sum(m_preds, C_preds, m_updates, C_updates, n, l96_step, Q, key)\n",
        "\n",
        "    # Use vmap for MC sampling speedup\n",
        "    subkeys = jnp.vstack(subkeys)\n",
        "    def inner_map(subkey):\n",
        "        return log_likelihood(random.multivariate_normal(subkey, m_updates, C_updates), y, H, R, num_steps, J0)\n",
        "\n",
        "    ll_samples = jax.vmap(inner_map)(subkeys)\n",
        "    cost = kl_sum - jnp.mean(ll_samples)\n",
        "    return cost\n",
        "\n",
        "# ==========================================\n",
        "# 4. Training Loop (ACCELERATED)\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def kalman_step(state, observation, params):\n",
        "    m_prev, C_prev = state\n",
        "    l96_step, jac_func, H, Q, R = params\n",
        "    m_pred = l96_step(m_prev)\n",
        "    F_jac = jac_func(m_prev)\n",
        "    C_pred = F_jac @ C_prev @ F_jac.T + Q\n",
        "\n",
        "    is_obs = ~jnp.isnan(observation[0])\n",
        "\n",
        "    S = H @ C_pred @ H.T + R\n",
        "    K_curr = C_pred @ H.T @ jnp.linalg.inv(S)\n",
        "\n",
        "    K_use = K_curr * is_obs\n",
        "    m_update = m_pred + K_use @ (jnp.where(is_obs, observation, H@m_pred) - H @ m_pred)\n",
        "    C_update = (jnp.eye(n) - K_use @ H) @ C_pred\n",
        "\n",
        "    return (m_update, C_update), (m_update, C_update, K_use)\n",
        "\n",
        "@jit\n",
        "def kalman_filter_process(m0, C0, observations):\n",
        "    params = (l96_step, jac_func, H, Q, R)\n",
        "    initial_state = (m0, C0)\n",
        "    _, (m, C, K) = lax.scan(lambda state, obs: kalman_step(state, obs, params),\n",
        "                            initial_state, observations)\n",
        "    return m, C, K\n",
        "\n",
        "print(\"Computing Baseline...\")\n",
        "base_m, base_C, base_K = kalman_filter_process(m0, C0, observations)\n",
        "K_steady = jnp.mean(base_K[-50:, :, :], axis=0)\n",
        "\n",
        "var_cost_grad = grad(var_cost, argnums=0)\n",
        "K_opt = jnp.eye(n) * 0.4 # Keeping your initial guess\n",
        "\n",
        "# --- ACCELERATION START ---\n",
        "# We JIT compile the update step AND the evaluation calculation\n",
        "# to avoid Python loop overhead.\n",
        "\n",
        "@jit\n",
        "def train_step(K, key):\n",
        "    \"\"\"Computes gradient and applies update in one JIT-compiled step.\"\"\"\n",
        "    grad_K = var_cost_grad(K, m0, C0, n, H, Q, R, y, key, num_steps, J0)\n",
        "    K_new = K - alpha * grad_K\n",
        "    return K_new\n",
        "\n",
        "@jit\n",
        "def evaluate_step(K, true_states, base_m, base_C):\n",
        "    \"\"\"Computes error and KL divergence in JIT.\"\"\"\n",
        "    _, _, predicted_states, covariances = apply_filtering_fixed_nonlinear(m0, C0, y, K, n, l96_step, jac_func, H, Q, R)\n",
        "\n",
        "    # MSE\n",
        "    prediction_error = jnp.mean(jnp.mean((predicted_states - true_states)**2, axis=1))\n",
        "\n",
        "    # KL Div to Baseline (Vectorized)\n",
        "    def compute_kl(p_m, p_c, b_m, b_c):\n",
        "        return KL_gaussian(n, p_m, p_c, b_m, b_c)\n",
        "\n",
        "    kl_divs = jax.vmap(compute_kl)(predicted_states, covariances, base_m, base_C)\n",
        "    mean_kl = jnp.mean(kl_divs)\n",
        "\n",
        "    return prediction_error, mean_kl\n",
        "\n",
        "# --- ACCELERATION END ---\n",
        "\n",
        "prediction_errors = []\n",
        "true_div = []\n",
        "\n",
        "print(f\"Starting Training ({n_iters} iterations)...\")\n",
        "for i in tqdm(range(n_iters)):\n",
        "    key, subkey = random.split(key)\n",
        "\n",
        "    # 1. Update (JIT accelerated)\n",
        "    K_opt = train_step(K_opt, subkey)\n",
        "\n",
        "    # 2. Evaluate (JIT accelerated)\n",
        "    # Note: Evaluating every step is still costly, but JIT helps.\n",
        "    err, kl = evaluate_step(K_opt, true_states, base_m, base_C)\n",
        "\n",
        "    prediction_errors.append(err)\n",
        "    true_div.append(kl)\n",
        "\n",
        "# ==========================================\n",
        "# 5. Plotting\n",
        "# ==========================================\n",
        "\n",
        "def plot_optimization_results(prediction_errors, true_div, n_iters, scaling=1.3, max_n_locator=5):\n",
        "    fig, (ax1, ax3) = plt.subplots(figsize=(10, 4), ncols=2)\n",
        "\n",
        "    # K norms and KL Divergence\n",
        "    ax1.set_xlabel('Iteration', fontsize=14*scaling)\n",
        "    ax1.tick_params(axis='y', labelsize=12*scaling)\n",
        "    ax1.tick_params(axis='x', labelsize=12*scaling)\n",
        "    ax1.yaxis.set_major_locator(plt.MaxNLocator(max_n_locator))\n",
        "    ax1.set_ylabel('KL divergence to true filter', fontsize=14*scaling)\n",
        "    ax1.plot(range(1, n_iters+1), true_div[:n_iters], label='KL divergence to true filter')\n",
        "\n",
        "    # MSE from True States\n",
        "    ax3.plot(range(1, n_iters+1), prediction_errors[:n_iters])\n",
        "    ax3.set_xlabel(\"Iteration\", fontsize=14*scaling)\n",
        "    ax3.set_ylabel(\"Prediction error (MSE)\", fontsize=14*scaling)\n",
        "    ax3.tick_params(axis='x', labelsize=12*scaling)\n",
        "    ax3.tick_params(axis='y', labelsize=12*scaling)\n",
        "    ax3.yaxis.set_major_locator(plt.MaxNLocator(max_n_locator))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_optimization_results(prediction_errors, true_div, n_iters, scaling=1.3, max_n_locator=3)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def plot_k_matrices(K_steady, K_opt, scaling=1.2, max_n_locator=5):\n",
        "    # 1. Determine shared color scale limits\n",
        "    vmin = min(np.min(K_steady), np.min(K_opt))\n",
        "    vmax = max(np.max(K_steady), np.max(K_opt))\n",
        "\n",
        "    # Create subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2, constrained_layout=True)\n",
        "\n",
        "    # Plot K_steady\n",
        "    c1 = ax1.pcolormesh(K_steady, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
        "    ax1.set_title('$K_\\\\mathrm{steady}$ (Baseline)', fontsize=14*scaling)\n",
        "    ax1.invert_yaxis()\n",
        "    ax1.tick_params(axis='both', labelsize=12*scaling)\n",
        "    ax1.yaxis.set_major_locator(plt.MaxNLocator(max_n_locator))\n",
        "\n",
        "    # Plot K_opt\n",
        "    c2 = ax2.pcolormesh(K_opt, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
        "    ax2.set_title('$K_\\\\mathrm{opt}$ (Learned)', fontsize=14*scaling)\n",
        "    ax2.invert_yaxis()\n",
        "    ax2.tick_params(axis='both', labelsize=12*scaling)\n",
        "    ax2.yaxis.set_major_locator(plt.MaxNLocator(max_n_locator))\n",
        "\n",
        "    # Add Shared Colorbar\n",
        "    cbar = fig.colorbar(c2, ax=[ax1, ax2], orientation='vertical', fraction=0.05, pad=0.02)\n",
        "    cbar.ax.tick_params(labelsize=12*scaling)\n",
        "    cbar.set_label('Gain Value', fontsize=12*scaling)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot the matrices\n",
        "plot_k_matrices(K_steady, K_opt, scaling=1.2)\n",
        "\n",
        "# --- Save Learned Trajectory Mean ---\n",
        "\n",
        "print(\"Generating final trajectory with K_opt...\")\n",
        "# Run the filter one last time with the optimized K\n",
        "_, _, learned_traj_mean, _ = apply_filtering_fixed_nonlinear(m0, C0, y, K_opt, n, l96_step, jac_func, H, Q, R)\n",
        "\n",
        "# Save to file\n",
        "save_path = 'learned_traj_mean.npy'\n",
        "np.save(save_path, learned_traj_mean)\n",
        "print(f\"Learned trajectory mean saved to: {save_path}\")\n",
        "print(f\"Shape: {learned_traj_mean.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "srGCMCgp0IIV",
        "outputId": "0dd61b6a-7207-4c22-d960-01e2275caf90"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2.3.2 Evaluation: EnKF Baseline & RMSE Comparison\n",
        "# ==========================================\n",
        "\n",
        "# @title EnKF Parameters\n",
        "N_ens = 40              # @param {type:\"integer\"}\n",
        "inflation = 1.05        # @param {type:\"number\"}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. EnKF Implementation (Handles Sparse Observations)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "@partial(jit, static_argnums=(0, 1))\n",
        "def enkf_step_fn(N_ens, infl, carry, y_curr):\n",
        "    \"\"\"\n",
        "    Single step of the Ensemble Kalman Filter.\n",
        "    \"\"\"\n",
        "    key, ens = carry\n",
        "    key, subkey_q, subkey_r = random.split(key, 3)\n",
        "\n",
        "    # --- Forecast Step ---\n",
        "    ens_pred = jax.vmap(l96_step)(ens)\n",
        "    q_noise = random.multivariate_normal(subkey_q, jnp.zeros(n), Q, shape=(N_ens,))\n",
        "    ens_pred = ens_pred + q_noise\n",
        "\n",
        "    # --- Analysis Step ---\n",
        "    is_obs = ~jnp.isnan(y_curr[0])\n",
        "\n",
        "    def update(ens_p):\n",
        "        r_noise = random.multivariate_normal(subkey_r, jnp.zeros(n), R, shape=(N_ens,))\n",
        "        y_perturbed = y_curr + r_noise\n",
        "\n",
        "        h_ens = ens_p\n",
        "        x_bar = jnp.mean(ens_p, axis=0)\n",
        "        h_bar = jnp.mean(h_ens, axis=0)\n",
        "\n",
        "        X_prime = ens_p - x_bar\n",
        "        H_prime = h_ens - h_bar\n",
        "\n",
        "        P_xy = (X_prime.T @ H_prime) / (N_ens - 1)\n",
        "        P_yy = (H_prime.T @ H_prime) / (N_ens - 1) + R\n",
        "\n",
        "        K_gain = P_xy @ jnp.linalg.inv(P_yy)\n",
        "\n",
        "        innov = y_perturbed - h_ens\n",
        "        ens_ana = ens_p + (K_gain @ innov.T).T\n",
        "\n",
        "        ens_mean_ana = jnp.mean(ens_ana, axis=0)\n",
        "        ens_ana = ens_mean_ana + infl * (ens_ana - ens_mean_ana)\n",
        "        return ens_ana\n",
        "\n",
        "    def no_update(ens_p):\n",
        "        return ens_p\n",
        "\n",
        "    ens_updated = lax.cond(is_obs, update, no_update, ens_pred)\n",
        "    step_mean = jnp.mean(ens_updated, axis=0)\n",
        "\n",
        "    return (key, ens_updated), step_mean\n",
        "\n",
        "def run_enkf(N_ens, infl, y_seq, m0, C0, key):\n",
        "    key, subkey = random.split(key)\n",
        "    # Initialize ensemble around m0\n",
        "    ens_0 = random.multivariate_normal(subkey, m0, C0, shape=(N_ens,))\n",
        "\n",
        "    scan_fn = partial(enkf_step_fn, N_ens, infl)\n",
        "    _, enkf_means = lax.scan(scan_fn, (key, ens_0), y_seq)\n",
        "\n",
        "    return enkf_means\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Execution & RMSE Comparison\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "print(f\"Running EnKF with N={N_ens} ensemble members...\")\n",
        "enkf_key = random.PRNGKey(101)\n",
        "# Generate EnKF trajectory\n",
        "enkf_traj_mean = run_enkf(N_ens, inflation, y, m0, C0, enkf_key)\n",
        "\n",
        "# RMSE Calculation Function\n",
        "def calculate_rmse(estimated_traj, true_traj):\n",
        "    \"\"\"\n",
        "    Computes RMSE averaged over time (J) and dimensions (d).\n",
        "    RMSE = sqrt( 1/(J*d) * sum( ||m_j - v_j||^2 ) )\n",
        "    \"\"\"\n",
        "    # Ensure shapes match before calculation\n",
        "    # Truncate to the minimum length to avoid broadcasting errors\n",
        "    min_len = min(len(estimated_traj), len(true_traj))\n",
        "    est = estimated_traj[:min_len]\n",
        "    tru = true_traj[:min_len]\n",
        "\n",
        "    sq_diff = (est - tru) ** 2\n",
        "    mean_sq_error = jnp.mean(sq_diff)\n",
        "    return jnp.sqrt(mean_sq_error)\n",
        "\n",
        "# Calculate RMSEs (Corrected: No [1:] slicing)\n",
        "rmse_learned = calculate_rmse(learned_traj_mean, true_states)\n",
        "rmse_enkf = calculate_rmse(enkf_traj_mean, true_states)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Results Output\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"### 2.3.2 Evaluation Results (RMSE)\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Learned Gain (VI) RMSE : {rmse_learned:.5f}\")\n",
        "print(f\"EnKF (N={N_ens}) RMSE      : {rmse_enkf:.5f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "diff = rmse_enkf - rmse_learned\n",
        "if diff > 0:\n",
        "    print(f\">> Result: Learned Gain outperforms EnKF by {diff:.5f}\")\n",
        "else:\n",
        "    print(f\">> Result: EnKF outperforms Learned Gain by {-diff:.5f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "dim_to_plot = 0\n",
        "# Plot a subset of steps for clarity\n",
        "steps_to_show = 100\n",
        "t_axis = jnp.arange(steps_to_show)\n",
        "\n",
        "plt.plot(t_axis, true_states[:steps_to_show, dim_to_plot], 'k-', lw=1.5, alpha=0.6, label='True State')\n",
        "plt.plot(t_axis, learned_traj_mean[:steps_to_show, dim_to_plot], 'r--', lw=1.5, label=f'Learned (RMSE={rmse_learned:.3f})')\n",
        "plt.plot(t_axis, enkf_traj_mean[:steps_to_show, dim_to_plot], 'g:', lw=2, label=f'EnKF (RMSE={rmse_enkf:.3f})')\n",
        "\n",
        "plt.title(f\"Trajectory Reconstruction (Dim {dim_to_plot}, First {steps_to_show} Steps)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"State Value\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAyxRDNb29K"
      },
      "source": [
        "### 2.4 Machine Learning Enhanced EnKF Strategies\n",
        "\n",
        "We explore two paradigms for integrating machine learning into the data assimilation cycle. Instead of relying on manual tuning or empirical approximations, we parameterize critical components of the filter and optimize them through a data-driven framework.\n",
        "\n",
        "#### **Data Assumption**\n",
        "We assume the physical dynamics $\\Psi$, the initial condition $\\mathbf{v}_0 \\sim \\mathcal{N}(m_0, C_0)$, and the noise covariances $\\Sigma$ and $\\Gamma$ are known. To generate training data, we simulate the system to obtain a set of ground truth trajectories $\\{\\mathbf{v}_j^\\dagger\\}_{j=1}^J$ and their corresponding noisy, sparse observations $\\{\\mathbf{y}_j^\\dagger\\}_{j=1}^J$, where $J$ denotes the total number of assimilation cycles in the training set.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4.1 Learning Regularization Parameters\n",
        "In this approach, we treat the inflation rate $\\alpha$ and the localization radius $r$ as learnable parameters. While these are traditionally fixed constants, we represent them as optimizable variables $\\theta_{\\text{reg}} = \\{\\alpha, r\\}$. This allows the regularization to be optimized directly for the 40-dimensional Lorenz-96 space, providing the necessary ensemble spread and spatial filtering to prevent filter divergence.\n",
        "\n",
        "### 2.4.2 Neural Kalman Gain Operator\n",
        "This method replaces the empirical Kalman Gain calculation with a learned neural operator $\\mathbf{K}_\\theta$. The analysis update for each ensemble member is reformulated as:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_j^{(n)} = \\hat{\\mathbf{v}}_j^{(n)} + \\mathbf{K}_\\theta(\\dots) \\underbrace{\\left( \\mathbf{y}^\\dagger_j - h(\\hat{\\mathbf{v}}_j^{(n)}) - \\eta_j^{(n)} \\right)}_{\\text{Innovation}}\n",
        "$$\n",
        "\n",
        "The neural gain $\\mathbf{K}_\\theta$ is designed to capture complex, potentially non-linear mappings. Its output is conditioned on the following features:\n",
        "* **Ensemble Statistics:** The predictive mean $\\bar{\\mathbf{v}}_j$ and the predictive covariance $\\hat{\\mathbf{C}}_j$.\n",
        "* **Local State Information:** The individual forecast member $\\hat{\\mathbf{v}}_j^{(n)}$.\n",
        "* **Observation Data:** The actual measurement $\\mathbf{y}_j^\\dagger$ and the resulting innovation vector.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4.3 Training Objective (Loss Function)\n",
        "The parameters $\\theta$ are optimized by minimizing the discrepancy between the filter's estimation and the known ground truth. We define the loss function as the Mean Squared Error (MSE) or Normalized MSE between the analysis ensemble mean and the true state:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\frac{1}{J} \\sum_{j=1}^J \\| \\bar{\\mathbf{v}}_j^{\\text{ana}}(\\theta) - \\mathbf{v}_j^\\dagger \\|^2\n",
        "$$\n",
        "\n",
        "where $\\bar{\\mathbf{v}}_j^{\\text{ana}}(\\theta)$ is the mean of the ensemble after the ML-augmented analysis step. This objective directly rewards the model for producing the most accurate state estimate across the training trajectory.\n",
        "\n",
        "### 2.4.4 Evaluation Metrics\n",
        "To compare the performance of the ML-enhanced filters against the baseline EnKF, we use the Root Mean Squared Error (RMSE) calculated over a separate, unseen test trajectory:\n",
        "\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{J_{\\text{test}} \\cdot 40} \\sum_{j=1}^{J_{\\text{test}}} \\sum_{k=1}^{40} (v_{j,k}^{\\text{est}} - v_{j,k}^\\dagger)^2}\n",
        "$$\n",
        "\n",
        "A lower RMSE indicates that the machine learning model has successfully learned to compensate for the limitations of the standard ensemble-based update, such as sampling errors or suboptimal localization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "QOLpzx1UPBuN",
        "outputId": "da5ed446-b706-410c-9f5f-6c193eec9023"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. Colab Form Configuration\n",
        "# ==========================================\n",
        "# @title Lorenz-96 Data Generation Parameters\n",
        "\n",
        "# @markdown **Time Stepping**\n",
        "delta_t = 0.15 # @param {type:\"number\"}\n",
        "rk4_steps = 5 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown **Noise Levels**\n",
        "sigma_y = 1.0 # @param {type:\"number\"}\n",
        "sigma_v = 0.01 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **Trajectory Settings**\n",
        "traj_steps = 60 # @param {type:\"integer\"}\n",
        "traj_nums = 250 # @param {type:\"integer\"}\n",
        "burn_in_steps = 1000 # @param {type:\"integer\"}\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, vmap, lax\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Constants (Fixed for Lorenz-96 standard case)\n",
        "DIM = 40\n",
        "F_FORCE = 8.0\n",
        "DT_INT = delta_t / rk4_steps\n",
        "# Full observation: Observe all state dimensions\n",
        "H_IDX = jnp.arange(DIM)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Lorenz-96 Dynamics & Solver\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def lorenz96_deriv(v):\n",
        "    \"\"\"Lorenz-96 equations with cyclic boundaries.\"\"\"\n",
        "    return (jnp.roll(v, -1) - jnp.roll(v, 2)) * jnp.roll(v, 1) - v + F_FORCE\n",
        "\n",
        "@jit\n",
        "def rk4_step(v, dt):\n",
        "    \"\"\"Standard RK4 integration step.\"\"\"\n",
        "    k1 = lorenz96_deriv(v)\n",
        "    k2 = lorenz96_deriv(v + 0.5 * dt * k1)\n",
        "    k3 = lorenz96_deriv(v + 0.5 * dt * k2)\n",
        "    k4 = lorenz96_deriv(v + dt * k3)\n",
        "    return v + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def integrate_window(v_init, steps, dt):\n",
        "    \"\"\"Integrate for one assimilation window (Delta t).\"\"\"\n",
        "    def body_fn(v, _):\n",
        "        return rk4_step(v, dt), None\n",
        "    v_final, _ = lax.scan(body_fn, v_init, None, length=steps)\n",
        "    return v_final\n",
        "\n",
        "# ==========================================\n",
        "# 3. Data Generation (Vmapped)\n",
        "# ==========================================\n",
        "\n",
        "@partial(jit, static_argnums=(1, 2, 3))\n",
        "def generate_single_traj(key, b_steps, t_steps, r_steps):\n",
        "    \"\"\"Generates one truth trajectory and corresponding observations.\"\"\"\n",
        "    k_init, k_dyn, k_obs = random.split(key, 3)\n",
        "\n",
        "    # 1. Burn-in to find v0 on attractor\n",
        "    v_start = random.normal(k_init, (DIM,))\n",
        "    def burn_in_step(v_carry, _):\n",
        "        return rk4_step(v_carry, 0.01), None\n",
        "    v0, _ = lax.scan(burn_in_step, v_start, None, length=b_steps)\n",
        "\n",
        "    # 2. Generate trajectory with dynamic noise\n",
        "    def scan_fn(v_prev, k_step):\n",
        "        # Forecast for one window\n",
        "        v_next_clean = integrate_window(v_prev, r_steps, DT_INT)\n",
        "        # Add dynamic noise\n",
        "        v_next = v_next_clean + sigma_v * random.normal(k_step, (DIM,))\n",
        "        return v_next, v_next\n",
        "\n",
        "    keys_dyn = random.split(k_dyn, t_steps)\n",
        "    _, v_traj = lax.scan(scan_fn, v0, keys_dyn)\n",
        "    v_traj = jnp.concatenate([v0[None, :], v_traj], axis=0)\n",
        "\n",
        "    # 3. Generate observations (full)\n",
        "    y_clean = v_traj[1:, H_IDX]\n",
        "    y_obs = y_clean + sigma_y * random.normal(k_obs, y_clean.shape)\n",
        "\n",
        "    return v_traj, y_obs\n",
        "\n",
        "print(f\"Generating {traj_nums} trajectories parallelly...\")\n",
        "keys = random.split(random.PRNGKey(42), traj_nums)\n",
        "all_v, all_y = vmap(lambda k: generate_single_traj(k, burn_in_steps, traj_steps, rk4_steps))(keys)\n",
        "\n",
        "# Split 80/20\n",
        "split_idx = int(0.8 * traj_nums)\n",
        "train_v, test_v = all_v[:split_idx], all_v[split_idx:]\n",
        "train_y, test_y = all_y[:split_idx], all_y[split_idx:]\n",
        "\n",
        "# Save\n",
        "jnp.savez(\"lorenz96_data.npz\",\n",
        "          train_v=train_v, train_y=train_y,\n",
        "          test_v=test_v, test_y=test_y)\n",
        "print(f\"Data saved: {train_v.shape[0]} train trajs, {test_v.shape[0]} test trajs.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Visualizations\n",
        "# ==========================================\n",
        "\n",
        "# 4.1 Visualization: GC Function\n",
        "def gc_function(d, r):\n",
        "    x = jnp.abs(d) / r\n",
        "    return jnp.where(x < 1.0,\n",
        "                     1 - 5/3*x**2 + 5/8*x**3 + 1/2*x**4 - 1/4*x**5,\n",
        "                     jnp.where(x < 2.0,\n",
        "                               4 - 5*x + 5/3*x**2 + 5/8*x**3 - 1/2*x**4 + 1/12*x**5 - 2/(3*x),\n",
        "                               0.0))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "d_vals = jnp.linspace(0, 12, 200)\n",
        "for r_val in [2.0, 4.0, 6.0]:\n",
        "    plt.plot(d_vals, gc_function(d_vals, r_val), label=f\"r={r_val}\", lw=2)\n",
        "plt.title(\"Gaspari-Cohn Localization Weights\")\n",
        "plt.xlabel(\"Distance\"); plt.ylabel(\"Weight\")\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 4.2 Visualization: Lorenz-96 Trajectory Heatmap\n",
        "plt.figure(figsize=(12, 5))\n",
        "sample_traj = all_v[0, 1:]\n",
        "plt.imshow(sample_traj.T, aspect='auto', cmap='coolwarm', origin='lower')\n",
        "plt.colorbar(label=\"State Value\")\n",
        "plt.title(f\"Lorenz-96 Ground Truth (First {traj_steps} Cycles)\")\n",
        "plt.xlabel(\"Time Step (Assimilation Cycle)\"); plt.ylabel(\"Dimension Index\")\n",
        "plt.tight_layout()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825,
          "referenced_widgets": [
            "22778526a4a347c692daf87a5d7fbf27",
            "b36334acdbff430a8e6a192920da0522",
            "8123aba16527447d8613aecbc72f7855",
            "25f4ce0f40624e3fa5f6ca6e4ff0e345",
            "86356fdcc3e146d5a7f34dadbfb79fd6",
            "3c9f47b5e0604c038ed9e73217685b55",
            "14801ad3ff7e432c9699b5f85b653884",
            "86eae8ffc69f4e43b206f8ec74e416a7",
            "220eeb7eb4a64ebd9f26250ad59e026e",
            "9221bfa6b47647d397a3791c27a000df",
            "ea23f17887fb4ff1b767b9f2207f6f1c"
          ]
        },
        "id": "kAjT8KeWe9LR",
        "outputId": "0af04b61-f9b7-432e-a61a-3687d0a26d98"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. Training & Evaluation Configuration\n",
        "# ==========================================\n",
        "# @title EnKF Hyperparameter Optimization (Robust Version)\n",
        "\n",
        "optimizer_choice = \"Adam\" # @param [\"L-BFGS-B\", \"Adam\", \"AdamW\"]\n",
        "learning_rate = 0.0001 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "training_iterations = 500 # @param {type:\"integer\"}\n",
        "train_ens_size_val = 100 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ---\n",
        "# Test settings\n",
        "test_ens_sizes = [10, 50, 100, 200]\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, vmap, value_and_grad, lax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.optimize\n",
        "from functools import partial\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Precompute cyclic distance matrix (State Dim 40 x Obs Dim 40)\n",
        "dist_mat = jnp.abs(jnp.arange(40)[:, None] - H_IDX[None, :])\n",
        "dist_mat = jnp.minimum(dist_mat, 40 - dist_mat)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Core Assimilation Logic\n",
        "# ==========================================\n",
        "\n",
        "@jit\n",
        "def gc_function(d, r):\n",
        "    \"\"\"Gaspari-Cohn localization kernel.\"\"\"\n",
        "    x = d / r\n",
        "    return jnp.where(x < 1.0,\n",
        "                     1 - 5/3*x**2 + 5/8*x**3 + 1/2*x**4 - 1/4*x**5,\n",
        "                     jnp.where(x < 2.0,\n",
        "                               4 - 5*x + 5/3*x**2 + 5/8*x**3 - 1/2*x**4 + 1/12*x**5 - 2/(3*x),\n",
        "                               0.0))\n",
        "\n",
        "@jit\n",
        "def enkf_cycle_step(carry, inputs, alpha, r, ens_size):\n",
        "    \"\"\"Single EnKF assimilation cycle.\"\"\"\n",
        "    ens, k_step = carry\n",
        "    y_obs, _ = inputs\n",
        "    k1, k2, k3 = random.split(k_step, 3)\n",
        "\n",
        "    # 1. Forecast: Integrate members for one window + dynamic noise\n",
        "    ens_f = vmap(lambda v: integrate_window(v, rk4_steps, DT_INT))(ens)\n",
        "    ens_f += sigma_v * random.normal(k1, ens_f.shape)\n",
        "\n",
        "    # 2. Localized Analysis\n",
        "    ens_mean = jnp.mean(ens_f, axis=0)\n",
        "    anom = ens_f - ens_mean\n",
        "    h_ens = ens_f[:, H_IDX]\n",
        "    h_anom = h_ens - jnp.mean(h_ens, axis=0)\n",
        "\n",
        "    C_vh = (1.0 / (ens_size - 1)) * (anom.T @ h_anom)\n",
        "    C_hh = (1.0 / (ens_size - 1)) * (h_anom.T @ h_anom)\n",
        "\n",
        "    L = gc_function(dist_mat, r)\n",
        "    C_vh_loc = L * C_vh\n",
        "\n",
        "    # CHANGED: 10 -> 40 (Full observation dimension)\n",
        "    R_mat = (sigma_y**2) * jnp.eye(40)\n",
        "    K = C_vh_loc @ jnp.linalg.inv(C_hh + R_mat)\n",
        "\n",
        "    eta = sigma_y * random.normal(k2, h_ens.shape)\n",
        "    innovation = y_obs[None, :] - h_ens - eta\n",
        "    ens_a = ens_f + (innovation @ K.T)\n",
        "\n",
        "    # 3. Post-Analysis Inflation\n",
        "    ens_a_mean = jnp.mean(ens_a, axis=0)\n",
        "    ens_a = ens_a_mean + alpha * (ens_a - ens_a_mean)\n",
        "\n",
        "    return (ens_a, k3), ens_a_mean\n",
        "\n",
        "@partial(jit, static_argnums=(3,))\n",
        "def assimilate_l96_trajectory(params, v_true_traj, y_obs_traj, ens_size, key):\n",
        "    \"\"\"Full DA sequence for a Lorenz-96 trajectory.\"\"\"\n",
        "    alpha, r = params\n",
        "    k_init, k_cycle = random.split(key)\n",
        "    ens_init = v_true_traj[0] + random.normal(k_init, (ens_size, DIM))\n",
        "\n",
        "    _, mean_traj_out = lax.scan(\n",
        "        lambda c, i: enkf_cycle_step(c, i, alpha, r, ens_size),\n",
        "        (ens_init, k_cycle),\n",
        "        (y_obs_traj, v_true_traj[1:])\n",
        "    )\n",
        "    return mean_traj_out\n",
        "\n",
        "# ==========================================\n",
        "# 3. Training Logic (Direct & Fail-Fast)\n",
        "# ==========================================\n",
        "\n",
        "truncation_steps = 10  # Window size for backprop\n",
        "\n",
        "@jit\n",
        "def sequence_loss(params, start_v, obs_seq, truth_seq, key):\n",
        "    \"\"\"\n",
        "    Computes pure MSE loss. No penalties, no masking.\n",
        "    \"\"\"\n",
        "    alpha, r = params\n",
        "    ens_size = train_ens_size_val\n",
        "\n",
        "    # Initialize ensemble\n",
        "    k_init, k_scan = random.split(key)\n",
        "    ens_init = start_v + random.normal(k_init, (ens_size, DIM))\n",
        "\n",
        "    def scan_body(carry, inputs):\n",
        "        ens_prev, k = carry\n",
        "        y, _ = inputs\n",
        "        (ens_next, k_next), mean_next = enkf_cycle_step((ens_prev, k), (y, None), alpha, r, ens_size)\n",
        "        return (ens_next, k_next), mean_next\n",
        "\n",
        "    _, mean_traj = lax.scan(\n",
        "        scan_body,\n",
        "        (ens_init, k_scan),\n",
        "        (obs_seq, truth_seq),\n",
        "        length=truncation_steps\n",
        "    )\n",
        "\n",
        "    # Direct MSE calculation. If this explodes, we let it explode.\n",
        "    diff = mean_traj - truth_seq\n",
        "    mse = jnp.mean(diff**2)\n",
        "    return mse\n",
        "\n",
        "@jit\n",
        "def train_step(params, m, v, batch_idx, key, i):\n",
        "    \"\"\"\n",
        "    Performs one update step. Returns valid=False if NaNs occur.\n",
        "    \"\"\"\n",
        "\n",
        "    def batch_loss_fn(p):\n",
        "        # Select random windows for the batch\n",
        "        # Note: We use a simple hash of index+iter for randomness to stay JIT-compatible\n",
        "        def single_traj_loss(b_i):\n",
        "            k_local = random.fold_in(key, b_i)\n",
        "            t_start = random.randint(k_local, shape=(), minval=0, maxval=traj_steps - truncation_steps)\n",
        "\n",
        "            # Use dynamic_slice to handle random start index\n",
        "            curr_v = train_v[b_i]\n",
        "            curr_y = train_y[b_i]\n",
        "\n",
        "            v_start = lax.dynamic_slice(curr_v, (t_start, 0), (1, DIM))[0]\n",
        "            truth_seq = lax.dynamic_slice(curr_v, (t_start + 1, 0), (truncation_steps, DIM))\n",
        "            # CHANGED: 10 -> 40 (Full observation dimension)\n",
        "            obs_seq = lax.dynamic_slice(curr_y, (t_start, 0), (truncation_steps, 40))\n",
        "\n",
        "            return sequence_loss(p, v_start, obs_seq, truth_seq, k_local)\n",
        "\n",
        "        # Batch loss\n",
        "        losses = vmap(single_traj_loss)(jnp.arange(32)) # Fixed batch size 32\n",
        "        return jnp.mean(losses)\n",
        "\n",
        "    # Compute Gradients\n",
        "    loss_val, grads = value_and_grad(batch_loss_fn)(params)\n",
        "\n",
        "    # Check for NaNs\n",
        "    is_nan = jnp.any(jnp.isnan(grads)) | jnp.isnan(loss_val) | jnp.any(jnp.isinf(grads))\n",
        "\n",
        "    # Update logic (only if not NaN)\n",
        "    # If NaN, we return old params and set valid=False\n",
        "\n",
        "    # Clip grads\n",
        "    grads = jnp.clip(grads, -1.0, 1.0)\n",
        "\n",
        "    m_new = 0.9 * m + 0.1 * grads\n",
        "    v_new = 0.999 * v + 0.001 * (grads**2)\n",
        "    m_hat = m_new / (1 - 0.9**(i+1))\n",
        "    v_hat = v_new / (1 - 0.999**(i+1))\n",
        "\n",
        "    update = learning_rate * m_hat / (jnp.sqrt(v_hat) + 1e-8)\n",
        "    params_new = params - update\n",
        "\n",
        "    # Constraints\n",
        "    params_new = jnp.array([\n",
        "        jnp.clip(params_new[0], 1.001, 1.5),\n",
        "        jnp.clip(params_new[1], 1.0, 20.0)\n",
        "    ])\n",
        "\n",
        "    # If is_nan, keep old params\n",
        "    final_params = jnp.where(is_nan, params, params_new)\n",
        "    final_m = jnp.where(is_nan, m, m_new)\n",
        "    final_v = jnp.where(is_nan, v, v_new)\n",
        "\n",
        "    return final_params, final_m, final_v, loss_val, is_nan\n",
        "\n",
        "# ==========================================\n",
        "# Execution Loop\n",
        "# ==========================================\n",
        "\n",
        "# Reset Parameters\n",
        "params = jnp.array([1.05, 5.0])\n",
        "m, v = jnp.zeros(2), jnp.zeros(2)\n",
        "\n",
        "print(f\"Starting Training (Window={truncation_steps})...\")\n",
        "pbar = tqdm(range(training_iterations), desc=\"Training\")\n",
        "\n",
        "for i in pbar:\n",
        "    key = random.PRNGKey(i)\n",
        "    params, m, v, loss_val, is_nan_flag = train_step(params, m, v, None, key, i)\n",
        "\n",
        "    if is_nan_flag:\n",
        "        print(f\"\\n[CRITICAL] Training diverged at iteration {i}!\")\n",
        "        print(f\"Last Loss: {loss_val}\")\n",
        "        print(f\"Current Params: Alpha={params[0]:.4f}, R={params[1]:.4f}\")\n",
        "        print(\"Stopping training immediately.\")\n",
        "        break\n",
        "\n",
        "    pbar.set_postfix({\"loss\": f\"{loss_val:.2f}\", \"\": f\"{params[0]:.3f}\", \"r\": f\"{params[1]:.3f}\"})\n",
        "\n",
        "opt_alpha, opt_r = params\n",
        "print(f\"\\nFinal Parameters -> Alpha: {opt_alpha:.4f}, Radius: {opt_r:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Comparative Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def run_test_evaluation(a_learned, r_learned):\n",
        "    n_test = 50\n",
        "    print(\"\\n\" + \"=\"*95)\n",
        "    print(f\"{'Ens Size':<10} | {'Baseline RMSE':<15} | {'Base NaN%':<10} | {'Learned RMSE':<15} | {'Learned NaN%':<10}\")\n",
        "    print(\"-\" * 95)\n",
        "\n",
        "    test_results = {}\n",
        "    for sz in test_ens_sizes:\n",
        "        def eval_single(i, a, r):\n",
        "            m_traj = assimilate_l96_trajectory([a, r], test_v[i], test_y[i], sz, random.PRNGKey(i+5000))\n",
        "            rmse = jnp.sqrt(jnp.mean((m_traj - test_v[i, 1:])**2))\n",
        "            return rmse, jnp.isnan(rmse), m_traj\n",
        "\n",
        "        # Baseline: No inflation (1.0), Moderate Radius (4.0)\n",
        "        b_rmse_list, b_nan_list, b_trajs = vmap(lambda i: eval_single(i, 1.0, 4.0))(jnp.arange(n_test))\n",
        "        l_rmse_list, l_nan_list, l_trajs = vmap(lambda i: eval_single(i, a_learned, r_learned))(jnp.arange(n_test))\n",
        "\n",
        "        def compute_stats(rmses, nans):\n",
        "            valid = ~nans\n",
        "            avg = jnp.sum(jnp.where(valid, rmses, 0.0)) / jnp.maximum(jnp.sum(valid), 1)\n",
        "            percent = (jnp.sum(nans) / n_test) * 100\n",
        "            return avg, percent\n",
        "\n",
        "        b_avg, b_nan_p = compute_stats(b_rmse_list, b_nan_list)\n",
        "        l_avg, l_nan_p = compute_stats(l_rmse_list, l_nan_list)\n",
        "        print(f\"{sz:<10d} | {b_avg:<15.4f} | {b_nan_p:<9.1f}% | {l_avg:<15.4f} | {l_nan_p:<10.1f}%\")\n",
        "\n",
        "        test_results[sz] = {\"b_trajs\": b_trajs, \"l_trajs\": l_trajs, \"l_nans\": l_nan_list}\n",
        "    print(\"=\"*95)\n",
        "    return test_results\n",
        "\n",
        "test_metrics = run_test_evaluation(opt_alpha, opt_r)\n",
        "\n",
        "# ==========================================\n",
        "# 5. Robust Visualization\n",
        "# ==========================================\n",
        "\n",
        "def find_stable_index(results_data):\n",
        "    for sz in reversed(test_ens_sizes):\n",
        "        l_nans = results_data[sz][\"l_nans\"]\n",
        "        valid_idxs = jnp.where(~l_nans)[0]\n",
        "        if len(valid_idxs) > 0:\n",
        "            return int(valid_idxs[0]), sz\n",
        "    return None, None\n",
        "\n",
        "plot_idx, plot_ens = find_stable_index(test_metrics)\n",
        "\n",
        "if plot_idx is not None:\n",
        "    print(f\"\\nVisualizing Test Index: {plot_idx} (Using stable data from Ens={plot_ens})\")\n",
        "    v_true = test_v[plot_idx, 1:]\n",
        "    b_plot = test_metrics[plot_ens][\"b_trajs\"][plot_idx]\n",
        "    l_plot = test_metrics[plot_ens][\"l_trajs\"][plot_idx]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    titles = [\"Ground Truth\", f\"Baseline (Ens={plot_ens})\", f\"Learned (Ens={plot_ens})\"]\n",
        "    v_min, v_max = v_true.min(), v_true.max()\n",
        "\n",
        "    for i, data_mat in enumerate([v_true, b_plot, l_plot]):\n",
        "        im = axes[i].imshow(data_mat.T, aspect='auto', cmap='coolwarm', origin='lower', vmin=v_min, vmax=v_max)\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"Cycle\")\n",
        "        axes[i].set_ylabel(\"Dimension\")\n",
        "        plt.colorbar(im, ax=axes[i])\n",
        "    plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"\\n[Critical Warning]: All tested configurations resulted in filter divergence (NaN).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KdBggLLg-_h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14801ad3ff7e432c9699b5f85b653884": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21e7b61498684659992e758a6b8e949d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b5be04d59547e88609eb14ca015ed7",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fee0baf6ca7349faae7e6dffe86c0ac3",
            "value": 100
          }
        },
        "220eeb7eb4a64ebd9f26250ad59e026e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22778526a4a347c692daf87a5d7fbf27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b36334acdbff430a8e6a192920da0522",
              "IPY_MODEL_8123aba16527447d8613aecbc72f7855",
              "IPY_MODEL_25f4ce0f40624e3fa5f6ca6e4ff0e345"
            ],
            "layout": "IPY_MODEL_86356fdcc3e146d5a7f34dadbfb79fd6"
          }
        },
        "25f4ce0f40624e3fa5f6ca6e4ff0e345": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9221bfa6b47647d397a3791c27a000df",
            "placeholder": "",
            "style": "IPY_MODEL_ea23f17887fb4ff1b767b9f2207f6f1c",
            "value": "0/500[00:05&lt;?,?it/s]"
          }
        },
        "29e47a15c6a9487aa78e88cfe353b312": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c9f47b5e0604c038ed9e73217685b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6063e5c5f5a64fadb36c97bc2357858e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a200acc39de54875aa39cc4fc4618fe8",
            "placeholder": "",
            "style": "IPY_MODEL_a6ac2119bbfe474bbb95841349f7f462",
            "value": "100%"
          }
        },
        "8123aba16527447d8613aecbc72f7855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86eae8ffc69f4e43b206f8ec74e416a7",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_220eeb7eb4a64ebd9f26250ad59e026e",
            "value": 0
          }
        },
        "86356fdcc3e146d5a7f34dadbfb79fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86eae8ffc69f4e43b206f8ec74e416a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c6ec8bdade34c6bbe7e7a3f57c82c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9221bfa6b47647d397a3791c27a000df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e04f998a0444877b07794157cf0b5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3bf65d582c9482dbbe670cbdfbefd33",
            "placeholder": "",
            "style": "IPY_MODEL_8c6ec8bdade34c6bbe7e7a3f57c82c51",
            "value": "100/100[00:36&lt;00:00,3.42it/s]"
          }
        },
        "a200acc39de54875aa39cc4fc4618fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6ac2119bbfe474bbb95841349f7f462": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7d783c0405248b79c64d46cb4094d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6063e5c5f5a64fadb36c97bc2357858e",
              "IPY_MODEL_21e7b61498684659992e758a6b8e949d",
              "IPY_MODEL_9e04f998a0444877b07794157cf0b5de"
            ],
            "layout": "IPY_MODEL_29e47a15c6a9487aa78e88cfe353b312"
          }
        },
        "b0b5be04d59547e88609eb14ca015ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36334acdbff430a8e6a192920da0522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c9f47b5e0604c038ed9e73217685b55",
            "placeholder": "",
            "style": "IPY_MODEL_14801ad3ff7e432c9699b5f85b653884",
            "value": "Training:0%"
          }
        },
        "d3bf65d582c9482dbbe670cbdfbefd33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea23f17887fb4ff1b767b9f2207f6f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fee0baf6ca7349faae7e6dffe86c0ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
